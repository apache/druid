{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73dbb71f-0253-4940-a7b6-0b808839a82d",
   "metadata": {},
   "source": [
    "# Batch Ingestion\n",
    "\n",
    "In this notebook we are focusing on [SQL based ingestion](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#sql-reference). \n",
    "\n",
    "While [Native Batch Ingestion](https://druid.apache.org/docs/latest/ingestion/native-batch.html) is still available in Apache Druid 26.0.0, the ease of use and improved performance of SQL based ingestion powered by the Multi-stage Query Framework is the quickly becoming the norm.\n",
    "\n",
    "Batch ingestion is the process of reading raw data from files or other external batch sources tranforming them into well organizing and fully indexed Druid segment files. \n",
    "\n",
    "This notebook focuses on the basics of batch ingestion in Druid. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 25.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see [Docker for Jupyter Notebook tutorials](https://druid.apache.org/docs/latest/tutorials/tutorial-jupyter-docker.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6aef8-a11d-48d5-bcdc-e6231ba594b7",
   "metadata": {},
   "source": [
    "<details><summary>    \n",
    "<b>Run without Docker Compose</b>    \n",
    "</summary>\n",
    "\n",
    "In order to run this notebook you will need:\n",
    "\n",
    "<b>Required Services</b>\n",
    "* <!-- include list of components needed for notebook, i.e. kafka, druid instance, etc. -->\n",
    "\n",
    "<b>Python packages</b>\n",
    "* druidapi, a [Python client for Apache Druid](https://github.com/apache/druid/blob/master/examples/quickstart/jupyter-notebooks/druidapi/README.md)\n",
    "*  <!-- include any python package dependencies -->\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DRUID_HOST'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdruidapi\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDRUID_HOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     druid_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://router:8888\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DRUID_HOST'"
     ]
    }
   ],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "if (os.environ['DRUID_HOST'] == None):\n",
    "    druid_host=f\"http://router:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "druid = druidapi.jupyter_client(druid_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4a2e1-2fbb-420b-a052-5e80885e79c1",
   "metadata": {},
   "source": [
    "## SQL Based Ingestion\n",
    "\n",
    "Let's start with something simple. With this first example we are simply loading all the data from an external file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd47fcff-9055-4058-852f-6f5c61d07965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wikipedia_events\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.sql.run_task(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776cd84-9f38-40a3-9e47-5e821d15c901",
   "metadata": {},
   "source": [
    "Let's look at the different parts:\n",
    "\n",
    "The initial `REPLACE` or `INSERT` statement tells Druid to execute an ingestion task. `INSERT` is used when appending data, `REPLACE` when replacing data. Both methods work to add data to a new or empty Druid datasource.\n",
    "\n",
    "```\n",
    "REPLACE INTO \"wikipedia_events\" OVERWRITE ALL\n",
    "```\n",
    "\n",
    "The `WITH` clause is used to declare one or more input sources, this could also be placed directly in the `FROM` clause of the final `SELECT`, but this is easier to read:\n",
    "\n",
    "```\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT ...\n",
    "      FROM TABLE (EXTERN ( ... ) )\n",
    ") EXTEND (...)\n",
    "```\n",
    "\n",
    "`EXTERN` supports many batch [input sources](https://druid.apache.org/docs/latest/ingestion/native-batch-input-sources.html) and [formats](https://druid.apache.org/docs/latest/ingestion/data-formats.html). In this case we are using input source type `http` to access a set or `uris` that each contain a data file in the `json` data format. Note that compressed files are allowed and will automatically be decompressed.\n",
    "```\n",
    "FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "```\n",
    "The `EXTEND` clause describes the input schema using SQL data types:\n",
    "```\n",
    "EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR,  ...)\n",
    "```\n",
    "\n",
    "The final SELECT statement defines the transformations and schema of the resulting Druid table. A `__time` column is usually parsed from the source, this expression will be mapped to Druid's primary time partitioning of segments. In this case we specified the `__time` column and ingested the rest of the columns \"AS IS\" using `*`.\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "```\n",
    "\n",
    "The final portion of this ingestion is the `PARTITIONED BY DAY` clause which tells Driud to create a separate set of segments for each day. A `PARTITION BY` clause must be included in all `INSERT`/`REPLACE` statements. The [Time Partitioning notebook](03-time-partitioning.ipynb) reviews this option in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b30164-b3c3-4de6-ba68-d269f0bee2d1",
   "metadata": {},
   "source": [
    "#### Wait for Segment Availibility\n",
    "In the next cell, `sql_wait_until_ready` function is used to pause until all the ingested data is available in the Historical cacheing layer before executing any queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0429f0e-ae98-472a-b686-8aa308985de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.sql.wait_until_ready('wikipedia_events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f811a-7795-4351-bc6b-3abb34da0116",
   "metadata": {},
   "source": [
    "#### Query the Data\n",
    "Let's take a look at the data that was loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2584e65-c952-47f1-a885-2e5d3a4fef2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"druid\"><table>\n",
       "<tr><th>channel</th><th>EXPR\\$1</th></tr>\n",
       "<tr><td>#en.wikipedia</td><td>6650</td></tr>\n",
       "<tr><td>#sh.wikipedia</td><td>3969</td></tr>\n",
       "<tr><td>#sv.wikipedia</td><td>1867</td></tr>\n",
       "<tr><td>#ceb.wikipedia</td><td>1808</td></tr>\n",
       "<tr><td>#de.wikipedia</td><td>1357</td></tr>\n",
       "<tr><td>#fr.wikipedia</td><td>1328</td></tr>\n",
       "<tr><td>#ru.wikipedia</td><td>996</td></tr>\n",
       "<tr><td>#it.wikipedia</td><td>916</td></tr>\n",
       "<tr><td>#es.wikipedia</td><td>708</td></tr>\n",
       "<tr><td>#ja.wikipedia</td><td>472</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT channel, count(*) \n",
    "FROM \"wikipedia_events\" \n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43d5f3-0350-40e6-b5a6-737c610d7562",
   "metadata": {},
   "source": [
    "### Ingesting from Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340d912-2441-48ef-b6ce-e182f57734d4",
   "metadata": {},
   "source": [
    "[Druid Input Sources](https://druid.apache.org/docs/latest/ingestion/native-batch.html#splittable-input-sources) allow you to specify multiple files as input to an ingestion job.\n",
    "\n",
    "In the following example we are using the same file three times as an example of multiple sources. Normally this would be a list of different files to load: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd91d16b-1880-4a4c-8e8a-650075e26015",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wikipedia_events_3\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\"\n",
    "                 ]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('wikipedia_events_3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce84c5-2c1c-4c6a-b9f4-7a22e2216ec5",
   "metadata": {},
   "source": [
    "Let's look at the data now, the quantities are 3x, which is expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f278c0d2-9fd5-4a7b-bb65-a6db7cf98971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"druid\"><table>\n",
       "<tr><th>channel</th><th>EXPR\\$1</th></tr>\n",
       "<tr><td>#en.wikipedia</td><td>19950</td></tr>\n",
       "<tr><td>#sh.wikipedia</td><td>11907</td></tr>\n",
       "<tr><td>#sv.wikipedia</td><td>5601</td></tr>\n",
       "<tr><td>#ceb.wikipedia</td><td>5424</td></tr>\n",
       "<tr><td>#de.wikipedia</td><td>4071</td></tr>\n",
       "<tr><td>#fr.wikipedia</td><td>3984</td></tr>\n",
       "<tr><td>#ru.wikipedia</td><td>2988</td></tr>\n",
       "<tr><td>#it.wikipedia</td><td>2748</td></tr>\n",
       "<tr><td>#es.wikipedia</td><td>2124</td></tr>\n",
       "<tr><td>#ja.wikipedia</td><td>1416</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT channel, count(*) \n",
    "FROM \"wikipedia_events_3\" \n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751316a2-bd0c-43eb-a74f-fed15782f54d",
   "metadata": {},
   "source": [
    "#### Context Parameters\n",
    "Certain aspects of the ingestion can be controlled using [context parameter](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#context-parameters)s. This section discussed two of the commonly used parameters:\n",
    "\n",
    "##### maxNumTasks\n",
    "The Multi-stage Query Framework uses parallel workers to execute each stage of the ingestion process. Each stage creates output partitions that organize the data in preparation for the next stage. \n",
    "\n",
    "The input stage parallelism is limited by the input sources, as each file is processed by one of the workers. While multiple input files are split evenly among parallel worker tasks. As such a single large file cannot be parallelized at this stage. Consider splitting single large files into multiple files to improve parallelism at this stage. \n",
    "\n",
    "After the initial input stage, the level of parallelism of the job will remain consistent and is controlled by the [context parameter](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#context-parameters) `maxNumTasks`\n",
    "\n",
    "If you are running Druid on your laptop, the default configuration only provides 2 worker slots on the Middle Manager, so you can only run with `maxNumTasks=2` resulting in one controller and one worker. If you are using this notebook against a larger Druid cluster, feel free to experiment with higher values. Note that if `maxNumTasks` exceeds the available worker slots, the job will fail with a time out error because it waits for all the worker tasks to be active.\n",
    "\n",
    "##### rowsPerSegment\n",
    "`rowsPerSegment` defaults to 3,000,000. You can adjust it to produce larger or smaller segments. \n",
    "\n",
    "This example shows how to set context parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55313e69-e63e-47d9-917c-2da87926ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wikipedia_events_4\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\"\n",
    "                 ]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)         # init request object\n",
    "request.add_context( 'rowsPerSegment', 20000) # setting it low to produce many segments\n",
    "request.add_context( 'maxNumTasks', 2)        # can't go any higher in test env\n",
    "\n",
    "druid.sql.run_task(request)\n",
    "druid.sql.wait_until_ready('wikipedia_events_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87e942-000a-49d3-b915-4319519c4ed1",
   "metadata": {},
   "source": [
    "With a `rowsPerSegment` of only 20,000, the same ingestion as before produces more segments. Open the [Druid console in the Data Sources view](http://localhost:8888/unified-console.html#datasources) to see the difference in segments between `wikipedia_events_3` and `wikipedia_events_4`.\n",
    "\n",
    "Note that 20,000 is a very low value used to illustrate setting parameters. Normally this value is in the millions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f95a00-cc40-478f-aad9-cb8243f6c363",
   "metadata": {},
   "source": [
    "#### Filter Data During Ingestion\n",
    "\n",
    "In situations where you need data cleansing or your only interested in a subset of the data, the ingestion job can filter the data by simply adding a `WHERE` clause.\n",
    "\n",
    "The example excludes all robotic wikipedia updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3b1fcaa-1e78-475b-b2ef-131aa88ead51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wikipedia_events_only_human\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\"]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "\n",
    "WHERE \"isRobot\"='false'\n",
    "\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('wikipedia_events_only_human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c825d39-0bbd-4cd3-ae8a-760146d9fff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"druid\"><table>\n",
       "<tr><th>isRobot</th><th>channel</th><th>EXPR\\$2</th></tr>\n",
       "<tr><td>false</td><td>#en.wikipedia</td><td>6114</td></tr>\n",
       "<tr><td>false</td><td>#de.wikipedia</td><td>1171</td></tr>\n",
       "<tr><td>false</td><td>#fr.wikipedia</td><td>1148</td></tr>\n",
       "<tr><td>false</td><td>#ru.wikipedia</td><td>930</td></tr>\n",
       "<tr><td>false</td><td>#es.wikipedia</td><td>658</td></tr>\n",
       "<tr><td>false</td><td>#it.wikipedia</td><td>494</td></tr>\n",
       "<tr><td>false</td><td>#ja.wikipedia</td><td>467</td></tr>\n",
       "<tr><td>false</td><td>#zh.wikipedia</td><td>382</td></tr>\n",
       "<tr><td>false</td><td>#pt.wikipedia</td><td>348</td></tr>\n",
       "<tr><td>false</td><td>#nl.wikipedia</td><td>299</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT isRobot, channel, count(*) \n",
    "FROM \"wikipedia_events_only_human\" \n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561d663-9cfe-4a62-80cc-dcbb0ce199ff",
   "metadata": {},
   "source": [
    "#### Transform Data During Ingestion\n",
    "\n",
    "The SQL language provides a rich [set of functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html) that can be applied to input columns to transform the data as it is being ingested. All scalar SQL function are available for normal ingestion. Rollup ingestion is discussed in the [Rollup Notebook](05-rollup.ipynb) which includes the use of aggregate functions at ingestion time as well.\n",
    "\n",
    "Here are some examples of such transformations:\n",
    "\n",
    "##### Time manipulation\n",
    "There are many [time parsing and manipulation functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html#date-and-time-functions) available in Apache Druid. It is common to do some time cleansing/transformation at ingestion. Here are some examples of time manipulation functions:\n",
    "```\n",
    "  TIME_PARSE( \"timestamp\") AS \"__time\",   \n",
    "  TIME_FLOOR( TIME_PARSE( \"timestamp\"), 'P1W') AS \"week_start\",\n",
    "  TIMESTAMPDIFF( DAY,\n",
    "                 TIME_FLOOR( TIME_PARSE( \"timestamp\"), 'P1W'),\n",
    "                 TIME_PARSE( \"timestamp\")\n",
    "               ) AS \"days_since_week_start\"\n",
    "   \n",
    "```\n",
    "\n",
    "##### Use CASE statements to transform data\n",
    "CASE statements can be used to resolve complex logic and prepare columns for certain query patterns. \n",
    "Examples:\n",
    "```\n",
    "  CASE\n",
    "     WHEN UPPER(\"adblock_list\")='NOADBLOCK' THEN 0\n",
    "     ELSE 1\n",
    "  END AS adblock_count,\n",
    "\n",
    "  CASE\n",
    "     WHEN UPPER(\"adblock_list\")='EASYLIST' THEN 1\n",
    "     ELSE 0\n",
    "  END AS easylist_count\n",
    "```\n",
    "The two case statements above are examples of converting a categorical column like `adblock_list` into a numerical column that can be used as a meaningful metric when aggregated across different dimensions to get the count of events that were affected by an ad blocker.\n",
    "\n",
    "##### String manipulation\n",
    "Apache Druid has [string manipulation functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html#string-functions) that can be very useful for transformation during ingestion. Some examples:\n",
    "```\n",
    "  REPLACE(REGEXP_EXTRACT(\"app_version\", '[^\\.]*\\.'),'.','') AS major_version,\n",
    "  STRING_TO_ARRAY(\"app_version\",'\\.') AS version_array,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),3) AS patch_version\n",
    "```\n",
    "The above makes use of regex-based extraction, string replacement, string to array conversion and access to array elements as examples of the string transformation functions available.\n",
    "\n",
    "##### Data Flattening functions\n",
    "If you need to extract fields from nested structures in the input data, JSON_VALUE function can be used to retrieve them and cast them to the desired data type:\n",
    "```\n",
    "  JSON_VALUE(\"event\", '$.percentage' RETURNING BIGINT) as percent_cleared,\n",
    "  JSON_VALUE(\"geo_ip\", '$.city') AS city,\n",
    "```\n",
    "\n",
    "Here's a SQL based ingestion statement that uses all of these examples and a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbddfb3-a482-477a-94a7-18a5539590ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'druid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m      1\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mREPLACE INTO \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkttm_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OVERWRITE ALL\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mWITH \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AS \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124mPARTITIONED BY DAY\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mdruid\u001b[49m\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mrun_task(sql)\n\u001b[1;32m     37\u001b[0m druid\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mwait_until_ready(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkttm_transformation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'druid' is not defined"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"kttm_transformation\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-nested-v2/kttm-nested-v2-2019-08-25.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "    ) EXTEND (\"timestamp\" VARCHAR, \"session\" VARCHAR, \"number\" VARCHAR, \"event\" TYPE('COMPLEX<json>'), \"agent\" TYPE('COMPLEX<json>'), \"client_ip\" VARCHAR, \"geo_ip\" TYPE('COMPLEX<json>'), \"language\" VARCHAR, \"adblock_list\" VARCHAR, \"app_version\" VARCHAR, \"path\" VARCHAR, \"loaded_image\" VARCHAR, \"referrer\" VARCHAR, \"referrer_host\" VARCHAR, \"server_ip\" VARCHAR, \"screen\" VARCHAR, \"window\" VARCHAR, \"session_length\" BIGINT, \"timezone\" VARCHAR, \"timezone_offset\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  session, \n",
    "  number,\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  TIMESTAMPDIFF(DAY, TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1W'), TIME_PARSE(\"timestamp\")) AS days_since_week_start,\n",
    "  TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1W') AS week_start,\n",
    "  TIME_CEIL(TIME_PARSE(\"timestamp\"), 'P1W') AS week_end,\n",
    "  TIME_SHIFT(TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1D'),'P1D', -1) AS start_of_yesterday,\n",
    "  \n",
    "  JSON_VALUE(\"event\", '$.percentage' RETURNING BIGINT) as percent_cleared,\n",
    "  JSON_VALUE(\"geo_ip\", '$.city') AS city,\n",
    "  \n",
    "  CASE WHEN UPPER(\"adblock_list\")='NOADBLOCK' THEN 0 ELSE 1 END AS adblock_count,\n",
    "  CASE WHEN UPPER(\"adblock_list\")='EASYLIST' THEN 1 ELSE 0 END AS easylist_count,\n",
    "  \n",
    "  REPLACE(REGEXP_EXTRACT(\"app_version\", '[^\\.]*\\.'),'.','') AS major_version\n",
    "  -- ,\n",
    "  -- ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),2) AS minor_version,\n",
    "  -- ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),3) AS patch_version\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('kttm_transformation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e787fe-65f5-4110-bc69-ce583285ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT session, count(distinct \"week_start\")\n",
    "FROM \"kttm_transformation\" \n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d47d9-ceaa-4623-9e8e-215b3ef391a4",
   "metadata": {},
   "source": [
    "#### Nested Columns\n",
    "\n",
    "Apache Druid supports ingestion of [nested columns](https://druid.apache.org/docs/latest/querying/nested-columns.html). These are columns whose values  contain nested structures with its own set of fields which in turn are either a literal value or a nested structure as well. Druid can automatically parse nested columns and index all internal fields into columnar form making them all available for fast filtering and aggregation just as if they were top level columns. The schema of the nested columns is automatically discovered and access to the columns is through familiar JSON paths by using the JSON_VALUE function.\n",
    "\n",
    "This example load the Koalas to the Max sample dataset that includes multiple nested columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b89b2f6-cbff-437e-9468-278651947039",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 28\u001b[0m\n\u001b[1;32m      1\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mREPLACE INTO \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkttm_nested\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OVERWRITE ALL\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mWITH \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AS \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mPARTITIONED BY DAY\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mdruid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m druid\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mwait_until_ready(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkttm_nested\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/druidapi/sql.py:761\u001b[0m, in \u001b[0;36mQueryClient.run_task\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientError(resp\u001b[38;5;241m.\u001b[39merror_message)\n\u001b[0;32m--> 761\u001b[0m \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/druidapi/sql.py:571\u001b[0m, in \u001b[0;36mQueryTaskResult.wait_until_done\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03mWait for the task to complete. Raises an error if the task fails.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03mA caller can proceed to do something with the successful result\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03monce this method returns without raising an error.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin():\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DruidError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuery failed: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_message\u001b[49m())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/druidapi/sql.py:529\u001b[0m, in \u001b[0;36mQueryTaskResult.error_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_message\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 529\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err:\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"kttm_nested\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-nested-v2/kttm-nested-v2-2019-08-25.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "    ) EXTEND ( \"timestamp\" VARCHAR, \"session\" VARCHAR, \"number\" VARCHAR, \n",
    "               \"event\" TYPE('COMPLEX<json>'), \n",
    "               \"agent\" TYPE('COMPLEX<json>'), \n",
    "               \"client_ip\" VARCHAR, \n",
    "               \"geo_ip\" TYPE('COMPLEX<json>'), \n",
    "               \"language\" VARCHAR, \"adblock_list\" VARCHAR, \"app_version\" VARCHAR, \n",
    "               \"path\" VARCHAR, \"loaded_image\" VARCHAR, \"referrer\" VARCHAR, \n",
    "               \"referrer_host\" VARCHAR, \"server_ip\" VARCHAR, \n",
    "               \"screen\" VARCHAR, \"window\" VARCHAR, \n",
    "               \"session_length\" BIGINT, \"timezone\" VARCHAR, \n",
    "               \"timezone_offset\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\", *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('kttm_nested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57016cb-ff17-420d-882b-174ec6fd5ab9",
   "metadata": {},
   "source": [
    "As you can see ingesting nested columns is very easy. All you need to do is declare them as `TYPE('COMPLEX<json>')`, include the input field in the main SELECT clause ( `*` = all columns ) and you're done!\n",
    "Take a look at the query example below where we access these nested fields as dimensions, metrics and filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2f86d-f074-4d33-98a0-c6e5f44cc1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a76a4b-d68a-43b3-87eb-239d9d72c042",
   "metadata": {},
   "source": [
    "#### Enhancing Data at Ingestion\n",
    "\n",
    "- Lookups - Broadcast Joins\n",
    "- Fact-to-Fact - Shuffle Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e916694-85aa-45a8-bd7a-ef14cdfa8d52",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Run the following cell to remove all data sources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e5e2b3-0b6f-4d6e-afbd-3ea435dde241",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop('wikipedia_events', True)\n",
    "druid.datasources.drop('wikipedia_events_3', True)\n",
    "druid.datasources.drop('wikipedia_events_4', True)\n",
    "druid.datasources.drop('wikipedia_events_only_human', True)\n",
    "druid.datasources.drop('kttm_transformation', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafa6ce-c3e7-45df-bd7b-4dd66a23498f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
