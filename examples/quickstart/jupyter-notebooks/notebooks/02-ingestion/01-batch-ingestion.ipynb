{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73dbb71f-0253-4940-a7b6-0b808839a82d",
   "metadata": {},
   "source": [
    "# Batch Ingestion\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "  \n",
    "In this notebook we are focusing on [SQL based ingestion](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#sql-reference). \n",
    "\n",
    "While [Native Batch Ingestion](https://druid.apache.org/docs/latest/ingestion/native-batch.html) is still available in Apache Druid 26.0.0, the ease of use and improved performance of SQL based ingestion powered by the Multi-stage Query Framework is the quickly becoming the norm.\n",
    "\n",
    "Batch ingestion is the process of reading raw data from files or other external batch sources tranforming them into well organizing and fully indexed Druid segment files. \n",
    "\n",
    "This notebook focuses on the basics of batch ingestion in Druid. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 26.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see [Docker for Jupyter Notebook tutorials](https://druid.apache.org/docs/latest/tutorials/tutorial-jupyter-docker.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6aef8-a11d-48d5-bcdc-e6231ba594b7",
   "metadata": {},
   "source": [
    "<details><summary>    \n",
    "<b>Run without Docker Compose</b>    \n",
    "</summary>\n",
    "\n",
    "In order to run this notebook you will need:\n",
    "\n",
    "<b>Required Services</b>\n",
    "* <!-- include list of components needed for notebook, i.e. kafka, druid instance, etc. -->\n",
    "\n",
    "<b>Python packages</b>\n",
    "* druidapi, a [Python client for Apache Druid](https://github.com/apache/druid/blob/master/examples/quickstart/jupyter-notebooks/druidapi/README.md)\n",
    "*  <!-- include any python package dependencies -->\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "if (os.environ['DRUID_HOST'] == None):\n",
    "    druid_host=f\"http://router:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "druid = druidapi.jupyter_client(druid_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4a2e1-2fbb-420b-a052-5e80885e79c1",
   "metadata": {},
   "source": [
    "## SQL Based Ingestion\n",
    "\n",
    "Let's start with something simple. With this first example we are loading all the data from an external file into the table called \"wikipedia_events\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47fcff-9055-4058-852f-6f5c61d07965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wikipedia_events\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.sql.run_task(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776cd84-9f38-40a3-9e47-5e821d15c901",
   "metadata": {},
   "source": [
    "Let's look at the different parts:\n",
    "\n",
    "The initial `REPLACE` or `INSERT` statement tells Druid to execute an ingestion task. `INSERT` is used when appending data, `REPLACE` when replacing data. Both methods work when adding data to a new or empty Druid datasource. \n",
    "\n",
    "```\n",
    "REPLACE INTO \"wikipedia_events\" OVERWRITE ALL\n",
    "```\n",
    "\n",
    "The `WITH` clause is used to declare one or more input sources, this could also be placed directly in the `FROM` clause of the final `SELECT`, but this is easier to read:\n",
    "\n",
    "```\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT ...\n",
    "      FROM TABLE (EXTERN ( ... ) )\n",
    ") EXTEND (...)\n",
    "```\n",
    "\n",
    "`EXTERN` supports many batch [input sources](https://druid.apache.org/docs/latest/ingestion/native-batch-input-sources.html) and [formats](https://druid.apache.org/docs/latest/ingestion/data-formats.html). In this case we are using input source type `http` to access a set or `uris` that each contain a data file in the `json` data format. Note that compressed files are allowed and will automatically be decompressed.\n",
    "```\n",
    "FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://druid.apache.org/data/wikipedia.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "```\n",
    "The `EXTEND` clause describes the input schema using SQL data types:\n",
    "```\n",
    "EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR,  ...)\n",
    "```\n",
    "\n",
    "The final SELECT statement defines the transformations and schema of the resulting Druid table. A `__time` column is usually parsed from the source, this expression will be mapped to Druid's primary time partitioning of segments. In this case we specified the `__time` column and ingested the rest of the columns \"AS IS\" using `*`.\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "```\n",
    "\n",
    "The final portion of this ingestion is the `PARTITIONED BY DAY` clause which tells Driud to create a separate set of segments for each day. A `PARTITION BY` clause must be included in all `INSERT`/`REPLACE` statements. The [Time Partitioning notebook](03-time-partitioning.ipynb) reviews this option in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b30164-b3c3-4de6-ba68-d269f0bee2d1",
   "metadata": {},
   "source": [
    "#### Wait for Segment Availibility\n",
    "In the next cell, `sql_wait_until_ready` function is used to pause until all the ingested data is available in the Historical cacheing layer before executing any queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0429f0e-ae98-472a-b686-8aa308985de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.sql.wait_until_ready('wikipedia_events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f811a-7795-4351-bc6b-3abb34da0116",
   "metadata": {},
   "source": [
    "#### Query the Data\n",
    "Let's take a look at the data that was loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2584e65-c952-47f1-a885-2e5d3a4fef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT channel, count(*) num_events\n",
    "FROM \"wikipedia_events\" \n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43d5f3-0350-40e6-b5a6-737c610d7562",
   "metadata": {},
   "source": [
    "### Ingesting from Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340d912-2441-48ef-b6ce-e182f57734d4",
   "metadata": {},
   "source": [
    "[Druid Input Sources](https://druid.apache.org/docs/latest/ingestion/native-batch.html#splittable-input-sources) allow you to specify multiple files as input to an ingestion job.\n",
    "\n",
    "In the following example we are using the same file three times simulating multiple sources. Normally this would be a list of different files to load: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91d16b-1880-4a4c-8e8a-650075e26015",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wikipedia_events_3\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\"\n",
    "                 ]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('wikipedia_events_3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce84c5-2c1c-4c6a-b9f4-7a22e2216ec5",
   "metadata": {},
   "source": [
    "Let's look at the data now. The quantities are 3 times larger as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278c0d2-9fd5-4a7b-bb65-a6db7cf98971",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT channel, count(*) num_events\n",
    "FROM \"wikipedia_events_3\" \n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751316a2-bd0c-43eb-a74f-fed15782f54d",
   "metadata": {},
   "source": [
    "#### Context Parameters\n",
    "Certain aspects of the ingestion can be controlled using [context parameter](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#context-parameters)s. This section discussed two of the commonly used parameters:\n",
    "\n",
    "##### maxNumTasks\n",
    "The Multi-stage Query Framework uses parallel workers to execute each stage of the ingestion process. Each stage creates output partitions that organize the data in preparation for the next stage. \n",
    "\n",
    "The input stage parallelism is limited by the input sources, as each file is processed by one of the workers. While multiple input files are split evenly among parallel worker tasks. As such a single large file cannot be parallelized at this stage. Consider splitting single large files into multiple files to improve parallelism at this stage. \n",
    "\n",
    "After the initial input stage, the level of parallelism of the job will remain consistent and is controlled by the [context parameter](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#context-parameters) `maxNumTasks`\n",
    "\n",
    "If you are running Druid on your laptop, the default configuration only provides 2 worker slots on the Middle Manager, so you can only run with `maxNumTasks=2` resulting in one controller and one worker. If you are using this notebook against a larger Druid cluster, feel free to experiment with higher values. Note that if `maxNumTasks` exceeds the available worker slots, the job will fail with a time out error because it waits for all the worker tasks to be active.\n",
    "\n",
    "##### rowsPerSegment\n",
    "`rowsPerSegment` defaults to 3,000,000. You can adjust it to produce larger or smaller segments. \n",
    "\n",
    "This example shows how to set context parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55313e69-e63e-47d9-917c-2da87926ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wikipedia_events_4\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\",\n",
    "                   \"https://druid.apache.org/data/wikipedia.json.gz\"\n",
    "                 ]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)         # init request object\n",
    "request.add_context( 'rowsPerSegment', 20000) # setting it low to produce many segments\n",
    "request.add_context( 'maxNumTasks', 2)        # can't go any higher in test env\n",
    "\n",
    "druid.sql.run_task(request)\n",
    "druid.sql.wait_until_ready('wikipedia_events_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87e942-000a-49d3-b915-4319519c4ed1",
   "metadata": {},
   "source": [
    "With a `rowsPerSegment` of only 20,000, the same ingestion as before produces more segments. Open the [Druid console in the Data Sources view](http://localhost:8888/unified-console.html#datasources) to see the difference in segments between `wikipedia_events_3` and `wikipedia_events_4`.\n",
    "\n",
    "Note that 20,000 is a very low value used to illustrate setting parameters. Normally this value is in the millions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f95a00-cc40-478f-aad9-cb8243f6c363",
   "metadata": {},
   "source": [
    "#### Filter Data During Ingestion\n",
    "\n",
    "In situations where you need data cleansing or your only interested in a subset of the data, the ingestion job can filter the data by simply adding a `WHERE` clause.\n",
    "\n",
    "The example excludes all robotic wikipedia updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1fcaa-1e78-475b-b2ef-131aa88ead51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wikipedia_events_only_human\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\"]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  *\n",
    "FROM \"ext\"\n",
    "\n",
    "WHERE \"isRobot\"='false'\n",
    "\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('wikipedia_events_only_human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c825d39-0bbd-4cd3-ae8a-760146d9fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT isRobot, channel, count(*) num_events\n",
    "FROM \"wikipedia_events_only_human\" \n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561d663-9cfe-4a62-80cc-dcbb0ce199ff",
   "metadata": {},
   "source": [
    "#### Transform Data During Ingestion\n",
    "\n",
    "The SQL language provides a rich [set of functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html) that can be applied to input columns to transform the data as it is being ingested. All scalar SQL function are available for normal ingestion. Rollup ingestion is discussed in the [Rollup Notebook](05-rollup.ipynb) which includes the use of aggregate functions at ingestion time as well.\n",
    "\n",
    "Here are some examples of such transformations:\n",
    "\n",
    "##### Time manipulation\n",
    "There are many [time parsing and manipulation functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html#date-and-time-functions) available in Apache Druid. It is common to do some time cleansing/transformation at ingestion. Here are some examples of time manipulation functions:\n",
    "```\n",
    "  TIME_PARSE( \"timestamp\") AS \"__time\",   \n",
    "  TIME_FLOOR( TIME_PARSE( \"timestamp\"), 'P1W') AS \"week_start\",\n",
    "  TIMESTAMPDIFF( DAY,\n",
    "                 TIME_FLOOR( TIME_PARSE( \"timestamp\"), 'P1W'),\n",
    "                 TIME_PARSE( \"timestamp\")\n",
    "               ) AS \"days_since_week_start\"\n",
    "   \n",
    "```\n",
    "\n",
    "##### Use CASE statements to transform data\n",
    "CASE statements can be used to resolve complex logic and prepare columns for certain query patterns. \n",
    "Examples:\n",
    "```\n",
    "  CASE\n",
    "     WHEN UPPER(\"adblock_list\")='NOADBLOCK' THEN 0\n",
    "     ELSE 1\n",
    "  END AS adblock_count,\n",
    "\n",
    "  CASE\n",
    "     WHEN UPPER(\"adblock_list\")='EASYLIST' THEN 1\n",
    "     ELSE 0\n",
    "  END AS easylist_count\n",
    "```\n",
    "The two case statements above are examples of converting a categorical column like `adblock_list` into a numerical column that can be used as a meaningful metric when aggregated across different dimensions to get the count of events that were affected by an ad blocker.\n",
    "\n",
    "##### String manipulation\n",
    "Apache Druid has [string manipulation functions](https://druid.apache.org/docs/latest/querying/sql-scalar.html#string-functions) that can be very useful for transformation during ingestion. Some examples:\n",
    "```\n",
    "  REPLACE(REGEXP_EXTRACT(\"app_version\", '[^\\.]*\\.'),'.','') AS major_version,\n",
    "  STRING_TO_ARRAY(\"app_version\",'\\.') AS version_array,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),3) AS patch_version\n",
    "```\n",
    "The above makes use of regex-based extraction, string replacement, string to array conversion and access to array elements as examples of the string transformation functions available.\n",
    "\n",
    "##### Data Flattening functions\n",
    "If you need to extract fields from nested structures in the input data, JSON_VALUE function can be used to retrieve them and cast them to the desired data type:\n",
    "```\n",
    "  JSON_VALUE(\"event\", '$.percentage' RETURNING BIGINT) as percent_cleared,\n",
    "  JSON_VALUE(\"geo_ip\", '$.city') AS city,\n",
    "```\n",
    "\n",
    "Here's a SQL based ingestion statement that uses all of these examples and a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbddfb3-a482-477a-94a7-18a5539590ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"kttm_transformation\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-nested-v2/kttm-nested-v2-2019-08-25.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "    ) EXTEND (\"timestamp\" VARCHAR, \"session\" VARCHAR, \"number\" VARCHAR, \"event\" TYPE('COMPLEX<json>'), \"agent\" TYPE('COMPLEX<json>'), \"client_ip\" VARCHAR, \"geo_ip\" TYPE('COMPLEX<json>'), \"language\" VARCHAR, \"adblock_list\" VARCHAR, \"app_version\" VARCHAR, \"path\" VARCHAR, \"loaded_image\" VARCHAR, \"referrer\" VARCHAR, \"referrer_host\" VARCHAR, \"server_ip\" VARCHAR, \"screen\" VARCHAR, \"window\" VARCHAR, \"session_length\" BIGINT, \"timezone\" VARCHAR, \"timezone_offset\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  session, \n",
    "  number,\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\",\n",
    "  TIMESTAMPDIFF(DAY, TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1W'), TIME_PARSE(\"timestamp\")) AS days_since_week_start,\n",
    "  TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1W') AS week_start,\n",
    "  TIME_CEIL(TIME_PARSE(\"timestamp\"), 'P1W') AS week_end,\n",
    "  TIME_SHIFT(TIME_FLOOR(TIME_PARSE(\"timestamp\"), 'P1D'),'P1D', -1) AS start_of_yesterday,\n",
    "  \n",
    "  JSON_VALUE(\"event\", '$.percentage' RETURNING BIGINT) as percent_cleared,\n",
    "  JSON_VALUE(\"geo_ip\", '$.city') AS city,\n",
    "  \n",
    "  CASE WHEN UPPER(\"adblock_list\")='NOADBLOCK' THEN 0 ELSE 1 END AS adblock_count,\n",
    "  CASE WHEN UPPER(\"adblock_list\")='EASYLIST' THEN 1 ELSE 0 END AS easylist_count,\n",
    "  \n",
    "  REPLACE(REGEXP_EXTRACT(\"app_version\", '[^\\.]*\\.'),'.','') AS major_version,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),2) AS minor_version,\n",
    "  ARRAY_ORDINAL(STRING_TO_ARRAY(\"app_version\",'\\.'),3) AS patch_version,\n",
    "  session_length\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('kttm_transformation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e787fe-65f5-4110-bc69-ce583285ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what time of day shows the highest user activity\n",
    "druid.display.sql(\"\"\"\n",
    "SELECT EXTRACT( HOUR FROM \"__time\") time_hour, city, count(distinct \"session\") session_count\n",
    "FROM \"kttm_transformation\" \n",
    "WHERE \"city\" IS NOT NULL\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC \n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d47d9-ceaa-4623-9e8e-215b3ef391a4",
   "metadata": {},
   "source": [
    "#### Nested Columns\n",
    "\n",
    "Apache Druid supports ingestion of [nested columns](https://druid.apache.org/docs/latest/querying/nested-columns.html). These are columns that contain nested structures with their own set of fields which in turn are either have literal values or nested structures as well. Druid can automatically parse nested columns and index all internal fields into columnar form. This makes all fields in the JSON objects available for fast filtering and aggregation just as if they were top level columns. The schema of the nested columns is automatically discovered and access to the columns is through familiar JSON paths by using the JSON_VALUE function.\n",
    "\n",
    "This example loads the Koalas to the Max sample dataset that includes multiple nested columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89b2f6-cbff-437e-9468-278651947039",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"kttm_nested\" OVERWRITE ALL\n",
    "WITH \"ext\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-nested-v2/kttm-nested-v2-2019-08-25.json.gz\"]}',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    "    ) EXTEND ( \"timestamp\" VARCHAR, \"session\" VARCHAR, \"number\" VARCHAR, \n",
    "               \"event\" TYPE('COMPLEX<json>'), \n",
    "               \"agent\" TYPE('COMPLEX<json>'), \n",
    "               \"client_ip\" VARCHAR, \n",
    "               \"geo_ip\" TYPE('COMPLEX<json>'), \n",
    "               \"language\" VARCHAR, \"adblock_list\" VARCHAR, \"app_version\" VARCHAR, \n",
    "               \"path\" VARCHAR, \"loaded_image\" VARCHAR, \"referrer\" VARCHAR, \n",
    "               \"referrer_host\" VARCHAR, \"server_ip\" VARCHAR, \n",
    "               \"screen\" VARCHAR, \"window\" VARCHAR, \n",
    "               \"session_length\" BIGINT, \"timezone\" VARCHAR, \n",
    "               \"timezone_offset\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"timestamp\") AS \"__time\", *\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('kttm_nested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57016cb-ff17-420d-882b-174ec6fd5ab9",
   "metadata": {},
   "source": [
    "As you can see, ingesting nested columns is straight forward. All you need to do is declare them as `TYPE('COMPLEX<json>')`, include the input field in the main SELECT clause ( `*` = all columns ) and you're done!\n",
    "Take a look at the query example below where we access these nested fields as dimensions we can group by, metrics we can aggregate and filters we can apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2f86d-f074-4d33-98a0-c6e5f44cc1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT\n",
    "  JSON_VALUE(\"agent\", '$.browser') AS \"browser\",\n",
    "  SUM( JSON_VALUE(\"event\", '$.layer' RETURNING BIGINT) ) AS \"sum_layers\",\n",
    "  COUNT( DISTINCT JSON_VALUE(\"geo_ip\", '$.city') ) AS \"unique_cities\"\n",
    "\n",
    "FROM \"kttm_nested\"\n",
    "\n",
    "WHERE JSON_VALUE(\"geo_ip\", '$.continent') = 'South America'\n",
    "\n",
    "GROUP BY 1 \n",
    "ORDER BY 3 DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb243a-276b-4227-a485-f398ba078f09",
   "metadata": {},
   "source": [
    "Since nested columns could have different fields from row to row or as their schema changes over time, you can inspect the fields that have been discovered during ingestion using the `JSON_PATHS` function on nested columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ef331-b119-4182-aed1-e8fa2e777650",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT 'agent' as nested_column, STRING_AGG( DISTINCT JSON_PATHS(\"agent\"), ', ') paths FROM \"kttm_nested\"\n",
    "UNION ALL\n",
    "SELECT 'event', STRING_AGG( DISTINCT JSON_PATHS(\"event\"), ', ') paths FROM \"kttm_nested\"\n",
    "UNION ALL\n",
    "SELECT 'geo_ip', STRING_AGG( DISTINCT JSON_PATHS(\"geo_ip\"), ', ') paths FROM \"kttm_nested\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a76a4b-d68a-43b3-87eb-239d9d72c042",
   "metadata": {},
   "source": [
    "#### Enhancing Data at Ingestion\n",
    "\n",
    "Adding dimensions and metrics to your data can enhance its analytic value. It's common, for example, to add product categorization, user demographics or additional location based metrics to Retail clickstream or POS data. In IoT scenarios, additional info like metric type (temperature, pressure, flow, etc) for a particular device is common, the device can be associated to a specific industrial process and grouped into components and subcomponents of the overall system being monitored are very useful in determining subsystem anomalies. \n",
    "\n",
    "Lookups and joins can be used at query time to enhance data in this fashion. But there is a performance penalty when using lookups and even more penalty with joins at query time. So in the interest of achieving fast analytic queries, joins can be applied at ingestion time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea89ec-e119-4478-b56e-13f056886dee",
   "metadata": {},
   "source": [
    "##### Broadcast Joins - Small Lookups Joins\n",
    "\n",
    "SQL Based Ingestion can process joins efficiently during ingestion using either broadcast or sort merge joins. Broadcast is the default method, in which the right table of the join is broadcast in its entirety to all workers involved in the ingestion. The content of the lookup is kept within each worker's memory in order to process the join. You'll need to take care that the whole set of lookup tables joined in this fashion for a given ingestion will fit within the heap of each worker JVM.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd142c44-9f9e-42c8-be62-740942e2c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"kttm_enhanced\" OVERWRITE ALL\n",
    "WITH\n",
    "kttm_data AS (\n",
    "  SELECT * \n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-v2/kttm-v2-2019-08-25.json.gz\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"timestamp\" VARCHAR, \"agent_category\" VARCHAR, \"agent_type\" VARCHAR, \"browser\" VARCHAR, \"browser_version\" VARCHAR, \"city\" VARCHAR, \"continent\" VARCHAR, \"country\" VARCHAR, \"version\" VARCHAR, \"event_type\" VARCHAR, \"event_subtype\" VARCHAR, \"loaded_image\" VARCHAR, \"adblock_list\" VARCHAR, \"forwarded_for\" VARCHAR, \"language\" VARCHAR, \"number\" VARCHAR, \"os\" VARCHAR, \"path\" VARCHAR, \"platform\" VARCHAR, \"referrer\" VARCHAR, \"referrer_host\" VARCHAR, \"region\" VARCHAR, \"remote_address\" VARCHAR, \"screen\" VARCHAR, \"session\" VARCHAR, \"session_length\" BIGINT, \"timezone\" VARCHAR, \"timezone_offset\" VARCHAR, \"window\" VARCHAR)\n",
    "),\n",
    "country_lookup AS (\n",
    "  SELECT * \n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/lookup/countries.tsv\"]}',\n",
    "      '{\"type\":\"tsv\",\"findColumnsFromHeader\":true}'\n",
    "    )\n",
    "  ) EXTEND (\"Country\" VARCHAR, \"Capital\" VARCHAR, \"ISO3\" VARCHAR, \"ISO2\" VARCHAR)\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  TIME_PARSE(kttm_data.\"timestamp\") AS __time,\n",
    "  kttm_data.\"session\",\n",
    "  kttm_data.\"agent_category\",\n",
    "  kttm_data.\"agent_type\",\n",
    "  kttm_data.\"browser\",\n",
    "  kttm_data.\"browser_version\",\n",
    "  kttm_data.\"language\",\n",
    "  kttm_data.\"os\",\n",
    "  kttm_data.\"city\",\n",
    "  kttm_data.\"country\",\n",
    "  country_lookup.\"Capital\" AS \"capital\",\n",
    "  country_lookup.\"ISO3\" AS \"iso3\",\n",
    "  kttm_data.\"forwarded_for\" AS \"ip_address\",\n",
    "  kttm_data.\"session_length\",\n",
    "  kttm_data.\"event_type\"\n",
    "FROM kttm_data\n",
    "LEFT JOIN country_lookup ON country_lookup.Country = kttm_data.country\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "druid.sql.run_task(sql)\n",
    "druid.sql.wait_until_ready('kttm_enhanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6e5b2-f3ce-41a5-85cb-242a15edac7e",
   "metadata": {},
   "source": [
    "Data for both sources `kttm_data` and `country_lookup` are obtained from external sources:\n",
    "```\n",
    "WITH\n",
    "kttm_data AS\n",
    "(\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "               '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/kttm-v2/kttm-v2-2019-08-25.json.gz\"]}',\n",
    "               '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"timestamp\" VARCHAR, \"agent_category\" VARCHAR, ...)\n",
    "),\n",
    "country_lookup AS\n",
    "(\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/lookup/countries.tsv\"]}',\n",
    "      '{\"type\":\"tsv\",\"findColumnsFromHeader\":true}'\n",
    "    )\n",
    "  ) EXTEND (\"Country\" VARCHAR, \"Capital\" VARCHAR, \"ISO3\" VARCHAR, \"ISO2\" VARCHAR)\n",
    ")\n",
    "```\n",
    "\n",
    "Columns from both tables can be used in the SELECT expressions using the alias `country_lookup` to reference any joined column:\n",
    "```\n",
    "  kttm_data.\"country\",\n",
    "  country_lookup.\"Capital\" AS \"capital\",\n",
    "  country_lookup.\"ISO3\" AS \"iso3\"\n",
    "```\n",
    "\n",
    "The join is specified in the FROM clause:\n",
    "```\n",
    "FROM kttm_data\n",
    "LEFT JOIN country_lookup ON country_lookup.Country = kttm_data.country\n",
    "```\n",
    "LEFT JOIN insured that we get all the rows from `kttm_data` source, an INNER JOIN would exclude rows from `kttm_data` if the value for `kttm_data.country` is not present in `country_lookup.Country`. \n",
    "Since we did not set any context parameters, the join is processed as a broadcast join. The first table in the FROM clause is the distributed table and all other joined tables will be shipped to the workers to execute the join.\n",
    "\n",
    "Take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb13e7-9071-46b6-8088-810c72257073",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT\n",
    "  \"iso3\" AS \"country_code\", \n",
    "  \"capital\",\n",
    "  count( DISTINCT \"ip_address\" ) distinct_users, \n",
    "  MIN(\"session_length\")/1000 fastest_session_ms,\n",
    "  MAX(\"session_length\")/1000 slowest_session_ms\n",
    "FROM \"kttm_enhanced\"\n",
    "WHERE \"event_type\"='LayerClear'\n",
    "GROUP BY 1,2\n",
    "ORDER BY 3 DESC\n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa697e8-bec5-4d77-a424-5ee37a86b859",
   "metadata": {},
   "source": [
    "##### Shuffle Joins - Large Lookup to Fact Joins\n",
    "\n",
    "Druid 26.0.0 introduced [shuffle joins with SQL Based Ingestion](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#sort-merge).\n",
    "This is the ability to join large tables to other large tables without fully loading either one into memory. Both sources involved in the join are scanned in parallel across all workers, the intermediate data for both sources is then redistributed among the workers based on the join column(s) such that rows from both sources with the same values end up in the same worker.\n",
    "\n",
    "![](assets/shuffle-join.png)\n",
    "\n",
    "In order to use shuffle join the query context must include:\n",
    "```\n",
    "{\n",
    "   \"sqlJoinAlgorithm\":\"sortMerge\"\n",
    "}\n",
    "```\n",
    "\n",
    "Given that this example is meant to run on the local docker compose deployment, we can't really use two very large tables, so let's try it out with small sources and just pretend they are big. We'll use the `wikipedia` sample data and join it with `wiki_users` profile data. But first we'll need to create the users table because at the time of this writing there wasn't a matching user source handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995aeb98-b5c3-44e3-a4e5-2e13eb3a248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we don't have a source for wiki_users let's\n",
    "# create one using in-database transformation that\n",
    "# generates user profiles from the wikipedia data by\n",
    "# - grouping on \"user\"\n",
    "# - injecting variability into the \"group\" column using modulus of the __time of the event\n",
    "# - \"edits\" represent the number of edits done by the user, so just count the number of events\n",
    "# - calculate the registration time using the minimum __time from events and adjusting it some variable years back in time\n",
    "# - determine the preferred language of the user based on their earliest channel edit\n",
    "\n",
    "sql = '''\n",
    "REPLACE INTO \"wiki_users\" OVERWRITE ALL\n",
    "SELECT \n",
    "  \"user\", \n",
    "  EARLIEST(\n",
    "    CASE \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),5) > 3 THEN 'Reviewers' \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),17) > 13 THEN 'Patrollers' \n",
    "      WHEN  MOD(TIMESTAMP_TO_MILLIS(__time),23) > 21 THEN 'Bots'\n",
    "      ELSE 'Autoconfirmed'\n",
    "    END,\n",
    "    1024\n",
    "  ) AS \"group\",\n",
    "  count(*) \"edits\",\n",
    "  TIME_SHIFT(MIN(__time), 'P1Y', -1 * MOD(MIN(EXTRACT (MICROSECOND FROM __time)),20) ) AS \"registered_at_ms\",\n",
    "  EARLIEST(SUBSTRING(\"channel\", 2, 2), 1024) AS \"language\"\n",
    "FROM \"wikipedia\"\n",
    "GROUP BY 1\n",
    "PARTITIONED BY ALL\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)              # init request object\n",
    "request.add_context( 'finalizeAggregations', True) # EARLIEST functions will store a partial aggregation otherwise\n",
    "request.add_context( 'maxNumTasks', 2)             # can't go any higher in test env\n",
    "\n",
    "druid.sql.run_task(request)\n",
    "druid.sql.wait_until_ready('wiki_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ff8bb-2a1d-4445-be59-9d2460397875",
   "metadata": {},
   "source": [
    "Okay, now we are ready to try an ingestion using the sortMerge join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcc958-357c-4b64-b19f-bcbf0fc6941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"wiki_merge\" OVERWRITE ALL\n",
    "WITH \"wikidata\" AS \n",
    "(\n",
    "    SELECT *\n",
    "    FROM TABLE(\n",
    "      EXTERN(\n",
    "        '{\"type\":\"http\",\n",
    "          \"uris\":[ \"https://druid.apache.org/data/wikipedia.json.gz\"]\n",
    "         }',\n",
    "        '{\"type\":\"json\"}'\n",
    "      )\n",
    ") EXTEND (\"isRobot\" VARCHAR, \"channel\" VARCHAR, \"timestamp\" VARCHAR, \"flags\" VARCHAR, \"isUnpatrolled\" VARCHAR, \"page\" VARCHAR, \"diffUrl\" VARCHAR, \"added\" BIGINT, \"comment\" VARCHAR, \"commentLength\" BIGINT, \"isNew\" VARCHAR, \"isMinor\" VARCHAR, \"delta\" BIGINT, \"isAnonymous\" VARCHAR, \"user\" VARCHAR, \"deltaBucket\" BIGINT, \"deleted\" BIGINT, \"namespace\" VARCHAR, \"cityName\" VARCHAR, \"countryName\" VARCHAR, \"regionIsoCode\" VARCHAR, \"metroCode\" BIGINT, \"countryIsoCode\" VARCHAR, \"regionName\" VARCHAR))\n",
    "SELECT \n",
    "  TIME_PARSE(d.\"timestamp\") as \"__time\",\n",
    "  d.\"isRobot\", \n",
    "  d.\"channel\" , \n",
    "  d.\"timestamp\" , \n",
    "  d.\"flags\" , \n",
    "  d.\"isUnpatrolled\" , \n",
    "  d.\"page\" , \n",
    "  d.\"diffUrl\" , \n",
    "  d.\"added\" , \n",
    "  d.\"comment\" , \n",
    "  d.\"commentLength\" , \n",
    "  d.\"isNew\" , \n",
    "  d.\"isMinor\" , \n",
    "  d.\"delta\" , \n",
    "  d.\"isAnonymous\" , \n",
    "  d.\"user\" , \n",
    "  d.\"deltaBucket\" , \n",
    "  d.\"deleted\" , \n",
    "  d.\"namespace\" , \n",
    "  d.\"cityName\" , \n",
    "  d.\"countryName\" , \n",
    "  d.\"regionIsoCode\" , \n",
    "  d.\"metroCode\" , \n",
    "  d.\"countryIsoCode\" , \n",
    "  d.\"regionName\", \n",
    "  u.\"group\" AS \"user_group\",\n",
    "  u.\"edits\" AS \"user_edits\",\n",
    "  u.\"registered_at_ms\" AS \"user_registration_epoch\",\n",
    "  u.\"language\" AS \"user_language\"\n",
    "FROM \"wikidata\" AS d \n",
    "  LEFT JOIN \"wiki_users\" AS u ON u.\"user\"=d.\"user\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "request = druid.sql.sql_request( sql)                 # init request object\n",
    "request.add_context( 'sqlJoinAlgorithm', 'sortMerge') # use sortMerge to join the sources\n",
    "request.add_context( 'maxNumTasks', 2)                # can't go any higher in test env\n",
    "\n",
    "druid.sql.run_task(request)\n",
    "druid.sql.wait_until_ready('wiki_merge')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307374a4-635a-47bd-baf6-054373b3aa0c",
   "metadata": {},
   "source": [
    "... and a final query to look at the newly joined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbc94d-c835-4dd5-a8e6-c499d35e2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql(\"\"\"\n",
    "SELECT \"user_group\",\n",
    "  count( DISTINCT \"user\") \"distinct_users\",\n",
    "  sum(\"user_edits\") \"total_activity\"\n",
    "FROM \"wiki_merge\"\n",
    "GROUP BY 1\n",
    "ORDER BY 1, 3 DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b136d9-060e-4f5b-b0ca-bf58f626ba5c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Druid's [SQL Based ingestion](https://druid.apache.org/docs/latest/multi-stage-query/index.html) enables scalable batch ingestion from a large variety of [data sources](https://druid.apache.org/docs/latest/ingestion/native-batch-input-sources.html) and [formats](https://druid.apache.org/docs/latest/ingestion/data-formats.html). The familiarity and expressivity of SQL enables users to quickly transform, filter and generally enhance data directly in the cluster.\n",
    "\n",
    "SQL Based ingestion is also faster than native batch ingestion so it is quickly becoming the best practice with Apache Druid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e916694-85aa-45a8-bd7a-ef14cdfa8d52",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Run the following cell to remove all data sources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5e2b3-0b6f-4d6e-afbd-3ea435dde241",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.datasources.drop('wikipedia_events', True)\n",
    "druid.datasources.drop('wikipedia_events_3', True)\n",
    "druid.datasources.drop('wikipedia_events_4', True)\n",
    "druid.datasources.drop('wikipedia_events_only_human', True)\n",
    "druid.datasources.drop('kttm_transformation', True)\n",
    "druid.datasources.drop('kttm_nested', True)\n",
    "druid.datasources.drop('kttm_enhanced', True)\n",
    "druid.datasources.drop('wiki_users', True)\n",
    "druid.datasources.drop('wiki_merge', True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
