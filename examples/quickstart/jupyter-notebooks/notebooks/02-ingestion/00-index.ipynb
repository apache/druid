{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7244349c-5e4d-4f43-bf52-fba123bd9bde",
   "metadata": {},
   "source": [
    "# Ingesting Data in Apache Druid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a30a0746-9db4-4f19-a872-1fea98f0c413",
   "metadata": {},
   "source": [
    "This notebook is an introduction to data ingestion in Apache Druid and serves as an index into the other notebooks in this folder. \n",
    "\n",
    "Apache Druid is a database specialized in serving fast analytic applications with a goal of subsecond response times on queries. It is designed to execute efficient ad-hoc filtering and aggregation on large datasets ranging from terabytes to petabytes. Its efficiency in the use of resources means that it also supports high concurrency. Its ability to query data as it is ingested from streaming platform enables real-time analytics. \n",
    "\n",
    "In order to use these capabilities the data must first be ingested. Ingestion is the process of taking raw data, organizing it and converting it into Druid's segment format. The segment format and its distribution through a cluster are what enable fast filtering and aggregation even as the data volume scales. \n",
    "\n",
    "This set of notebooks focuses on the different forms of ingestion and provide runnable examples. The data used in these notebooks is small because we're assuming that you are running using the docker-compose driven cluster which is all running on your personal computer. \n",
    "\n",
    "## The Segment Format\n",
    "\n",
    "The Druid Segment is a column oriented data structure. This makes analytic queries faster, since only a small subset of the data is read given that only columns involved in the query are needed. Dimension columns use a sorted dictionary that helps search the segment and compress the data in the column. Dimensions have row indexes for each of the dictionary value for fast filtering and bitmap indices to resolve complex multi-value and multi column filter criteria by using bit-wise operations. Learn more about the [Druid Segment here](https://druid.apache.org/docs/latest/design/segments.html).\n",
    "\n",
    "\n",
    "### Segment Size \n",
    "At query time, each segment is processed by a single thread. Many threads process segments in parallel. There is some overhead associated with processing each segment. With a large number of small segments, more overhead is involved to resolve a query that needs them. With very a small number of very large segments, there is a reduction in parallelism. So there is a balance between the size and quantity of the segments for a given dataset. Apache Druid engineers have found that a good starting point for segment size is aproximately 5 million rows and somewhere between 500-700 MB.\n",
    "\n",
    "See the Time partitioning and Clustering sections below for details on how to achieve the ideal segment size. Read more about [segment size optimization here](https://druid.apache.org/docs/latest/operations/segment-optimization.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb56e3-e6fb-4642-b741-354826acc3e4",
   "metadata": {},
   "source": [
    "## Batch & Real-time\n",
    "Data can be ingested in batched or in real-time... describe scenarios where one or the other is used with advantages of each.\n",
    "\n",
    "Try them out:\n",
    "- [Batch Ingestion Notebook](01-batch-ingestion.ipynb)\n",
    "- [Streaming Ingestion Notebook](02-streaming-ingestion.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e171e-7667-4305-937b-53dd4f6926ca",
   "metadata": {},
   "source": [
    "## Time Partitioning\n",
    "\n",
    "talk about how the data is organized by time, using time buckets\n",
    "\n",
    "- [Time Partitioning Notebook](03-time-partitioning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb634267-1f4a-456b-ad34-903b19569685",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Talk about multiple segments within a time partition and how they can be organized for pruning. Talk about situations when clustering is useful.\n",
    "- [Clustering Notebook](04-clustering.ipynb) notebook link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd82ad-f2a0-41cd-99c7-d027a31f765d",
   "metadata": {},
   "source": [
    "## Rollup\n",
    "Talk about the concept of rollup at ingestion and how it improves query performance and higher concurrency. \n",
    "- [Rollup Notebook](05-rollup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df36a84-2053-4785-a9cc-c732ee3ca012",
   "metadata": {},
   "source": [
    "## Compaction\n",
    "Compaction and auto-compaction, when they are needed and why they are needed.\n",
    "- [Compaction Notebook](06-compaction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f72140-231b-415f-813b-7daa62c1041a",
   "metadata": {},
   "source": [
    "## Schema Auto-Discovery\n",
    "Introduce schema auto-discovery function and describe scenarios where it is desirable and when it is not.\n",
    "- [Auto Discovery Notebook](07-auto-discovery.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247090d-0404-4040-a41d-7d145a7a4b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
