{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e07b3f5-d919-4179-91a1-0f6b66c42757",
   "metadata": {},
   "source": [
    "# Data Generator Server\n",
 "The default Docker Compose deployment includes a data generation service created from the published Docker image at `imply/datagen:latest`. \n",
    "This image is built by the project https://github.com/implydata/druid-datagenerator. \n",
    "\n",
    "To interact with the data generation service, you can use the rest client provided in the druidapi python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84766c7-c6a5-4496-91a3-abdb8ddd2375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  .druid table {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "\n",
       "  .druid th, .druid td {\n",
       "    padding: 4px 1em ;\n",
       "    text-align: left;\n",
       "  }\n",
       "\n",
       "  td.druid-right, th.druid-right {\n",
       "    text-align: right;\n",
       "  }\n",
       "\n",
       "  td.druid-center, th.druid-center {\n",
       "    text-align: center;\n",
       "  }\n",
       "\n",
       "  .druid .druid-left {\n",
       "    text-align: left;\n",
       "  }\n",
       "\n",
       "  .druid-alert {\n",
       "    font-weight: bold;\n",
       "  }\n",
       "\n",
       "  .druid-error {\n",
       "    color: red;\n",
       "  }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "# Datagen client \n",
    "datagen = druidapi.rest.DruidRestClient(\"http://datagen:9999\")\n",
    "\n",
    "if (os.environ['DRUID_HOST'] == None):\n",
    "    druid_host=f\"http://router:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "# Druid client\n",
    "druid = druidapi.jupyter_client(druid_host)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54af617-0998-4010-90c3-9b5a38a09a5f",
   "metadata": {},
   "source": [
    "### List available configurations\n",
    "Use /list API to get the data generator's available configuration values with pre-defined data generator schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba6a80a-c49b-4abf-943b-9dad82f2ae13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clickstream/clickstream.json',\n",
       " 'clickstream/users_init.json',\n",
       " 'clickstream/users_changes.json',\n",
       " 'social/social_posts.json',\n",
       " 'examples/langmap.json',\n",
       " 'examples/missing.json',\n",
       " 'examples/simple.json',\n",
       " 'examples/list.json',\n",
       " 'examples/deepthought.json',\n",
       " 'examples/variable.json',\n",
       " 'examples/nulls.json',\n",
       " 'examples/counter.json',\n",
       " 'examples/object.json',\n",
       " 'examples/types.json']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(datagen.get(f\"/list\", require_ok=False).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88a3b7-60da-405d-bcf4-fb4affcfe973",
   "metadata": {},
   "source": [
    "### Generate a data file for back filling history\n",
    "When generating a file for backfill purposes, you can select the start time and the duration of the simulation.\n",
    "This example shows how to do that:\n",
    "- `\"target\"` specifies `\"type\":\"file\"` which generates a data file.\n",
    "- `\"path\"` within the `\"target\"` is only a filename, it will ignore any path specified on the file.\n",
    "- The data generator simulates time when you specify a start time in the `\"time_type\"` property and a duration in the `\"time\"` property.\n",
    "- `\"concurrency\"` indicates the maximum number of entities used concurrently to generate events. Each entity is a separate state machine that simulates things like user sessions, IoT devices, or other concurrent sources of event data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811ff58f-75af-4092-a08d-5e07a51592ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to generate history at 2023-08-04T15:31:00.001.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datagen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# this request if generating a data file at on the datagen server\u001b[39;00m\n\u001b[1;32m     15\u001b[0m datagen_request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: job_name,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclicks.json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: startDateTime\n\u001b[1;32m     22\u001b[0m }\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mdatagen\u001b[49m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/start\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(datagen_request), headers\u001b[38;5;241m=\u001b[39mheaders, require_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datagen' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# determine start time, in this example we are starting one hour ago \n",
    "startDateTime = (datetime.now() - timedelta(hours = 1)).strftime('%Y-%m-%dT%H:%M:%S.001')\n",
    "print(f\"Starting to generate history at {startDateTime}.\")\n",
    "\n",
    "job_name=\"gen_clickstream1\"\n",
    "\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# this request if generating a data file at on the datagen server\n",
    "datagen_request = {\n",
    "    \"name\": job_name,\n",
    "    \"target\": { \"type\": \"file\", \"path\":\"clicks.json\"},\n",
    "    \"config_file\": \"clickstream/clickstream.json\", \n",
    "    \"time\": \"1h\",\n",
    "    \"concurrency\":100,\n",
    "    \"time_type\": startDateTime\n",
    "}\n",
    "response = datagen.post(\"/start\", json.dumps(datagen_request), headers=headers, require_ok=False)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407d1d9-3f01-4128-a014-6a5f371c25a5",
   "metadata": {},
   "source": [
    "### Display jobs\n",
    "Use the /jobs API to get the current jobs and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de698c5-bcf4-40c7-b295-728fb54d1f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'sample_custom',\n",
       "  'config_file': '__custom__',\n",
       "  'target': {'type': 'kafka',\n",
       "   'endpoint': 'kafka:9092',\n",
       "   'topic': 'custom_data'},\n",
       "  'active_sessions': 10,\n",
       "  'total_records': 35955,\n",
       "  'start_time': '2023-08-03 00:36:40',\n",
       "  'run_time': 3600.0,\n",
       "  'status': 'COMPLETE'},\n",
       " {'name': 'social_stream',\n",
       "  'config_file': 'social/social_posts.json',\n",
       "  'target': {'type': 'kafka',\n",
       "   'endpoint': 'kafka:9092',\n",
       "   'topic': 'social_media'},\n",
       "  'active_sessions': 0,\n",
       "  'total_records': 50022,\n",
       "  'start_time': '2023-08-03 01:19:03',\n",
       "  'run_time': 140701.691052,\n",
       "  'status': 'COMPLETE'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(datagen.get(f\"/jobs\").json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ebed0-34a1-4ad2-909d-69b8b27c3046",
   "metadata": {},
   "source": [
    "### Get status of a job\n",
    "Use the /jobs API to get the current jobs and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debce4f8-9c16-476c-9593-21ec984985d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(datagen.get(f\"/status/{job_name}\", require_ok=False).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef818d78-6aa6-4d38-8a43-83416aede96f",
   "metadata": {},
   "source": [
    "### Stop a job\n",
    "Use the /stop/\\<job_name> API to stop a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631b8b8-d3d6-4803-9162-587f440d2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(datagen.post(f\"/stop/{job_name}\", '').json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8dc7d3-64e5-41e3-8c28-c5f19c0536f5",
   "metadata": {},
   "source": [
    "### List files created on datagen server\n",
    "Use the /files API to list files available on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ee36bd-2d2b-4904-9987-10636cf52aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_data.json',\n",
       " 'clicks.json',\n",
       " 'clicks1.json',\n",
       " 'clicks2.json',\n",
       " 'clickstream_data.json',\n",
       " 'clicks3.json']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(datagen.get(f\"/files\", '').json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef9edb-98e2-45b4-88e8-578703faedc1",
   "metadata": {},
   "source": [
    "### Batch Loading of Generated Files\n",
    "Use a [Druid HTTP input source](https://druid.apache.org/docs/latest/ingestion/native-batch-input-sources.html#http-input-source) in the [EXTERN function](https://druid.apache.org/docs/latest/multi-stage-query/reference.html#extern-function) of a [SQL Based ingestion](https://druid.apache.org/docs/latest/multi-stage-query/index.html) to load generated files.\n",
    "The files can be accessed by name using the `http://datagen:9999/file/<name of the file>` or if ingesting into a Druid instance outside of docker, but still running locally, then use `http://localhost:9999/file/<name of the file>`.\n",
    "The following example assumes that both Druid and the data generator server are running in docker compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d72b015-f8ec-4713-b6f2-fe7a15afff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'0': {'input0': {'type': 'channel', 'rows': [18096], 'bytes': [4666599], 'files': [1], 'totalFiles': [1]}, 'output': {'type': 'channel', 'rows': [18096], 'bytes': [3562777], 'frames': [1]}, 'shuffle': {'type': 'channel', 'rows': [18096], 'bytes': [3490597], 'frames': [7]}, 'sortProgress': {'type': 'sortProgress', 'totalMergingLevels': 3, 'levelToTotalBatches': {'0': 1, '1': 1, '2': 1}, 'levelToMergedBatches': {'0': 1, '1': 1, '2': 1}, 'totalMergersForUltimateLevel': 1, 'progressDigest': 1.0}}}, {'0': {'input0': {'type': 'channel', 'rows': [18096], 'bytes': [3490597], 'frames': [7]}, 'segmentGenerationProgress': {'type': 'segmentGenerationProgress', 'rowsProcessed': 18096, 'rowsPersisted': 18096, 'rowsMerged': 18096, 'rowsPushed': 18096}}}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mREPLACE INTO \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclicks\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OVERWRITE ALL\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mWITH \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AS (SELECT *\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mPARTITIONED BY DAY\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# display progress of task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mdruid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#r = druid.sql.task(sql)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#print(r._tasks().task_reports(r._id))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/druidapi/display.py:169\u001b[0m, in \u001b[0;36mDisplayClient.run_task\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m reports[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiStageQuery\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMPLETED\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAILED\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    168\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m;\n\u001b[0;32m--> 169\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "REPLACE INTO \"clicks\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (SELECT *\n",
    "FROM TABLE(\n",
    "  EXTERN(\n",
    "    '{\"type\":\"http\",\"uris\":[\"http://datagen:9999/file/clicks.json\"]}',\n",
    "    '{\"type\":\"json\"}'\n",
    "  )\n",
    ") EXTEND (\"time\" VARCHAR, \"user_id\" VARCHAR, \"event_type\" VARCHAR, \"client_ip\" VARCHAR, \"client_device\" VARCHAR, \"client_lang\" VARCHAR, \"client_country\" VARCHAR, \"referrer\" VARCHAR, \"keyword\" VARCHAR, \"product\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_PARSE(\"time\") AS \"__time\",\n",
    "  \"user_id\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\",\n",
    "  \"referrer\",\n",
    "  \"keyword\",\n",
    "  \"product\"\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "# display progress of task\n",
    "druid.display.run_task(sql)\n",
    "\n",
    "#r = druid.sql.task(sql)\n",
    "#print(r._tasks().task_reports(r._id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34838b23-e260-40fb-b10e-4d0637098df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': {'input0': {'type': 'channel',\n",
       "    'rows': [18096],\n",
       "    'bytes': [4666599],\n",
       "    'files': [1],\n",
       "    'totalFiles': [1]},\n",
       "   'output': {'type': 'channel',\n",
       "    'rows': [18096],\n",
       "    'bytes': [3562777],\n",
       "    'frames': [1]},\n",
       "   'shuffle': {'type': 'channel',\n",
       "    'rows': [18096],\n",
       "    'bytes': [3490597],\n",
       "    'frames': [7]},\n",
       "   'sortProgress': {'type': 'sortProgress',\n",
       "    'totalMergingLevels': 3,\n",
       "    'levelToTotalBatches': {'0': 1, '1': 1, '2': 1},\n",
       "    'levelToMergedBatches': {'0': 1, '1': 1, '2': 1},\n",
       "    'totalMergersForUltimateLevel': 1,\n",
       "    'progressDigest': 1.0}}},\n",
       " {'0': {'input0': {'type': 'channel',\n",
       "    'rows': [18096],\n",
       "    'bytes': [3490597],\n",
       "    'frames': [7]},\n",
       "   'segmentGenerationProgress': {'type': 'segmentGenerationProgress',\n",
       "    'rowsProcessed': 18096,\n",
       "    'rowsPersisted': 18096,\n",
       "    'rowsMerged': 18096,\n",
       "    'rowsPushed': 18096}}}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "payload = [r.reports['multiStageQuery']['payload']['counters'][s] for s in r.reports['multiStageQuery']['payload']['counters']]\n",
    "payload\n",
    "\n",
    "#print(json.dumps(payload, indent = 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0997b38-02c2-483e-bd15-439c4bf0097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql('''\n",
    "SELECT  event_type, \n",
    "        count( DISTINCT \"user_id\") users, \n",
    "        count( DISTINCT \"client_ip\") ips, \n",
    "        count( DISTINCT \"client_ip\") - count( DISTINCT \"user_id\") ips_minus_users\n",
    "FROM \"clicks\"\n",
    "GROUP BY 1\n",
    "HAVING count( DISTINCT \"user_id\") - count( DISTINCT \"client_ip\") < 0\n",
    "ORDER BY 4 DESC\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec013f-28e4-4d5a-94a6-06e0ed537b4e",
   "metadata": {},
   "source": [
    "## Generating custom data\n",
    "\n",
    "You can fine the full set of configuration option in the [data generator project's readme](https://github.com/implydata/druid-datagenerator#data-generator-configuration).\n",
    "\n",
    "In this section we use a simple custom configuration as an example to generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6451310-b7dd-4b39-a23b-7b735b152d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "  \"emitters\": [\n",
    "    {\n",
    "      \"name\": \"simple_record\",\n",
    "      \"dimensions\": [\n",
    "        {\n",
    "          \"type\": \"string\",\n",
    "          \"name\": \"random_string_column\",\n",
    "          \"length_distribution\": {\n",
    "            \"type\": \"constant\",\n",
    "            \"value\": 13\n",
    "          },\n",
    "          \"cardinality\": 0,\n",
    "          \"chars\": \"#.abcdefghijklmnopqrstuvwxyz\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"int\",\n",
    "          \"name\": \"distributed_number\",\n",
    "          \"distribution\": {\n",
    "            \"type\": \"uniform\",\n",
    "            \"min\": 0,\n",
    "            \"max\": 1000\n",
    "          },\n",
    "          \"cardinality\": 10,\n",
    "          \"cardinality_distribution\": {\n",
    "            \"type\": \"exponential\",\n",
    "            \"mean\": 5\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"interarrival\": {\n",
    "    \"type\": \"constant\",\n",
    "    \"value\": 1\n",
    "  },\n",
    "  \"states\": [\n",
    "    {\n",
    "      \"name\": \"state_1\",\n",
    "      \"emitter\": \"simple_record\",\n",
    "      \"delay\": {\n",
    "        \"type\": \"constant\",\n",
    "        \"value\": 1\n",
    "      },\n",
    "      \"transitions\": [\n",
    "        {\n",
    "          \"next\": \"state_1\",\n",
    "          \"probability\": 1.0\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "target = { \"type\":\"file\", \"path\":\"sample_data.json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a22645-aea5-4c15-b81a-959b27df731f",
   "metadata": {},
   "source": [
    "Now, instead of using a config_file, we use the config attribute of the request to use our new custom data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5c535-3474-42b4-9772-14279e712f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1 hour of simulated time using custom configuration\n",
    "datagen_request = {\n",
    "    \"name\": \"sample_custom\",\n",
    "    \"target\": target,\n",
    "    \"config\": gen_config, \n",
    "    \"time\": \"1h\",\n",
    "    \"concurrency\":10,\n",
    "    \"time_type\": \"SIM\"\n",
    "}\n",
    "response = datagen.post(\"/start\", json.dumps(datagen_request), headers=headers, require_ok=False)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952386f7-8181-4325-972b-5f30dc12cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(datagen.get(f\"/jobs\", require_ok=False).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b3a2a-4fd9-45a2-9221-497d906f62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display( datagen.get(f\"/file/sample_data.json\").content[:1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bff054-0f16-4fd5-8ade-2d44b30d0cf2",
   "metadata": {},
   "source": [
    "## Streaming generated data\n",
    "\n",
    "The data generator works exactly the same whether it is writing data to a file or publishing messages into a stream. You  only need to change the target configuration.\n",
    "\n",
    "To use the Kafka container running on Docker Compose, use the host name `kafka:9092`. This piece of code uses the KAFKA_HOST environment variable from Docker Compose to specify the Kafka host. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959b7c3-6223-479d-b0c2-115a1c555090",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.environ['KAFKA_HOST'] == None):\n",
    "    kafka_host=f\"kafka:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497abc18-6538-4536-a17f-fe10c4367611",
   "metadata": {},
   "source": [
    "The simplest `target` object for kafka (and similarly confluent) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a74ab-e2dd-458e-9e93-10291064e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {\n",
    "    \"type\":\"kafka\",\n",
    "    \"endpoint\": kafka_host,\n",
    "    \"topic\": \"custom_data\"\n",
    "}\n",
    "\n",
    "# Generate 1 hour of simulated time using custom configuration\n",
    "datagen_request = {\n",
    "    \"name\": \"sample_custom\",\n",
    "    \"target\": target,\n",
    "    \"config\": gen_config, \n",
    "    \"time\": \"1h\",\n",
    "    \"concurrency\":10,\n",
    "    \"time_type\": \"SIM\"\n",
    "}\n",
    "response = datagen.post(\"/start\", json.dumps(datagen_request), headers=headers, require_ok=False)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17d0c7-a3ab-4f37-bbf0-cc02bff44cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(datagen.get(f\"/jobs\", require_ok=False).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7b706-9040-4a69-a956-1b1bbb037c32",
   "metadata": {},
   "source": [
    "### Ingesting data from a stream \n",
    "This example shows how to start a streaming ingestion supervisor in Apache Druid to consume your custom data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51912409-e4e7-48d1-b3a5-b269622b4e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_spec ={\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": {\n",
    "      \"type\": \"kafka\",\n",
    "      \"consumerProperties\": {\n",
    "        \"bootstrap.servers\": \"kafka:9092\"\n",
    "      },\n",
    "      \"topic\": \"custom_data\",\n",
    "      \"inputFormat\": {\n",
    "        \"type\": \"json\"\n",
    "      },\n",
    "      \"useEarliestOffset\": True\n",
    "    },\n",
    "    \"tuningConfig\": {\n",
    "      \"type\": \"kafka\",\n",
    "      \"maxRowsInMemory\": 100000,\n",
    "      \"resetOffsetAutomatically\": False\n",
    "    },\n",
    "    \"dataSchema\": {\n",
    "      \"dataSource\": \"custom_data\",\n",
    "      \"timestampSpec\": {\n",
    "        \"column\": \"time\",\n",
    "        \"format\": \"iso\"\n",
    "      },\n",
    "      \"dimensionsSpec\": {\n",
    "        \"dimensions\": [\n",
    "          \"random_string_column\",\n",
    "          {\n",
    "            \"type\": \"long\",\n",
    "            \"name\": \"distributed_number\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"granularitySpec\": {\n",
    "        \"queryGranularity\": \"none\",\n",
    "        \"rollup\": False,\n",
    "        \"segmentGranularity\": \"hour\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "druid.rest.post(\"/druid/indexer/v1/supervisor\", json.dumps(ingestion_spec), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1284ed-5c49-4f37-81f7-c3b720473158",
   "metadata": {},
   "outputs": [],
   "source": [
    "druid.display.sql('''\n",
    "SELECT random_string_column, MAX(distributed_number)\n",
    "FROM custom_data\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 10\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
