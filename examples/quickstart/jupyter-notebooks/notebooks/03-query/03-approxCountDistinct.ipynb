{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4188e2c5-4ab0-45e3-9371-52d510a97413",
   "metadata": {},
   "source": [
    "# Counting distinct values\n",
    "\n",
    "It's extremely common for analysts to want to count unique occurences of some dimension value in data. With the Druid database's history of large volumes of data comes an advanced computer science technique to speed up this calculation through approximation. In this tutorial, work through some examples and see the effect of turning it on and off, and of making it even faster by pre-generating the objects that Druid uses to execute the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e06e8-9b35-4b34-8322-8a8ede6de709",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 26.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see [Docker for Jupyter Notebook tutorials](https://druid.apache.org/docs/latest/tutorials/tutorial-jupyter-docker.html).\n",
    "\n",
    "<details><summary>    \n",
    "<b>Run without Docker Compose</b>    \n",
    "</summary>\n",
    "\n",
    "If you do not use the Docker Compose environment, you need the following:\n",
    "\n",
    "* A running Druid instance.\n",
    "* [druidapi](https://github.com/apache/druid/blob/master/examples/quickstart/jupyter-notebooks/druidapi/README.md), a Python client for Apache Druid. Follow the instructions in the Install section of the README file.\n",
    "* [matplotlib](https://matplotlib.org/), a library for creating visualizations in Python,\n",
    "* [pandas](https://pandas.pydata.org/), a data analysis and manipulation tool.\n",
    "* Jupyter notebook or Jupyter Lab. See [jupyter.org](https://jupyter.org/) for installation instructions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3317c51-627c-4a44-ad73-0860a5f4c937",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Run the next cell to attempt a connection to Druid services. If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a19226-6abc-436d-ac3c-9c04d6026707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "    \n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee22cea-6176-4119-b4ea-24ed76cfa8fe",
   "metadata": {},
   "source": [
    "### Load example flight data\n",
    "\n",
    "Once your Druid environment is up and running, ingest the sample data for this tutorial.\n",
    "\n",
    "Open the Druid console:\n",
    "\n",
    "1. Load data\n",
    "2. Batch - SQL\n",
    "3. Example data\n",
    "4. Select \"FlightCarrierOnTime (1 month)\"\n",
    "\n",
    "For the purposes of this notebook, use all the defaults suggested by the console, including the default datasource name: \n",
    "\n",
    "`On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0d1f7-2f34-44fe-9912-4017ed10893c",
   "metadata": {},
   "source": [
    "When this is completed, run the following cell for the final part of the initialization. This will provide us some methods to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfd50d-5ef9-4fd1-8077-bf03758c35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388633f-195b-4381-98cc-7a2f80f48690",
   "metadata": {},
   "source": [
    "## COUNT(DISTINCT) queries on basic datasets\n",
    "\n",
    "This is a typical query you might run to find the number of distinct Tail Numbers in the example dataset.\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    \"Reporting_Airline\",\n",
    "    COUNT(DISTINCT \"Tail_Number\") AS \"Events\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "ORDER BY 2\n",
    "```\n",
    "\n",
    "You're about to discover several ways enables you to arrive at this answer, each one having different performance characteristics depending on the needs of your users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8d8bf-69fa-4266-b171-cb550009e89e",
   "metadata": {},
   "source": [
    "### Running COUNT(DISTINCT) with approximation\n",
    "\n",
    "Druid automatically looks for query patterns that benefit from from approximation. In this instance, Druid identifies a match for approximate `COUNT(DISTINCT)`.\n",
    "\n",
    "In Druid, each data server computes its own intermediate results that are then merged into the final result set that we see. In a `COUNT(DISTINCT)` query, they must create their own set of unique values, pass these back, and then computation is done to find uniques inside those larger sets. When the dimension being looked at contains many tens-of-thousands, perhaps even millions of unique values, both the transfer of intermediate results and the final computation can be both memory and CPU hungry.\n",
    "\n",
    "When Druid uses approximation, the intermediate results are contained in a representation called a [data sketch](https://datasketches.apache.org/). These much smaller objects are transfered and merged together when the query results are finalized, rather than Druid having to combine the individual lists of distinct values from each process. The most scalable part of Druid – the individual data servers – do much more of the work, and do so earlier, instead of leaving it to the merge stage.\n",
    "\n",
    "Interestingly, data sketch size is not dependent on the underlying data – the default size of a sketch in Druid is just over 2000 bytes.\n",
    "\n",
    "This translates into much faster query execution, especially when the intermediate results are large.\n",
    "\n",
    "> Approximations improve scalability, storage, and memory use - at the cost of some error.\n",
    "> \n",
    "> _[Gian Merlino](https://github.com/gianm)_\n",
    "\n",
    "In your own deployment, check Druid's [configuration files](https://druid.apache.org/docs/26.0.0/configuration/index.html#sql) to see what type of data sketch is being used (`druid.sql.approxCountDistinct.function`) and whether this approach has been left as the default by your system administrators (`druid.sql.planner.useApproximateCountDistinct`).\n",
    "\n",
    "Run the following cell to plot the number of events in the sample data for each `Tail_Number`. Remember by default, each Druid process will be calculating its local results as a data sketch, passing these back for the final merge operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e5184-9fe4-4f21-a471-4e15d16515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT\n",
    "    \"Reporting_Airline\",\n",
    "    COUNT(DISTINCT \"Tail_Number\") AS \"Unique Tail Numbers\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "ORDER BY 2\n",
    "'''\n",
    "\n",
    "df1 = pd.DataFrame(sql_client.sql(sql))\n",
    "\n",
    "df1.plot.bar(x='Reporting_Airline', y='Unique Tail Numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37d854-efd9-401a-8726-9949bff0c012",
   "metadata": {},
   "source": [
    "### Running COUNT(DISTINCT) without approximation\n",
    "\n",
    "We can supply a query context parameter, `useApproximateCountDistinct`, to force Druid to not use this approximation technique.\n",
    "\n",
    "Using the same SQL statement as before, the following cell crafts a request (`req`) where we set the context parameter before Druid executes the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652988ac-c256-46d4-a4ea-dbcf0e023991",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateCountDistinct\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "df2 = pd.DataFrame(resp.rows)\n",
    "df2.plot.bar(x='Reporting_Airline', y='Unique Tail Numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c91329-8d05-46eb-8c19-5eaf9043dcb6",
   "metadata": {},
   "source": [
    "### Comparing approximate and non-approximate results\n",
    "\n",
    "The next cell shows a comparison of the two results above: `df1` used the default approximation approach, while `df2` are the results where we turned approximation off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a031f-a805-45dd-935b-d8af808041a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.compare(df2, keep_equal=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3320d-d4ec-460a-b1fc-59c98f85cc3a",
   "metadata": {},
   "source": [
    "The table shows:\n",
    "\n",
    "* A row number\n",
    "* The reporting airline in the approximate results (`self`) versus that in the non-approximate results (`other`)\n",
    "* The calculated distinct number of `Tail Number`s\n",
    "\n",
    "Notice that there are _value_ errors, as you might expect with approximation, and that in some instances this affects the _order_ of results.\n",
    "\n",
    "Error in sketch-based approximation is probabilistic, rather than guaranteed. That's to say that a certain percentage of the time you can expect the measurements you take to be within a certain distance of the true value.\n",
    "\n",
    "As an experiment, you may want to:\n",
    "\n",
    "* Identify a high-cardinality column in one of your own data sets\n",
    "* Test how long an approximate `DISTINCT(COUNT)` query takes to run with approximation turned on\n",
    "* Test how long the same query takes to run with approximation turned off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff80c4-8ca6-4563-b67e-70b09a611877",
   "metadata": {},
   "source": [
    "## COUNT(DISTINCT) queries on sketched data\n",
    "\n",
    "For even faster performance, create a `TABLE` in Druid where sketches are stored directly in the table. At ingestion time, you create sets of unique values from the raw data that can then be addressed with SQL functions, including ones that solve for `COUNT(DISTINCT)` by addressing the sketches directly.\n",
    "\n",
    "Take a look at the SQL in the next cell: it applies a `GROUP BY` to `Reporting_Airline` and uses the [`DS_HLL`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#ds_hll) function to create sets of `Tail_Number`s inside a \"HyperLogLog\" sketch object.\n",
    "\n",
    "Run the cell to get a \"human readable\" version of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595eec0-a709-4cd6-985e-eec8a6e37b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT\n",
    "    \"Reporting_Airline\",\n",
    "    DS_HLL(\"Tail_Number\") AS \"Sketch\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "LIMIT 5\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb0596-2034-4993-a4a2-006d9aa53a9b",
   "metadata": {},
   "source": [
    "The results show, for each `Reporting_Airline`, the highly optimized, aggregated list of the `Tail_Number`s. By storing this in the table, data processes gain an overall speed boost from both an increase in table data efficiency and a decrease in computation effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13e4b5-9a8c-4519-9469-27a1518caa9a",
   "metadata": {},
   "source": [
    "### Creating sketches during batch ingestion\n",
    "\n",
    "The next cell ingests the example flight data into a new table, `flights-counts`. A `GROUP BY` aggregates all `Tail_Number`s into a HLL sketch using `DS_HLL` for each hour (`TIME_FLOOR`), `Reporting_Airline`, `Origin`, and `Dest`.\n",
    "\n",
    "Notice that it does not store the original `Tail_Number`s. If we kept that field, the `GROUP BY` wouldn't aggregate any rows into the sketch - there would be a 1:1 relationship between the row and each `Tail_Number` - which is the opposite of what we are designing for! By implication, it will be no longer possible to use the raw data as part of any SQL queries, like a `GROUP BY` or a `WHERE`.\n",
    "\n",
    "These two techniques – aggregation and generating sketches – increases query execution efficiency dramatically.\n",
    "\n",
    "\"HyperLogLog\" is not the only type of sketch that can be used: there are two types of Apache Datasketch that allow for `COUNT(DISTINCT)` computations.\n",
    "\n",
    "* [HyperLogLog](https://druid.apache.org/docs/26.0.0/querying/sql-aggregations.html#hll-sketch-functions)\n",
    "* [Theta](https://druid.apache.org/docs/26.0.0/querying/sql-aggregations.html#theta-sketch-functions)\n",
    "\n",
    "When you need to also carry out set operations, like intersection and difference, create Theta sketches using the [`DS_THETA`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#ds_theta) function. You can see this happening in the SQL in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213e7b5-14f8-4a6c-a489-8f5cd9c17359",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "REPLACE INTO \"flights-counts\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (SELECT *\n",
    "FROM TABLE(\n",
    "  EXTERN(\n",
    "    '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/flight_on_time/flights/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11.csv.zip\"]}',\n",
    "    '{\"type\":\"csv\",\"findColumnsFromHeader\":true}'\n",
    "  )\n",
    ") EXTEND (\"depaturetime\" VARCHAR, \"arrivalime\" VARCHAR, \"Year\" BIGINT, \"Quarter\" BIGINT, \"Month\" BIGINT, \"DayofMonth\" BIGINT, \"DayOfWeek\" BIGINT, \"FlightDate\" VARCHAR, \"Reporting_Airline\" VARCHAR, \"DOT_ID_Reporting_Airline\" BIGINT, \"IATA_CODE_Reporting_Airline\" VARCHAR, \"Tail_Number\" VARCHAR, \"Flight_Number_Reporting_Airline\" BIGINT, \"OriginAirportID\" BIGINT, \"OriginAirportSeqID\" BIGINT, \"OriginCityMarketID\" BIGINT, \"Origin\" VARCHAR, \"OriginCityName\" VARCHAR, \"OriginState\" VARCHAR, \"OriginStateFips\" BIGINT, \"OriginStateName\" VARCHAR, \"OriginWac\" BIGINT, \"DestAirportID\" BIGINT, \"DestAirportSeqID\" BIGINT, \"DestCityMarketID\" BIGINT, \"Dest\" VARCHAR, \"DestCityName\" VARCHAR, \"DestState\" VARCHAR, \"DestStateFips\" BIGINT, \"DestStateName\" VARCHAR, \"DestWac\" BIGINT, \"CRSDepTime\" BIGINT, \"DepTime\" BIGINT, \"DepDelay\" BIGINT, \"DepDelayMinutes\" BIGINT, \"DepDel15\" BIGINT, \"DepartureDelayGroups\" BIGINT, \"DepTimeBlk\" VARCHAR, \"TaxiOut\" BIGINT, \"WheelsOff\" BIGINT, \"WheelsOn\" BIGINT, \"TaxiIn\" BIGINT, \"CRSArrTime\" BIGINT, \"ArrTime\" BIGINT, \"ArrDelay\" BIGINT, \"ArrDelayMinutes\" BIGINT, \"ArrDel15\" BIGINT, \"ArrivalDelayGroups\" BIGINT, \"ArrTimeBlk\" VARCHAR, \"Cancelled\" BIGINT, \"CancellationCode\" VARCHAR, \"Diverted\" BIGINT, \"CRSElapsedTime\" BIGINT, \"ActualElapsedTime\" BIGINT, \"AirTime\" BIGINT, \"Flights\" BIGINT, \"Distance\" BIGINT, \"DistanceGroup\" BIGINT, \"CarrierDelay\" BIGINT, \"WeatherDelay\" BIGINT, \"NASDelay\" BIGINT, \"SecurityDelay\" BIGINT, \"LateAircraftDelay\" BIGINT, \"FirstDepTime\" VARCHAR, \"TotalAddGTime\" VARCHAR, \"LongestAddGTime\" VARCHAR, \"DivAirportLandings\" VARCHAR, \"DivReachedDest\" VARCHAR, \"DivActualElapsedTime\" VARCHAR, \"DivArrDelay\" VARCHAR, \"DivDistance\" VARCHAR, \"Div1Airport\" VARCHAR, \"Div1AirportID\" VARCHAR, \"Div1AirportSeqID\" VARCHAR, \"Div1WheelsOn\" VARCHAR, \"Div1TotalGTime\" VARCHAR, \"Div1LongestGTime\" VARCHAR, \"Div1WheelsOff\" VARCHAR, \"Div1TailNum\" VARCHAR, \"Div2Airport\" VARCHAR, \"Div2AirportID\" VARCHAR, \"Div2AirportSeqID\" VARCHAR, \"Div2WheelsOn\" VARCHAR, \"Div2TotalGTime\" VARCHAR, \"Div2LongestGTime\" VARCHAR, \"Div2WheelsOff\" VARCHAR, \"Div2TailNum\" VARCHAR, \"Div3Airport\" VARCHAR, \"Div3AirportID\" VARCHAR, \"Div3AirportSeqID\" VARCHAR, \"Div3WheelsOn\" VARCHAR, \"Div3TotalGTime\" VARCHAR, \"Div3LongestGTime\" VARCHAR, \"Div3WheelsOff\" VARCHAR, \"Div3TailNum\" VARCHAR, \"Div4Airport\" VARCHAR, \"Div4AirportID\" VARCHAR, \"Div4AirportSeqID\" VARCHAR, \"Div4WheelsOn\" VARCHAR, \"Div4TotalGTime\" VARCHAR, \"Div4LongestGTime\" VARCHAR, \"Div4WheelsOff\" VARCHAR, \"Div4TailNum\" VARCHAR, \"Div5Airport\" VARCHAR, \"Div5AirportID\" VARCHAR, \"Div5AirportSeqID\" VARCHAR, \"Div5WheelsOn\" VARCHAR, \"Div5TotalGTime\" VARCHAR, \"Div5LongestGTime\" VARCHAR, \"Div5WheelsOff\" VARCHAR, \"Div5TailNum\" VARCHAR, \"Unnamed: 109\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_FLOOR(TIME_PARSE(\"depaturetime\"), 'PT1H') AS \"__time\",\n",
    "  \"Reporting_Airline\",\n",
    "  \"Origin\",\n",
    "  \"Dest\",\n",
    "  COUNT(*) AS \"Events\",\n",
    "  MAX(\"Distance\") AS \"Distance_Max\",\n",
    "  MIN(\"Distance\") AS \"Distance_Min\",\n",
    "  DS_HLL(\"Tail_Number\") AS \"Tail_Number_HLL\",\n",
    "  DS_THETA(\"Tail_Number\") AS \"Tail_Number_THETA\"\n",
    "FROM \"ext\"\n",
    "GROUP BY 1, 2, 3, 4\n",
    "PARTITIONED BY DAY\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586d0f1-e2e4-498b-a98e-1aaa00b65a58",
   "metadata": {},
   "source": [
    "When doing this programmatically we need to be sure to include a context parameter that prompts Druid to store the true sketch value: [`finalizeAggregations`](https://druid.apache.org/docs/26.0.0/multi-stage-query/reference.html#context-parameters). Notice that, if you build an ingestion using the console, these settings are applied for you automatically.\n",
    "\n",
    "The following cell adds the parameters and then executes the ingestion.\n",
    "\n",
    "When the ingestion is finished, you will see the table definition. Monitor the ingestion task itself in the Druid Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58a332-4f86-4d24-a4b6-286b3c4fe54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"finalize\", \"false\")\n",
    "req.add_context(\"finalizeAggregations\", \"false\")\n",
    "\n",
    "sql_client.run_task(req)\n",
    "sql_client.wait_until_ready('flights-counts')\n",
    "display.table('flights-counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db413cc-e2c3-4702-a693-c1285ab1c58a",
   "metadata": {},
   "source": [
    "As noted before, use specific SQL functions when doing analytics directly on sketches. For `COUNT(DISTINCT)` these are:\n",
    "\n",
    "* For HLL [`APPROX_COUNT_DISTINCT_DS_HLL`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#approx_count_distinct_ds_hll), and\n",
    "* for Theta [`APPROX_COUNT_DISTINCT_THETA`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#approx_count_distinct_ds_theta).\n",
    "\n",
    "Here's an example query showing our estimated results – notice the use of the `FILTER` clause to split results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4a293-b967-46e1-9a37-7856ce25f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "   \"Reporting_Airline\",\n",
    "   SUM(\"Distance_Max\") AS \"Miles_Flown\",\n",
    "   APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Distance_Max\" > 2000) AS \"HLLApprox-over2k\",\n",
    "   APPROX_COUNT_DISTINCT_DS_THETA(\"Tail_Number_THETA\") FILTER (WHERE \"Distance_Max\" < 2000) AS \"ThetaApprox-under2k\"\n",
    "FROM \"flights-counts\"\n",
    "GROUP BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d3cae-1e1d-47d0-9143-91b19d58e17e",
   "metadata": {},
   "source": [
    "The next query generates three HLL sketches covering flights out of three cities in the United States over a three week period.\n",
    "\n",
    "Sketch objects are mergable – this is why Druid is able to make use of them during massively-parallelised query execution. The next query takes advantage of this to estimate how many distinct `Tail_Number`s there were across all three sets.\n",
    "\n",
    "You'll recognise the `APPROX_COUNT_DISTINCT_DS_HLL` function and the `DS_HLL` function, generating sketches for the `Tail_Number`s originating in each city. To that the query adds the `HLL_SKETCH_UNION` function, which does the merging of each independent set of result sets.\n",
    "\n",
    "To turn it from a sketch into something readable, the query then uses the `HLL_SKETCH_ESTIMATE` function to give us a number instead of a sketch.\n",
    "\n",
    "It's also grouping those calculations by weeks by using `TIME_FLOOR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be5c57-bb35-4082-87f4-730c2f79621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "  TIME_FLOOR(\"__time\",'P1W') AS \"Week commencing\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='ATL') AS \"From Atlanta\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='DFW') AS \"From Dallas\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='SFO') AS \"From San Francisco\",\n",
    "  HLL_SKETCH_ESTIMATE(\n",
    "     HLL_SKETCH_UNION(\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='ATL'),\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='DFW'),\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='SFO')\n",
    "      )\n",
    "    ) AS \"From any of the three\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") AS \"From any city\"\n",
    "FROM \"flights-counts\"\n",
    "WHERE TIMESTAMP '2005-10-31' <= __time AND __time <= TIMESTAMP '2005-11-20'\n",
    "GROUP BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940bc114-17a1-4e65-bb47-a8b47469fff6",
   "metadata": {},
   "source": [
    "### Creating sketches during streaming ingestion\n",
    "\n",
    "In streaming ingestion, rather than using `DS_HLL` or `DS_THETA` you include the Druid Native equivallent in the [`metricsSpec`](https://druid.apache.org/docs/26.0.0/ingestion/ingestion-spec.html#metricsspec) and - instead of `GROUP BY` - enable [`queryGranularity`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html#granularityspec) and `rollup` to truncate the time stamp and pre-aggregate the rows.\n",
    "\n",
    "The `INSERT` statement above is therefore equivallent to:\n",
    "\n",
    "```json\n",
    "    {\n",
    "      \"type\": \"HLLSketchBuild\",\n",
    "      \"fieldName\": \"Tail_Number\",\n",
    "      \"lgK\": 12,\n",
    "      \"tgtHllType\": \"HLL_4\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"thetaSketch\",\n",
    "      \"fieldName\": \"Tail_Number\",\n",
    "      \"size\": 16384\n",
    "    }\n",
    "```\n",
    "\n",
    "Notice that here it's easy to see some internal parameters for sketch generation, like the `lgK` value for HLL. In SQL mode, these are exposed as supplementary parameters to the `DS_HLL` function. Be cautious of changing these values without researching the effects - not just in accuracy but also in terms of performance and segment size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a1846-5072-4495-b840-a620de3c0442",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* Approximation is the default execution model for `COUNT(DISTINCT)` queries\n",
    "* You can turn it off with a query context parameter\n",
    "* Accuracy is highly dependent on the distribution and cardinality of data across the database\n",
    "* Druid can be pre-loaded with sketch objects that speed up approximation both in batch and streaming ingestion\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Watch [Employ Approximation](https://youtu.be/fSWwJs1gCvQ?list=PLDZysOZKycN7MZvNxQk_6RbwSJqjSrsNR) by Peter Marshall\n",
    "* Read [Ingesting Data Sketches into Apache Druid](https://blog.hellmar-becker.de/2022/12/26/ingesting-data-sketches-into-apache-druid/) by Hellmar Becker\n",
    "* Read more about the native \"aggregator\" functions for streaming ingestion\n",
    "    * [ThetaSketch function](https://druid.apache.org/docs/26.0.0/development/extensions-core/datasketches-theta.html)\n",
    "    * [HyperLogLog function](https://druid.apache.org/docs/26.0.0/development/extensions-core/datasketches-hll.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71235a-aa0e-4221-bbee-75c1fd6bafc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
