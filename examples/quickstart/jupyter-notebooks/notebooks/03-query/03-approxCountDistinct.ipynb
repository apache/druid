{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557e06e8-9b35-4b34-8322-8a8ede6de709",
   "metadata": {},
   "source": [
    "# Counting distinct values\n",
    "\n",
    "__It's extremely common for analysts to want to count unique occurences of some dimension value in data. With the Druid database's history of large volumes of data comes an advanced computer science technique to speed up this calculation through approximation. In this tutorial, work through some examples and see the effect of turning it on and off, and of making it even faster by pre-generating the objects that Druid uses to execute the query.__\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 26.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see [Docker for Jupyter Notebook tutorials](https://druid.apache.org/docs/latest/tutorials/tutorial-jupyter-docker.html).\n",
    "\n",
    "You must also have loaded the \"FlightCarrierOnTime (1 month)\" sample data, using defaults, into the table `On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11`.\n",
    "\n",
    "If you do not use the Docker Compose environment, you need the following:\n",
    "* A running Druid instance.\n",
    "   * Update the `druid_host` variable to point to your Router endpoint. For example, `druid_host = \"http://localhost:8888\"`.\n",
    "* The following Python packages:\n",
    "   * `druidapi`, a Python client for Apache Druid\n",
    "\n",
    "To start this tutorial, run the next cell. It defines variables for two datasources and the Druid host the tutorial uses. The quickstart deployment configures Druid to listen on port `8888` by default, so you'll make API calls against `http://localhost:8888`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a19226-6abc-436d-ac3c-9c04d6026707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# druid_host is the hostname and port for your Druid deployment. \n",
    "# In the Docker Compose tutorial environment, this is the Router\n",
    "# service running at \"http://router:8888\".\n",
    "# If you are not using the Docker Compose environment, edit the `druid_host`.\n",
    "\n",
    "druid_host = \"http://router:8888\"\n",
    "druid_host\n",
    "\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "display.tables('INFORMATION_SCHEMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388633f-195b-4381-98cc-7a2f80f48690",
   "metadata": {},
   "source": [
    "## COUNT(DISTINCT) queries on basic datasets\n",
    "\n",
    "Here's a very simple query to find the number of distinct Tail Numbers in the example dataset.\n",
    "\n",
    "```sql\n",
    "SELECT \"Reporting_Airline\", COUNT(DISTINCT \"Tail_Number\") AS \"Events\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "ORDER BY 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8d8bf-69fa-4266-b171-cb550009e89e",
   "metadata": {},
   "source": [
    "### Running COUNT(DISTINCT) with approximation\n",
    "\n",
    "Druid automatically looks for query patterns that benefit from from approximation. In this instance, Druid identifies a match for approximate `COUNT(DISTINCT)`.\n",
    "\n",
    "Each data server computes its own intermediate results for merging into the final result set. In a `COUNT(DISTINCT)` query, that means that they will create their own count inside a representation called a [data sketch](https://datasketches.apache.org/). These much smaller objects are then merged together when the query results are finalized, rather than Druid having to combine the individual lists of distinct values from each process.\n",
    "\n",
    "This translates into much faster query execution, especially when the intermediate results are large – say when there are a lot of unique values in the source data.\n",
    "\n",
    "It also means that the most scalable part of Druid – the individual data servers – do much more of the work, and do so earlier, instead of leaving it to the merge stage.\n",
    "\n",
    "> Approximations improve scalability, storage, and memory use - at the cost of some error.\n",
    "> \n",
    "> _[Gian Merlino](https://github.com/gianm)_\n",
    "\n",
    "Let's run this query with all of Druid's defaults to see what the results are like. (We can safely omit a `__time` filter thanks to the tiny size of the example dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e5184-9fe4-4f21-a471-4e15d16515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT \"Reporting_Airline\", COUNT(DISTINCT \"Tail_Number\") AS \"Events\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "ORDER BY 2\n",
    "'''\n",
    "df = pd.DataFrame(sql_client.sql(sql))\n",
    "\n",
    "df.plot.bar(x='Reporting_Airline', y='Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37d854-efd9-401a-8726-9949bff0c012",
   "metadata": {},
   "source": [
    "### Running COUNT(DISTINCT) without approximation\n",
    "\n",
    "We can supply a query context parameter, `useApproximateCountDistinct`, to force Druid to not use approximation. We won't get the speed boost afforded by the sketching approach – but that's OK because the example dataset is so small! It would be a different story if `Tail_Number` had high cardinality - like if it was IP Addresses or User Identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652988ac-c256-46d4-a4ea-dbcf0e023991",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT \"Reporting_Airline\", COUNT(DISTINCT \"Tail_Number\") AS \"Events\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "ORDER BY 2\n",
    "'''\n",
    "\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateCountDistinct\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "df = pd.DataFrame(resp.rows)\n",
    "df.plot.bar(x='Reporting_Airline', y='Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c91329-8d05-46eb-8c19-5eaf9043dcb6",
   "metadata": {},
   "source": [
    "### Comparing approximate and non-approximate results\n",
    "\n",
    "On the surface, these do not _look_ different. And, in a lot of user interfaces, that's perfectly fine!\n",
    "\n",
    "But let's go a bit deeper and see how the results actually differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a031f-a805-45dd-935b-d8af808041a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT \"Reporting_Airline\", COUNT(DISTINCT \"Tail_Number\") AS \"Events\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "ORDER BY 2\n",
    "'''\n",
    "\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateCountDistinct\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "df1 = pd.DataFrame(sql_client.sql(sql))\n",
    "df2 = pd.DataFrame(resp.rows)\n",
    "\n",
    "df3 = df1.compare(df2, keep_equal=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3320d-d4ec-460a-b1fc-59c98f85cc3a",
   "metadata": {},
   "source": [
    "There are _value_ errors, as you might expect with approximation. This therefore affects _ordering_ of results.\n",
    "\n",
    "_Error in sketch-based approximation is probabilistic, rather than guaranteed. That's to say that a certain percentage of the time you can expect the measurements you take to be within a certain distance of the true value. Also, their size is not dependent on the data – the default size of a sketch in Druid is just over 2000 bytes._\n",
    "\n",
    "Approximation is especially helpful for very high cardinality data. When there are hundreds of thousands, millions, even tens-of-millions of distinct values, passing the individual distinct values to be merged takes longer and more data storage than using datasketches.\n",
    "\n",
    "As an experiment, you may want to:\n",
    "\n",
    "* Ingest or use a much larger data set\n",
    "* Identify a high-cardinality column\n",
    "* Issue an approximate `DISTINCT(COUNT)` with approximation turned on\n",
    "* Issue another query with approximation turned off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff80c4-8ca6-4563-b67e-70b09a611877",
   "metadata": {},
   "source": [
    "## COUNT(DISTINCT) queries on sketched data\n",
    "\n",
    "For even faster performance, we can provide Druid with compatible sketches inside the data itself. We do this at ingestion time, pre-populating some dimensions with the sketches that would otherwise have to be computed at query time.\n",
    "\n",
    "This technique also massively reduces the footprint of the data in the database. By storing highly optimized representations of groups of unique values, you avoid storing the individual values themselves.\n",
    "\n",
    "There are two types of Apache Datasketch that allow for `COUNT(DISTINCT)` computations:\n",
    "\n",
    "* [HyperLogLog](https://druid.apache.org/docs/26.0.0/querying/sql-aggregations.html#hll-sketch-functions)\n",
    "* [Theta](https://druid.apache.org/docs/26.0.0/querying/sql-aggregations.html#theta-sketch-functions)\n",
    "\n",
    "A Theta sketch allows for set operations, like intersection and difference, while HyperLogLog (\"HLL\") does not.\n",
    "\n",
    "To understand how this works, let's first create a `GROUP BY` query that generates a datasketch for us to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595eec0-a709-4cd6-985e-eec8a6e37b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT \"Reporting_Airline\", DS_HLL(\"Tail_Number\") AS \"Sketch\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "LIMIT 5\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb0596-2034-4993-a4a2-006d9aa53a9b",
   "metadata": {},
   "source": [
    "In our results, we get a \"human readable\" version of what a sketch looks like.\n",
    "\n",
    "This is thanks to the [`DS_HLL`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#ds_hll) function, which creates a HLL sketch. For a Theta sketch, we can use the [`DS_THETA`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#ds_theta) function.\n",
    "\n",
    "Each sketch represents, in a highly optimized format, the aggregated list of the `Tail_Number`s in the data set. Sketches are _mergable_ which is essential in a massively-parallelised query operation where individual microservices carry out individual calculations that must then be brought together to give a final result.\n",
    "\n",
    "Imagine that our query is executed in parallel on all the data in the database – the sketches, like you see above, are then merged into a final sketch. When presented with the very final _merged_ datasketch, Druid uses the Apache Datasketch library to estimate how many distinct `Tail_Number`s there are in that set, and present the result back to us. This operation is on much less data, and requires much less CPU power than a non-approximate `COUNT(DISTINCT)`, where every row of our `GROUP BY` would have to be passed back to be merged.\n",
    "\n",
    "### Creating sketches during batch ingestion\n",
    "\n",
    "Let's ingest the example flight data into a new table, `flights-counts`, and utilize `GROUP BY` to aggregate all the flight numbers into  one HLL sketch using `DS_HLL`, and (let's go wild!) a Theta sketch using `DS_THETA`.\n",
    "\n",
    "Notice that we no longer store the original field, `Tail_Number`. If we kept that field, the `GROUP BY` wouldn't aggregate any rows into the sketch - there would be a 1:1 relationship between the row and each `Tail_Number` - which is the opposite of what we are designing for! By implication, it will be no longer possible to use the raw data as part of any SQL queries, like `GROUP BY` or `WHERE`.\n",
    "\n",
    "The `GROUP BY` below will generate a sketch _for each_ of the dimensions that we `GROUP BY` - having too many dimensions defeats the purpose of aggregating the data! Therefore the `SELECT` has been crafted to retain only the dimensions our imaginary end users will want to filter or `GROUP BY` the `COUNT(DISTINCT)` data on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213e7b5-14f8-4a6c-a489-8f5cd9c17359",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "REPLACE INTO \"flights-counts\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (SELECT *\n",
    "FROM TABLE(\n",
    "  EXTERN(\n",
    "    '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/flight_on_time/flights/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11.csv.zip\"]}',\n",
    "    '{\"type\":\"csv\",\"findColumnsFromHeader\":true}'\n",
    "  )\n",
    ") EXTEND (\"depaturetime\" VARCHAR, \"arrivalime\" VARCHAR, \"Year\" BIGINT, \"Quarter\" BIGINT, \"Month\" BIGINT, \"DayofMonth\" BIGINT, \"DayOfWeek\" BIGINT, \"FlightDate\" VARCHAR, \"Reporting_Airline\" VARCHAR, \"DOT_ID_Reporting_Airline\" BIGINT, \"IATA_CODE_Reporting_Airline\" VARCHAR, \"Tail_Number\" VARCHAR, \"Flight_Number_Reporting_Airline\" BIGINT, \"OriginAirportID\" BIGINT, \"OriginAirportSeqID\" BIGINT, \"OriginCityMarketID\" BIGINT, \"Origin\" VARCHAR, \"OriginCityName\" VARCHAR, \"OriginState\" VARCHAR, \"OriginStateFips\" BIGINT, \"OriginStateName\" VARCHAR, \"OriginWac\" BIGINT, \"DestAirportID\" BIGINT, \"DestAirportSeqID\" BIGINT, \"DestCityMarketID\" BIGINT, \"Dest\" VARCHAR, \"DestCityName\" VARCHAR, \"DestState\" VARCHAR, \"DestStateFips\" BIGINT, \"DestStateName\" VARCHAR, \"DestWac\" BIGINT, \"CRSDepTime\" BIGINT, \"DepTime\" BIGINT, \"DepDelay\" BIGINT, \"DepDelayMinutes\" BIGINT, \"DepDel15\" BIGINT, \"DepartureDelayGroups\" BIGINT, \"DepTimeBlk\" VARCHAR, \"TaxiOut\" BIGINT, \"WheelsOff\" BIGINT, \"WheelsOn\" BIGINT, \"TaxiIn\" BIGINT, \"CRSArrTime\" BIGINT, \"ArrTime\" BIGINT, \"ArrDelay\" BIGINT, \"ArrDelayMinutes\" BIGINT, \"ArrDel15\" BIGINT, \"ArrivalDelayGroups\" BIGINT, \"ArrTimeBlk\" VARCHAR, \"Cancelled\" BIGINT, \"CancellationCode\" VARCHAR, \"Diverted\" BIGINT, \"CRSElapsedTime\" BIGINT, \"ActualElapsedTime\" BIGINT, \"AirTime\" BIGINT, \"Flights\" BIGINT, \"Distance\" BIGINT, \"DistanceGroup\" BIGINT, \"CarrierDelay\" BIGINT, \"WeatherDelay\" BIGINT, \"NASDelay\" BIGINT, \"SecurityDelay\" BIGINT, \"LateAircraftDelay\" BIGINT, \"FirstDepTime\" VARCHAR, \"TotalAddGTime\" VARCHAR, \"LongestAddGTime\" VARCHAR, \"DivAirportLandings\" VARCHAR, \"DivReachedDest\" VARCHAR, \"DivActualElapsedTime\" VARCHAR, \"DivArrDelay\" VARCHAR, \"DivDistance\" VARCHAR, \"Div1Airport\" VARCHAR, \"Div1AirportID\" VARCHAR, \"Div1AirportSeqID\" VARCHAR, \"Div1WheelsOn\" VARCHAR, \"Div1TotalGTime\" VARCHAR, \"Div1LongestGTime\" VARCHAR, \"Div1WheelsOff\" VARCHAR, \"Div1TailNum\" VARCHAR, \"Div2Airport\" VARCHAR, \"Div2AirportID\" VARCHAR, \"Div2AirportSeqID\" VARCHAR, \"Div2WheelsOn\" VARCHAR, \"Div2TotalGTime\" VARCHAR, \"Div2LongestGTime\" VARCHAR, \"Div2WheelsOff\" VARCHAR, \"Div2TailNum\" VARCHAR, \"Div3Airport\" VARCHAR, \"Div3AirportID\" VARCHAR, \"Div3AirportSeqID\" VARCHAR, \"Div3WheelsOn\" VARCHAR, \"Div3TotalGTime\" VARCHAR, \"Div3LongestGTime\" VARCHAR, \"Div3WheelsOff\" VARCHAR, \"Div3TailNum\" VARCHAR, \"Div4Airport\" VARCHAR, \"Div4AirportID\" VARCHAR, \"Div4AirportSeqID\" VARCHAR, \"Div4WheelsOn\" VARCHAR, \"Div4TotalGTime\" VARCHAR, \"Div4LongestGTime\" VARCHAR, \"Div4WheelsOff\" VARCHAR, \"Div4TailNum\" VARCHAR, \"Div5Airport\" VARCHAR, \"Div5AirportID\" VARCHAR, \"Div5AirportSeqID\" VARCHAR, \"Div5WheelsOn\" VARCHAR, \"Div5TotalGTime\" VARCHAR, \"Div5LongestGTime\" VARCHAR, \"Div5WheelsOff\" VARCHAR, \"Div5TailNum\" VARCHAR, \"Unnamed: 109\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_FLOOR(TIME_PARSE(\"depaturetime\"), 'PT1H') AS \"__time\",\n",
    "  \"Reporting_Airline\",\n",
    "  \"Origin\",\n",
    "  \"Dest\",\n",
    "  COUNT(*) AS \"Events\",\n",
    "  MAX(\"Distance\") AS \"Distance_Max\",\n",
    "  MIN(\"Distance\") AS \"Distance_Min\",\n",
    "  DS_HLL(\"Tail_Number\") AS \"Tail_Number_HLL\",\n",
    "  DS_THETA(\"Tail_Number\") AS \"Tail_Number_THETA\"\n",
    "FROM \"ext\"\n",
    "GROUP BY 1, 2, 3, 4\n",
    "PARTITIONED BY DAY\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586d0f1-e2e4-498b-a98e-1aaa00b65a58",
   "metadata": {},
   "source": [
    "When doing this programmatically we need to be sure to include a context parameter that prompts Druid to store the true sketch value: [`finalizeAggregations`](https://druid.apache.org/docs/26.0.0/multi-stage-query/reference.html#context-parameters). Notice that, if you build an ingestion using the console, these settings are applied for you automatically.\n",
    "\n",
    "Let's add those parameters and then execute the ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58a332-4f86-4d24-a4b6-286b3c4fe54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"finalize\", \"false\")\n",
    "req.add_context(\"finalizeAggregations\", \"false\")\n",
    "\n",
    "sql_client.run_task(req)\n",
    "sql_client.wait_until_ready('flights-counts')\n",
    "display.table('flights-counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db413cc-e2c3-4702-a693-c1285ab1c58a",
   "metadata": {},
   "source": [
    "Open your Druid console's ingestion tab to monitor the progress of the ingestion.\n",
    "\n",
    "Now we can use specific SQL functions that inform Druid to use sketches we have created:\n",
    "\n",
    "* For HLL [`APPROX_COUNT_DISTINCT_DS_HLL`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#approx_count_distinct_ds_hll), and\n",
    "* for Theta [`APPROX_COUNT_DISTINCT_THETA`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#approx_count_distinct_ds_theta).\n",
    "\n",
    "Here's an example query showing our estimated results – notice that we can still use the `FILTER` clause to split results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4a293-b967-46e1-9a37-7856ce25f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "   \"Reporting_Airline\",\n",
    "   SUM(\"Distance_Max\") AS \"Miles_Flown\",\n",
    "   APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Distance_Max\" > 2000) AS \"HLLApprox-over2k\",\n",
    "   APPROX_COUNT_DISTINCT_DS_THETA(\"Tail_Number_THETA\") FILTER (WHERE \"Distance_Max\" < 2000) AS \"ThetaApprox-under2k\"\n",
    "FROM \"flights-counts\"\n",
    "GROUP BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d3cae-1e1d-47d0-9143-91b19d58e17e",
   "metadata": {},
   "source": [
    "Let's have a little bit of fun and use an out-there function afforded by HLL.\n",
    "\n",
    "Remembering that HLL sketches are mergable, we can take multiple sets of results and estimate an overall distinct count.\n",
    "\n",
    "In this query, we generate three HLL sketches covering flights out of three cities in the United States over a three week period. We then merge these together, and estimate how many distinct `Tail_Number`s there were.  You'll recognise the `APPROX_COUNT_DISTINCT_DS_HLL` function and the `DS_HLL` function, generating sketches for the `Tail_Number`s originating in each city. And to that we add the `HLL_SKETCH_UNION` function, which merges each of our result sets. To turn it from a sketch into something readable, we then use the `HLL_SKETCH_ESTIMATE` function to give us a number instead of a sketch.\n",
    "\n",
    "We're then grouping those calculations by weeks by using `TIME_FLOOR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be5c57-bb35-4082-87f4-730c2f79621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "  TIME_FLOOR(\"__time\",'P1W') AS \"Week commencing\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='ATL') AS \"From Atlanta\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='DFW') AS \"From Dallas\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='SFO') AS \"From San Francisco\",\n",
    "  HLL_SKETCH_ESTIMATE(\n",
    "     HLL_SKETCH_UNION(\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='ATL'),\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='DFW'),\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='SFO')\n",
    "      )\n",
    "    ) AS \"From any of the three\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") AS \"From any city\"\n",
    "FROM \"flights-counts\"\n",
    "WHERE TIMESTAMP '2005-10-31' <= __time AND __time <= TIMESTAMP '2005-11-20'\n",
    "GROUP BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940bc114-17a1-4e65-bb47-a8b47469fff6",
   "metadata": {},
   "source": [
    "### Creating sketches during streaming ingestion\n",
    "\n",
    "In streaming ingestion, the same principles apply – you include an entry in the [`metricsSpec`](https://druid.apache.org/docs/26.0.0/ingestion/ingestion-spec.html#metricsspec) part of your ingestion specification, enabling [`queryGranularity`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html#granularityspec) and `rollup` to truncate the time stamp and pre-aggregate the rows.\n",
    "\n",
    "The statement above is equivallent to:\n",
    "\n",
    "```json\n",
    "    {\n",
    "      \"type\": \"HLLSketchBuild\",\n",
    "      \"fieldName\": \"Tail_Number\",\n",
    "      \"lgK\": 12,\n",
    "      \"tgtHllType\": \"HLL_4\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"thetaSketch\",\n",
    "      \"fieldName\": \"Tail_Number\",\n",
    "      \"size\": 16384\n",
    "    }\n",
    "```\n",
    "\n",
    "Notice that here it's easy to see some internal parameters for sketch generation, like the `lgK` value for HLL. In SQL mode, these are exposed as supplementary parameters to the `DS_HLL` function. Be cautious of changing these values without researching the effects - not just in accuracy but also in terms of performance and segment size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a1846-5072-4495-b840-a620de3c0442",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* Approximation is the default execution model for `COUNT(DISTINCT)` queries\n",
    "* You can turn it off with a query context parameter\n",
    "* Accuracy is highly dependent on the distribution and cardinality of data across the database\n",
    "* Druid can be pre-loaded with sketch objects that speed up approximation both in batch and streaming ingestion\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Watch [Employ Approximation](https://youtu.be/fSWwJs1gCvQ?list=PLDZysOZKycN7MZvNxQk_6RbwSJqjSrsNR) by Peter Marshall\n",
    "* Read [Ingesting Data Sketches into Apache Druid](https://blog.hellmar-becker.de/2022/12/26/ingesting-data-sketches-into-apache-druid/) by Hellmar Becker\n",
    "* Read more about the native \"aggregator\" functions for streaming ingestion\n",
    "    * [ThetaSketch function](https://druid.apache.org/docs/26.0.0/development/extensions-core/datasketches-theta.html)\n",
    "    * [HyperLogLog function](https://druid.apache.org/docs/26.0.0/development/extensions-core/datasketches-hll.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
