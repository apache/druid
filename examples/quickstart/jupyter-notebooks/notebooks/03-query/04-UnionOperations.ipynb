{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557e06e8-9b35-4b34-8322-8a8ede6de709",
   "metadata": {},
   "source": [
    "# Performing set operations\n",
    "\n",
    "Users often call for a way to compare or combine multiple sets of results. They may want to merge results into a single list, find where the sets overlap, or see differences. The Druid database contains standard SQL functions as well as those that leverage advanced computer science techniques to speed up these types of calculation through approximation. In this tutorial, work through some examples of different techniques, and see the effect of making it even faster through approximation.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid 26.0.0 or later.\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `druid-jupyter` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see [Docker for Jupyter Notebook tutorials](https://druid.apache.org/docs/latest/tutorials/tutorial-jupyter-docker.html).\n",
    "\n",
    "You must also have loaded the \"FlightCarrierOnTime (1 month)\" sample data, using defaults, into the table `On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11`.\n",
    "\n",
    "If you do not use the Docker Compose environment, you need the following:\n",
    "* A running Druid instance.\n",
    "   * Update the `druid_host` variable to point to your Router endpoint. For example, `druid_host = \"http://localhost:8888\"`.\n",
    "* The following Python packages:\n",
    "   * `druidapi`, a Python client for Apache Druid\n",
    "\n",
    "To start this tutorial, run the next cell. It defines variables for two datasources and the Druid host the tutorial uses. The quickstart deployment configures Druid to listen on port `8888` by default, so you'll make API calls against `http://localhost:8888`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a19226-6abc-436d-ac3c-9c04d6026707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# druid_host iRs the hostname and port for your Druid deployment. \n",
    "# In the Docker Compose tutorial environment, this is the Router\n",
    "# service running at \"http://router:8888\".\n",
    "# If you are not using the Docker Compose environment, edit the `druid_host`.\n",
    "\n",
    "druid_host = \"http://druid-master-0:8888\"\n",
    "druid_host\n",
    "\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display = druid.display\n",
    "sql_client = druid.sql\n",
    "display.tables('INFORMATION_SCHEMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388633f-195b-4381-98cc-7a2f80f48690",
   "metadata": {},
   "source": [
    "## Combining result sets\n",
    "\n",
    "These features of Druid allow us to combine multiple sets of results together to create one single set.\n",
    "\n",
    "### Merging result sets with `UNION ALL`\n",
    "\n",
    "Execute the following query to combine together two different queries - one that contains 10 flights taking off from San Fransisco at around 11 o'clock in the morning, and another with flights departing from Atlanta in the same hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e5184-9fe4-4f21-a471-4e15d16515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "WITH\n",
    "set1 AS (\n",
    "  SELECT\n",
    "  __time,\n",
    "  \"Origin\",\n",
    "  \"Tail_Number\",\n",
    "  \"Flight_Number_Reporting_Airline\"\n",
    "  FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "  WHERE Origin = 'SFO'\n",
    "  AND DATE_TRUNC('HOUR', __time) = TIMESTAMP '2005-11-01 11:00:00'\n",
    "  ORDER BY __time\n",
    "  LIMIT 10\n",
    "  ),\n",
    "set2 AS (\n",
    "SELECT\n",
    "  __time,\n",
    "  \"Origin\",\n",
    "  \"Tail_Number\",\n",
    "  \"Flight_Number_Reporting_Airline\"\n",
    "  FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "  WHERE Origin = 'ATL'\n",
    "  AND DATE_TRUNC('HOUR', __time) = TIMESTAMP '2005-11-01 11:00:00'\n",
    "  ORDER BY __time\n",
    "  LIMIT 10\n",
    "  )\n",
    "  \n",
    "SELECT * from set1\n",
    "UNION ALL\n",
    "SELECT * from set2\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e77fa9-a60c-4955-b763-58d970d7326d",
   "metadata": {},
   "source": [
    "This is what's known as a \"top-level\" `UNION` operation: each set of results was gathered individually, one after the other, and the list of results concatenated.\n",
    "\n",
    "Notice that these results are not in order by time – even though the individual sets did `ORDER BY` time, because the result is a concatenation.\n",
    "\n",
    "For extra detail, run the cell below to see the `EXPLAIN` of the query plan. Notice that there are two `query` parts, with each one being one of our queries.\n",
    "\n",
    "* [Top-level UNION ALL](https://druid.apache.org/docs/latest/querying/sql.html#top-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97934da2-17d1-4c91-8ae3-926cc89185c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json.loads(sql_client.explain_sql(sql)['PLAN']), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b3244-f6bc-4ba6-bb4d-7fb4057d222e",
   "metadata": {},
   "source": [
    "One might think that the solution is to bring back `__time` in each result set as an additional dimension, and to then use `ORDER BY` in a new outer query, like this:\n",
    "\n",
    "```sql\n",
    "SELECT \"Origin\",\n",
    "    \"Tail_Number\",\n",
    "    \"Flight_Number_Reporting_Airline\"\n",
    "FROM (\n",
    "    SELECT * from set1\n",
    "    UNION ALL\n",
    "    SELECT * from set2\n",
    "    )\n",
    "ORDER BY __time\n",
    "```\n",
    "\n",
    "However, when there is a level of abstraction over the concatenated result set, Druid switches how it executes the query, and this adds a number of constraints on what is possible.\n",
    "\n",
    "Take a look at this query. Each set still looks at SFO and ATL, which we expect to `UNION` into a single set, with `__time`, that we can then `ORDER BY` to get a single, time-ordered, result set.\n",
    "\n",
    "Run this cell, however, and you will receive an error from Druid, complete with information about the constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b557a62-623d-4d16-b4a1-3bd6484efe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "WITH\n",
    "set1 AS (\n",
    "  SELECT\n",
    "  __time,\n",
    "  \"Origin\",\n",
    "  \"Tail_Number\",\n",
    "  \"Flight_Number_Reporting_Airline\"\n",
    "  FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "  WHERE Origin = 'SFO'\n",
    "  AND DATE_TRUNC('HOUR', __time) = TIMESTAMP '2005-11-01 11:00:00'\n",
    "  ),\n",
    "set2 AS (\n",
    "SELECT\n",
    "  __time,\n",
    "  \"Origin\",\n",
    "  \"Tail_Number\",\n",
    "  \"Flight_Number_Reporting_Airline\"\n",
    "  FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "  WHERE Origin = 'ATL'\n",
    "  AND DATE_TRUNC('HOUR', __time) = TIMESTAMP '2005-11-01 11:00:00'\n",
    "  )\n",
    "  \n",
    "SELECT __time,\n",
    "    \"Origin\",\n",
    "    \"Tail_Number\",\n",
    "    \"Flight_Number_Reporting_Airline\"\n",
    "FROM (\n",
    "    SELECT * from set1\n",
    "    UNION ALL\n",
    "    SELECT * from set2\n",
    "    )\n",
    "ORDER BY __time\n",
    "'''\n",
    "\n",
    "print(json.dumps(json.loads(sql_client.explain_sql(sql)['PLAN']), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25001794-e1a7-4325-adb3-2b8f26036261",
   "metadata": {},
   "source": [
    "If you'd like the detail on why, run the next cell, which displays the query plan for a working version of the query above. You will see that Druid executes the query on a special `union` datasource, built very simply from entire `table`s, and they cannot have:\n",
    "\n",
    "> expressions, column aliasing, JOIN, GROUP BY, ORDER BY, and so on\n",
    "\n",
    "Learn more here:\n",
    "\n",
    "* [`union` datasources](https://druid.apache.org/docs/26.0.0/querying/datasource.html#union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0fe2a-3dba-4e34-a716-6b753395c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "WITH\n",
    "set1 AS (\n",
    "  SELECT\n",
    "  __time,\n",
    "  \"Origin\",\n",
    "  \"Tail_Number\",\n",
    "  \"Flight_Number_Reporting_Airline\"\n",
    "  FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "  ),\n",
    "set2 AS (\n",
    "SELECT\n",
    "  __time,\n",
    "  \"Origin\",\n",
    "  \"Tail_Number\",\n",
    "  \"Flight_Number_Reporting_Airline\"\n",
    "  FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "  )\n",
    "  \n",
    "SELECT \"Origin\",\n",
    "    \"Tail_Number\",\n",
    "    \"Flight_Number_Reporting_Airline\"\n",
    "FROM (\n",
    "    SELECT * from set1\n",
    "    UNION ALL\n",
    "    SELECT * from set2\n",
    "    )\n",
    "ORDER BY __time\n",
    "'''\n",
    "\n",
    "print(json.dumps(json.loads(sql_client.explain_sql(sql)['PLAN']), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227a38e-0015-4f77-8a57-954ce9ef114c",
   "metadata": {},
   "source": [
    "### Aggregations on result sets with `UNION ALL`\n",
    "\n",
    "Using `UNION ALL` it's possible to concatenate aggregate calculations.\n",
    "\n",
    "Take a look at this cell before you run it – what do you expect to happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f2434-3f32-4040-b7c7-3a185c084db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "WITH\n",
    "set1 AS (\n",
    "  SELECT\n",
    "  \"Origin\",\n",
    "  COUNT(*),\n",
    "  MAX(Distance),\n",
    "  MIN(Distance)\n",
    "  FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "  WHERE Origin = 'SFO'\n",
    "  AND DATE_TRUNC('HOUR', __time) = TIMESTAMP '2005-11-01 11:00:00'\n",
    "  GROUP BY 1\n",
    "  ),\n",
    "set2 AS (\n",
    "SELECT\n",
    "  \"Origin\",\n",
    "  \"Tail_Number\",\n",
    "  \"Flight_Number_Reporting_Airline\"\n",
    "  FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "  WHERE Origin = 'ATL'\n",
    "  AND DATE_TRUNC('HOUR', __time) = TIMESTAMP '2005-11-01 11:00:00'\n",
    "  ORDER BY __time\n",
    "  LIMIT 10\n",
    "  )\n",
    "  \n",
    "SELECT * from set1\n",
    "UNION ALL\n",
    "SELECT * from set2\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85932d8c-7908-4073-8fcc-208c12da1c74",
   "metadata": {},
   "source": [
    "As the `UNION ALL` concatenated the sets, it very simply added the results for Atlanta to the end of the results for San Francisco. It did not take into account that the columns in set 2 were in a different order, nor did it take into account _either_ of the errors in field names.\n",
    "\n",
    "Instead, the query ought to have been more explicit, taking into account the proper field names in each set, ensuring consistency with `set1`'s schema:\n",
    "\n",
    "```sql\n",
    "SELECT \"Flights\", \"Shortest\", \"Longest\" from set1\n",
    "UNION ALL\n",
    "SELECT \"Frights\", \"Shortest\", \"Lengthiest\" from set2\n",
    "```\n",
    "\n",
    "### Merging sets where each element is a single value approximately\n",
    "\n",
    "HyperLogLog is a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03711c7-7e4e-46f6-afb6-4b36a2c3cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "  HLL_SKETCH_ESTIMATE (\n",
    "   HLL_SKETCH_UNION (\n",
    "      DS_HLL(\"user\") FILTER (WHERE DATE_TRUNC('HOUR', __time) = TIMESTAMP '2016-06-27 11:00:00'),\n",
    "      DS_HLL(\"user\") FILTER (WHERE DATE_TRUNC('HOUR', __time) = TIMESTAMP '2016-06-27 12:00:00')\n",
    "      )\n",
    "    ),\n",
    "  THETA_SKETCH_ESTIMATE(\n",
    "   THETA_SKETCH_UNION (\n",
    "      DS_THETA(\"user\") FILTER (WHERE DATE_TRUNC('HOUR', __time) = TIMESTAMP '2016-06-27 11:00:00'),\n",
    "      DS_THETA(\"user\") FILTER (WHERE DATE_TRUNC('HOUR', __time) = TIMESTAMP '2016-06-27 12:00:00')\n",
    "      )\n",
    "    ),\n",
    "  THETA_SKETCH_ESTIMATE(\n",
    "   THETA_SKETCH_NOT (\n",
    "      DS_THETA(\"user\") FILTER (WHERE DATE_TRUNC('HOUR', __time) = TIMESTAMP '2016-06-27 11:00:00'),\n",
    "      DS_THETA(\"user\") FILTER (WHERE DATE_TRUNC('HOUR', __time) = TIMESTAMP '2016-06-27 12:00:00')\n",
    "      )\n",
    "    ),\n",
    "  THETA_SKETCH_ESTIMATE(\n",
    "   THETA_SKETCH_INTERSECT (\n",
    "      DS_THETA(\"user\") FILTER (WHERE DATE_TRUNC('HOUR', __time) = TIMESTAMP '2016-06-27 11:00:00'),\n",
    "      DS_THETA(\"user\") FILTER (WHERE DATE_TRUNC('HOUR', __time) = TIMESTAMP '2016-06-27 12:00:00')\n",
    "      )\n",
    "    )\n",
    "FROM \"wikipedia\"\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37d854-efd9-401a-8726-9949bff0c012",
   "metadata": {},
   "source": [
    "### Running COUNT(DISTINCT) without approximation\n",
    "\n",
    "We can supply a query context parameter, `useApproximateCountDistinct`, to force Druid to not use approximation. We won't get the speed boost afforded by the sketching approach – but that's OK because the example dataset is so small! It would be a different story if `Tail_Number` had high cardinality - like if it was IP Addresses or User Identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652988ac-c256-46d4-a4ea-dbcf0e023991",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT \"Reporting_Airline\", COUNT(DISTINCT \"Tail_Number\") AS \"Events\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "ORDER BY 2\n",
    "'''\n",
    "\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateCountDistinct\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "df = pd.DataFrame(resp.rows)\n",
    "df.plot.bar(x='Reporting_Airline', y='Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c91329-8d05-46eb-8c19-5eaf9043dcb6",
   "metadata": {},
   "source": [
    "### Comparing approximate and non-approximate results\n",
    "\n",
    "On the surface, these do not _look_ different. And, in a lot of user interfaces, that's perfectly fine!\n",
    "\n",
    "The next cell will run the query in two modes – accurate and approximate. It then displays a `diff` between the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a031f-a805-45dd-935b-d8af808041a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT \"Reporting_Airline\", COUNT(DISTINCT \"Tail_Number\") AS \"Events\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "ORDER BY 2\n",
    "'''\n",
    "\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"useApproximateCountDistinct\", \"false\")\n",
    "resp = sql_client.sql_query(req)\n",
    "\n",
    "df1 = pd.DataFrame(sql_client.sql(sql))\n",
    "df2 = pd.DataFrame(resp.rows)\n",
    "\n",
    "df3 = df1.compare(df2, keep_equal=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3320d-d4ec-460a-b1fc-59c98f85cc3a",
   "metadata": {},
   "source": [
    "There are _value_ errors, as you might expect with approximation. This therefore affects _ordering_ of results.\n",
    "\n",
    "Error in sketch-based approximation is probabilistic, rather than guaranteed. That's to say that a certain percentage of the time you can expect the measurements you take to be within a certain distance of the true value. Also, their size is not dependent on the data – the default size of a sketch in Druid is just over 2000 bytes.\n",
    "\n",
    "Approximation is especially helpful for very high cardinality data. When there are hundreds of thousands, millions, even tens-of-millions of distinct values, passing the individual distinct values to be merged takes longer and more data storage than using datasketches.\n",
    "\n",
    "As an experiment, you may want to:\n",
    "\n",
    "* Ingest or use a much larger data set\n",
    "* Identify a high-cardinality column\n",
    "* Issue an approximate `DISTINCT(COUNT)` with approximation turned on\n",
    "* Issue another query with approximation turned off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff80c4-8ca6-4563-b67e-70b09a611877",
   "metadata": {},
   "source": [
    "## COUNT(DISTINCT) queries on sketched data\n",
    "\n",
    "For even faster performance, we can provide Druid with compatible sketches inside the data itself. We do this at ingestion time, pre-populating some dimensions with the sketches that would otherwise have to be computed at query time.\n",
    "\n",
    "This technique also massively reduces the footprint of the data in the database. By storing highly optimized representations of groups of unique values, you avoid storing the individual values themselves.\n",
    "\n",
    "There are two types of Apache Datasketch that allow for `COUNT(DISTINCT)` computations:\n",
    "\n",
    "* [HyperLogLog](https://druid.apache.org/docs/26.0.0/querying/sql-aggregations.html#hll-sketch-functions)\n",
    "* [Theta](https://druid.apache.org/docs/26.0.0/querying/sql-aggregations.html#theta-sketch-functions)\n",
    "\n",
    "A Theta sketch allows for set operations, like intersection and difference, while HyperLogLog (\"HLL\") does not.\n",
    "\n",
    "To illustrate how this works, the next cell uses a `GROUP BY` query and generates a datasketch with a `DS_HLL` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595eec0-a709-4cd6-985e-eec8a6e37b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT \"Reporting_Airline\", DS_HLL(\"Tail_Number\") AS \"Sketch\"\n",
    "FROM \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11\"\n",
    "GROUP BY 1\n",
    "LIMIT 5\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb0596-2034-4993-a4a2-006d9aa53a9b",
   "metadata": {},
   "source": [
    "In our results, we get a \"human readable\" version of what a sketch looks like.\n",
    "\n",
    "This is thanks to the [`DS_HLL`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#ds_hll) function, which creates a HLL sketch. For a Theta sketch, we can use the [`DS_THETA`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#ds_theta) function.\n",
    "\n",
    "Each sketch represents, in a highly optimized format, the aggregated list of the `Tail_Number`s in the data set. Sketches are _mergable_ which is essential in a massively-parallelised query operation where individual microservices carry out individual calculations that must then be brought together to give a final result.\n",
    "\n",
    "Imagine that our query is executed in parallel on all the data in the database – the sketches, like you see above, are then merged into a final sketch. When presented with the very final _merged_ datasketch, Druid uses the Apache Datasketch library to estimate how many distinct `Tail_Number`s there are in that set, and present the result back to us. This operation is on much less data, and requires much less CPU power than a non-approximate `COUNT(DISTINCT)`, where every row of our `GROUP BY` would have to be passed back to be merged.\n",
    "\n",
    "### Creating sketches during batch ingestion\n",
    "\n",
    "The next cell ingests the example flight data into a new table, `flights-counts`, and utilizes a `GROUP BY` to aggregate all the flight numbers into two sketches: a HLL sketch using `DS_HLL` and a Theta sketch using `DS_THETA`.\n",
    "\n",
    "Notice that we no longer store the original field, `Tail_Number`. If we kept that field, the `GROUP BY` wouldn't aggregate any rows into the sketch - there would be a 1:1 relationship between the row and each `Tail_Number` - which is the opposite of what we are designing for! By implication, it will be no longer possible to use the raw data as part of any SQL queries, like `GROUP BY` or `WHERE`.\n",
    "\n",
    "The `GROUP BY` below will generate a sketch _for each_ of the dimensions that we `GROUP BY` - having too many dimensions defeats the purpose of aggregating the data! Therefore the `SELECT` has been crafted to retain only the dimensions our imaginary end users will want to filter or `GROUP BY` the `COUNT(DISTINCT)` data on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213e7b5-14f8-4a6c-a489-8f5cd9c17359",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "REPLACE INTO \"flights-counts\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (SELECT *\n",
    "FROM TABLE(\n",
    "  EXTERN(\n",
    "    '{\"type\":\"http\",\"uris\":[\"https://static.imply.io/example-data/flight_on_time/flights/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2005_11.csv.zip\"]}',\n",
    "    '{\"type\":\"csv\",\"findColumnsFromHeader\":true}'\n",
    "  )\n",
    ") EXTEND (\"depaturetime\" VARCHAR, \"arrivalime\" VARCHAR, \"Year\" BIGINT, \"Quarter\" BIGINT, \"Month\" BIGINT, \"DayofMonth\" BIGINT, \"DayOfWeek\" BIGINT, \"FlightDate\" VARCHAR, \"Reporting_Airline\" VARCHAR, \"DOT_ID_Reporting_Airline\" BIGINT, \"IATA_CODE_Reporting_Airline\" VARCHAR, \"Tail_Number\" VARCHAR, \"Flight_Number_Reporting_Airline\" BIGINT, \"OriginAirportID\" BIGINT, \"OriginAirportSeqID\" BIGINT, \"OriginCityMarketID\" BIGINT, \"Origin\" VARCHAR, \"OriginCityName\" VARCHAR, \"OriginState\" VARCHAR, \"OriginStateFips\" BIGINT, \"OriginStateName\" VARCHAR, \"OriginWac\" BIGINT, \"DestAirportID\" BIGINT, \"DestAirportSeqID\" BIGINT, \"DestCityMarketID\" BIGINT, \"Dest\" VARCHAR, \"DestCityName\" VARCHAR, \"DestState\" VARCHAR, \"DestStateFips\" BIGINT, \"DestStateName\" VARCHAR, \"DestWac\" BIGINT, \"CRSDepTime\" BIGINT, \"DepTime\" BIGINT, \"DepDelay\" BIGINT, \"DepDelayMinutes\" BIGINT, \"DepDel15\" BIGINT, \"DepartureDelayGroups\" BIGINT, \"DepTimeBlk\" VARCHAR, \"TaxiOut\" BIGINT, \"WheelsOff\" BIGINT, \"WheelsOn\" BIGINT, \"TaxiIn\" BIGINT, \"CRSArrTime\" BIGINT, \"ArrTime\" BIGINT, \"ArrDelay\" BIGINT, \"ArrDelayMinutes\" BIGINT, \"ArrDel15\" BIGINT, \"ArrivalDelayGroups\" BIGINT, \"ArrTimeBlk\" VARCHAR, \"Cancelled\" BIGINT, \"CancellationCode\" VARCHAR, \"Diverted\" BIGINT, \"CRSElapsedTime\" BIGINT, \"ActualElapsedTime\" BIGINT, \"AirTime\" BIGINT, \"Flights\" BIGINT, \"Distance\" BIGINT, \"DistanceGroup\" BIGINT, \"CarrierDelay\" BIGINT, \"WeatherDelay\" BIGINT, \"NASDelay\" BIGINT, \"SecurityDelay\" BIGINT, \"LateAircraftDelay\" BIGINT, \"FirstDepTime\" VARCHAR, \"TotalAddGTime\" VARCHAR, \"LongestAddGTime\" VARCHAR, \"DivAirportLandings\" VARCHAR, \"DivReachedDest\" VARCHAR, \"DivActualElapsedTime\" VARCHAR, \"DivArrDelay\" VARCHAR, \"DivDistance\" VARCHAR, \"Div1Airport\" VARCHAR, \"Div1AirportID\" VARCHAR, \"Div1AirportSeqID\" VARCHAR, \"Div1WheelsOn\" VARCHAR, \"Div1TotalGTime\" VARCHAR, \"Div1LongestGTime\" VARCHAR, \"Div1WheelsOff\" VARCHAR, \"Div1TailNum\" VARCHAR, \"Div2Airport\" VARCHAR, \"Div2AirportID\" VARCHAR, \"Div2AirportSeqID\" VARCHAR, \"Div2WheelsOn\" VARCHAR, \"Div2TotalGTime\" VARCHAR, \"Div2LongestGTime\" VARCHAR, \"Div2WheelsOff\" VARCHAR, \"Div2TailNum\" VARCHAR, \"Div3Airport\" VARCHAR, \"Div3AirportID\" VARCHAR, \"Div3AirportSeqID\" VARCHAR, \"Div3WheelsOn\" VARCHAR, \"Div3TotalGTime\" VARCHAR, \"Div3LongestGTime\" VARCHAR, \"Div3WheelsOff\" VARCHAR, \"Div3TailNum\" VARCHAR, \"Div4Airport\" VARCHAR, \"Div4AirportID\" VARCHAR, \"Div4AirportSeqID\" VARCHAR, \"Div4WheelsOn\" VARCHAR, \"Div4TotalGTime\" VARCHAR, \"Div4LongestGTime\" VARCHAR, \"Div4WheelsOff\" VARCHAR, \"Div4TailNum\" VARCHAR, \"Div5Airport\" VARCHAR, \"Div5AirportID\" VARCHAR, \"Div5AirportSeqID\" VARCHAR, \"Div5WheelsOn\" VARCHAR, \"Div5TotalGTime\" VARCHAR, \"Div5LongestGTime\" VARCHAR, \"Div5WheelsOff\" VARCHAR, \"Div5TailNum\" VARCHAR, \"Unnamed: 109\" VARCHAR))\n",
    "SELECT\n",
    "  TIME_FLOOR(TIME_PARSE(\"depaturetime\"), 'PT1H') AS \"__time\",\n",
    "  \"Reporting_Airline\",\n",
    "  \"Origin\",\n",
    "  \"Dest\",\n",
    "  COUNT(*) AS \"Events\",\n",
    "  MAX(\"Distance\") AS \"Distance_Max\",\n",
    "  MIN(\"Distance\") AS \"Distance_Min\",\n",
    "  DS_HLL(\"Tail_Number\") AS \"Tail_Number_HLL\",\n",
    "  DS_THETA(\"Tail_Number\") AS \"Tail_Number_THETA\"\n",
    "FROM \"ext\"\n",
    "GROUP BY 1, 2, 3, 4\n",
    "PARTITIONED BY DAY\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586d0f1-e2e4-498b-a98e-1aaa00b65a58",
   "metadata": {},
   "source": [
    "When doing this programmatically we need to be sure to include a context parameter that prompts Druid to store the true sketch value: [`finalizeAggregations`](https://druid.apache.org/docs/26.0.0/multi-stage-query/reference.html#context-parameters). Notice that, if you build an ingestion using the console, these settings are applied for you automatically.\n",
    "\n",
    "The following cell adds the parameters and then executes the ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58a332-4f86-4d24-a4b6-286b3c4fe54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"finalize\", \"false\")\n",
    "req.add_context(\"finalizeAggregations\", \"false\")\n",
    "\n",
    "sql_client.run_task(req)\n",
    "sql_client.wait_until_ready('flights-counts')\n",
    "display.table('flights-counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db413cc-e2c3-4702-a693-c1285ab1c58a",
   "metadata": {},
   "source": [
    "Open your Druid console's ingestion tab to monitor the progress of the ingestion.\n",
    "\n",
    "Now we can use specific SQL functions that inform Druid to use sketches we have created:\n",
    "\n",
    "* For HLL [`APPROX_COUNT_DISTINCT_DS_HLL`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#approx_count_distinct_ds_hll), and\n",
    "* for Theta [`APPROX_COUNT_DISTINCT_THETA`](https://druid.apache.org/docs/26.0.0/querying/sql-functions.html#approx_count_distinct_ds_theta).\n",
    "\n",
    "Here's an example query showing our estimated results – notice that we can still use the `FILTER` clause to split results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4a293-b967-46e1-9a37-7856ce25f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "   \"Reporting_Airline\",\n",
    "   SUM(\"Distance_Max\") AS \"Miles_Flown\",\n",
    "   APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Distance_Max\" > 2000) AS \"HLLApprox-over2k\",\n",
    "   APPROX_COUNT_DISTINCT_DS_THETA(\"Tail_Number_THETA\") FILTER (WHERE \"Distance_Max\" < 2000) AS \"ThetaApprox-under2k\"\n",
    "FROM \"flights-counts\"\n",
    "GROUP BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d3cae-1e1d-47d0-9143-91b19d58e17e",
   "metadata": {},
   "source": [
    "Remembering that HLL sketches are mergable, we can take multiple sets of results and estimate an overall distinct count.\n",
    "\n",
    "In this query, we generate three HLL sketches covering flights out of three cities in the United States over a three week period. We then merge these together, and estimate how many distinct `Tail_Number`s there were.  You'll recognise the `APPROX_COUNT_DISTINCT_DS_HLL` function and the `DS_HLL` function, generating sketches for the `Tail_Number`s originating in each city. And to that we add the `HLL_SKETCH_UNION` function, which merges each of our result sets. To turn it from a sketch into something readable, we then use the `HLL_SKETCH_ESTIMATE` function to give us a number instead of a sketch.\n",
    "\n",
    "We're then grouping those calculations by weeks by using `TIME_FLOOR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be5c57-bb35-4082-87f4-730c2f79621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "SELECT\n",
    "  TIME_FLOOR(\"__time\",'P1W') AS \"Week commencing\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='ATL') AS \"From Atlanta\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='DFW') AS \"From Dallas\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='SFO') AS \"From San Francisco\",\n",
    "  HLL_SKETCH_ESTIMATE(\n",
    "     HLL_SKETCH_UNION(\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='ATL'),\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='DFW'),\n",
    "       DS_HLL(\"Tail_Number_HLL\") FILTER (WHERE \"Origin\"='SFO')\n",
    "      )\n",
    "    ) AS \"From any of the three\",\n",
    "  APPROX_COUNT_DISTINCT_DS_HLL(\"Tail_Number_HLL\") AS \"From any city\"\n",
    "FROM \"flights-counts\"\n",
    "WHERE TIMESTAMP '2005-10-31' <= __time AND __time <= TIMESTAMP '2005-11-20'\n",
    "GROUP BY 1\n",
    "'''\n",
    "\n",
    "display.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940bc114-17a1-4e65-bb47-a8b47469fff6",
   "metadata": {},
   "source": [
    "### Creating sketches during streaming ingestion\n",
    "\n",
    "In streaming ingestion, the same principles apply – you include an entry in the [`metricsSpec`](https://druid.apache.org/docs/26.0.0/ingestion/ingestion-spec.html#metricsspec) part of your ingestion specification, enabling [`queryGranularity`](https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html#granularityspec) and `rollup` to truncate the time stamp and pre-aggregate the rows.\n",
    "\n",
    "The statement above is equivallent to:\n",
    "\n",
    "```json\n",
    "    {\n",
    "      \"type\": \"HLLSketchBuild\",\n",
    "      \"fieldName\": \"Tail_Number\",\n",
    "      \"lgK\": 12,\n",
    "      \"tgtHllType\": \"HLL_4\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"thetaSketch\",\n",
    "      \"fieldName\": \"Tail_Number\",\n",
    "      \"size\": 16384\n",
    "    }\n",
    "```\n",
    "\n",
    "Notice that here it's easy to see some internal parameters for sketch generation, like the `lgK` value for HLL. In SQL mode, these are exposed as supplementary parameters to the `DS_HLL` function. Be cautious of changing these values without researching the effects - not just in accuracy but also in terms of performance and segment size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a1846-5072-4495-b840-a620de3c0442",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* Approximation is the default execution model for `COUNT(DISTINCT)` queries\n",
    "* You can turn it off with a query context parameter\n",
    "* Accuracy is highly dependent on the distribution and cardinality of data across the database\n",
    "* Druid can be pre-loaded with sketch objects that speed up approximation both in batch and streaming ingestion\n",
    "\n",
    "## Learn more\n",
    "\n",
    "* Watch [Employ Approximation](https://youtu.be/fSWwJs1gCvQ?list=PLDZysOZKycN7MZvNxQk_6RbwSJqjSrsNR) by Peter Marshall\n",
    "* Read [Ingesting Data Sketches into Apache Druid](https://blog.hellmar-becker.de/2022/12/26/ingesting-data-sketches-into-apache-druid/) by Hellmar Becker\n",
    "* Read more about the native \"aggregator\" functions for streaming ingestion\n",
    "    * [ThetaSketch function](https://druid.apache.org/docs/26.0.0/development/extensions-core/datasketches-theta.html)\n",
    "    * [HyperLogLog function](https://druid.apache.org/docs/26.0.0/development/extensions-core/datasketches-hll.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
