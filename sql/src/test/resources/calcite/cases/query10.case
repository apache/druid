Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
==============================================================
Converted from testFilterOnTimeFloorComparisonMisaligned()
=== case
Filter on time floor comparison misaligned
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE FLOOR(__time TO MONTH) < TIMESTAMP '2000-02-01 00:00:01'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[<(FLOOR($0, FLAG(MONTH)), 2000-02-01 00:00:01)])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/2000-03-01T00:00:00.000Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[3]
==============================================================
Converted from testFilterOnTimeExtract()
Cannot vectorize due to expression filter.
=== case
Filter on time extract
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE EXTRACT(YEAR FROM __time) = 2000
  AND EXTRACT(MONTH FROM __time) = 1
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[AND(=(EXTRACT(FLAG(YEAR), $0), 2000), =(EXTRACT(FLAG(MONTH), $0), 1))])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "timestamp_extract(\"__time\",'YEAR','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v1",
    "expression" : "timestamp_extract(\"__time\",'MONTH','UTC')",
    "outputType" : "LONG"
  } ],
  "filter" : {
    "type" : "and",
    "fields" : [ {
      "type" : "selector",
      "dimension" : "v0",
      "value" : "2000"
    }, {
      "type" : "selector",
      "dimension" : "v1",
      "value" : "1"
    } ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[3]
==============================================================
Converted from testFilterOnTimeExtractWithMultipleDays()
Cannot vectorize due to expression filters.
=== case
Filter on time extract with multiple days
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE EXTRACT(YEAR FROM __time) = 2000
  AND EXTRACT(DAY FROM __time) IN (2, 3, 5)
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[AND(=(EXTRACT(FLAG(YEAR), $0), 2000), OR(=(EXTRACT(FLAG(DAY), $0), 2), =(EXTRACT(FLAG(DAY), $0), 3), =(EXTRACT(FLAG(DAY), $0), 5)))])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "timestamp_extract(\"__time\",'YEAR','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v1",
    "expression" : "timestamp_extract(\"__time\",'DAY','UTC')",
    "outputType" : "LONG"
  } ],
  "filter" : {
    "type" : "and",
    "fields" : [ {
      "type" : "selector",
      "dimension" : "v0",
      "value" : "2000"
    }, {
      "type" : "in",
      "dimension" : "v1",
      "values" : [ "2", "3", "5" ]
    } ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[2]
==============================================================
Converted from testFilterOnTimeExtractWithVariousTimeUnits()
Cannot vectorize due to virtual columns.
=== case
Filter on time extract with various time units
=== SQL
SELECT COUNT(*) FROM druid.foo4
WHERE EXTRACT(YEAR FROM __time) = 2000
  AND EXTRACT(MICROSECOND FROM __time) = 946723
  AND EXTRACT(MILLISECOND FROM __time) = 695
  AND EXTRACT(ISODOW FROM __time) = 6
  AND EXTRACT(ISOYEAR FROM __time) = 2000
  AND EXTRACT(DECADE FROM __time) = 200
  AND EXTRACT(CENTURY FROM __time) = 20
  AND EXTRACT(MILLENNIUM FROM __time) = 2
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[AND(=(EXTRACT(FLAG(YEAR), $0), 2000), =(EXTRACT(FLAG(MICROSECOND), $0), 946723), =(EXTRACT(FLAG(MILLISECOND), $0), 695), =(EXTRACT(FLAG(ISODOW), $0), 6), =(EXTRACT(FLAG(ISOYEAR), $0), 2000), =(EXTRACT(FLAG(DECADE), $0), 200), =(EXTRACT(FLAG(CENTURY), $0), 20), =(EXTRACT(FLAG(MILLENNIUM), $0), 2))])
      LogicalTableScan(table=[[druid, foo4]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo4"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "timestamp_extract(\"__time\",'YEAR','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v1",
    "expression" : "timestamp_extract(\"__time\",'MICROSECOND','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v2",
    "expression" : "timestamp_extract(\"__time\",'MILLISECOND','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v3",
    "expression" : "timestamp_extract(\"__time\",'ISODOW','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v4",
    "expression" : "timestamp_extract(\"__time\",'ISOYEAR','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v5",
    "expression" : "timestamp_extract(\"__time\",'DECADE','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v6",
    "expression" : "timestamp_extract(\"__time\",'CENTURY','UTC')",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v7",
    "expression" : "timestamp_extract(\"__time\",'MILLENNIUM','UTC')",
    "outputType" : "LONG"
  } ],
  "filter" : {
    "type" : "and",
    "fields" : [ {
      "type" : "selector",
      "dimension" : "v0",
      "value" : "2000"
    }, {
      "type" : "selector",
      "dimension" : "v1",
      "value" : "946723"
    }, {
      "type" : "selector",
      "dimension" : "v2",
      "value" : "695"
    }, {
      "type" : "selector",
      "dimension" : "v3",
      "value" : "6"
    }, {
      "type" : "selector",
      "dimension" : "v4",
      "value" : "2000"
    }, {
      "type" : "selector",
      "dimension" : "v5",
      "value" : "200"
    }, {
      "type" : "selector",
      "dimension" : "v6",
      "value" : "20"
    }, {
      "type" : "selector",
      "dimension" : "v7",
      "value" : "2"
    } ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[1]
==============================================================
Converted from testFilterOnTimeFloorMisaligned()
=== case
Filter on time floor misaligned
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE floor(__time TO month) = TIMESTAMP '2000-01-01 00:00:01'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[=(FLOOR($0, FLAG(MONTH)), 2000-01-01 00:00:01)])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[0]
==============================================================
Converted from testGroupByFloor()
=== case
Group by floor
=== SQL
SELECT floor(CAST(dim1 AS float)), COUNT(*)
FROM druid.foo
GROUP BY floor(CAST(dim1 AS float))
=== options
vectorize=true
=== schema
EXPR$0 FLOAT
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
  LogicalProject(EXPR$0=[FLOOR(CAST($2):FLOAT)])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "floor(CAST(\"dim1\", 'DOUBLE'))",
    "outputType" : "FLOAT"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "v0",
    "outputName" : "d0",
    "outputType" : "FLOAT"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[0.0,3]
[1.0,1]
[2.0,1]
[10.0,1]
=== run
=== options
sqlCompatibleNulls=true
=== results
[null,3]
[1.0,1]
[2.0,1]
[10.0,1]
==============================================================
Converted from testQueryWithSelectProjectAndIdentityProjectDoesNotRename()
=== case
Query with select project and identity project does not rename
=== SQL
SELECT
  (SUM(CASE WHEN (TIMESTAMP '2000-01-04 17:00:00'<=__time AND __time<TIMESTAMP '2022-01-05 17:00:00') THEN 1
            ELSE 0 END)*1.0 /
       COUNT(DISTINCT CASE WHEN (TIMESTAMP '2000-01-04 17:00:00'<=__time AND __time<TIMESTAMP '2022-01-05 17:00:00') THEN dim1 END))
FROM druid.foo
GROUP BY ()
=== options
mergeBufferCount=3
planner.useApproximateCountDistinct=false
planner.useGroupingSetForExactDistinct=true
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 DECIMAL(19, 1)
=== plan
LogicalProject(EXPR$0=[/(*($0, 1.0:DECIMAL(2, 1)), $1)])
  LogicalAggregate(group=[{}], agg#0=[SUM($0)], agg#1=[COUNT(DISTINCT $1)])
    LogicalProject($f0=[CASE(AND(<=(2000-01-04 17:00:00, $0), <($0, 2022-01-05 17:00:00)), 1, 0)], $f1=[CASE(AND(<=(2000-01-04 17:00:00, $0), <($0, 2022-01-05 17:00:00)), $2, null:VARCHAR)])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "query",
    "query" : {
      "queryType" : "groupBy",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "virtualColumns" : [ {
        "type" : "expression",
        "name" : "v0",
        "expression" : "case_searched(((947005200000 <= \"__time\") && (\"__time\" < 1641402000000)),\"dim1\",null)",
        "outputType" : "STRING"
      }, {
        "type" : "expression",
        "name" : "v1",
        "expression" : "case_searched(((947005200000 <= \"__time\") && (\"__time\" < 1641402000000)),1,0)",
        "outputType" : "LONG"
      } ],
      "granularity" : {
        "type" : "all"
      },
      "dimensions" : [ {
        "type" : "default",
        "dimension" : "v0",
        "outputName" : "d0",
        "outputType" : "STRING"
      } ],
      "aggregations" : [ {
        "type" : "longSum",
        "name" : "a0",
        "fieldName" : "v1"
      }, {
        "type" : "grouping",
        "name" : "a1",
        "groupings" : [ "v0" ]
      } ],
      "limitSpec" : {
        "type" : "NoopLimitSpec"
      },
      "subtotalsSpec" : [ [ "d0" ], [ ] ]
    }
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ ],
  "aggregations" : [ {
    "type" : "filtered",
    "aggregator" : {
      "type" : "longMin",
      "name" : "_a0",
      "fieldName" : "a0"
    },
    "filter" : {
      "type" : "selector",
      "dimension" : "a1",
      "value" : "1"
    },
    "name" : "_a0"
  }, {
    "type" : "filtered",
    "aggregator" : {
      "type" : "count",
      "name" : "_a1"
    },
    "filter" : {
      "type" : "and",
      "fields" : [ {
        "type" : "not",
        "field" : {
          "type" : "selector",
          "dimension" : "d0",
          "value" : null
        }
      }, {
        "type" : "selector",
        "dimension" : "a1",
        "value" : "0"
      } ]
    },
    "name" : "_a1"
  } ],
  "postAggregations" : [ {
    "type" : "expression",
    "name" : "p0",
    "expression" : "((\"_a0\" * 1.0) / \"_a1\")"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== results
[1.0]
==============================================================
Converted from testGroupByFloorWithOrderBy()
=== case
Group by floor with order by
=== SQL
SELECT floor(CAST(dim1 AS float)) AS fl, COUNT(*)
FROM druid.foo
GROUP BY floor(CAST(dim1 AS float))
ORDER BY fl DESC
=== options
vectorize=true
=== schema
fl FLOAT
EXPR$1 BIGINT
=== plan
LogicalSort(sort0=[$0], dir0=[DESC])
  LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
    LogicalProject(fl=[FLOOR(CAST($2):FLOAT)])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "floor(CAST(\"dim1\", 'DOUBLE'))",
    "outputType" : "FLOAT"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "v0",
    "outputName" : "d0",
    "outputType" : "FLOAT"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "default",
    "columns" : [ {
      "dimension" : "d0",
      "direction" : "descending",
      "dimensionOrder" : {
        "type" : "numeric"
      }
    } ]
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[10.0,1]
[2.0,1]
[1.0,1]
[0.0,3]
=== run
=== options
sqlCompatibleNulls=true
=== results
[10.0,1]
[2.0,1]
[1.0,1]
[null,3]
==============================================================
Converted from testGroupByFloorTimeAndOneOtherDimensionWithOrderBy()
=== case
Group by floor time and one other dimension with order by
=== SQL
SELECT floor(__time TO year), dim2, COUNT(*)
FROM druid.foo
GROUP BY floor(__time TO year), dim2
ORDER BY floor(__time TO year), dim2, COUNT(*) DESC
=== options
vectorize=true
=== schema
EXPR$0 TIMESTAMP(3)
dim2 VARCHAR
EXPR$2 BIGINT
=== plan
LogicalSort(sort0=[$0], sort1=[$1], sort2=[$2], dir0=[ASC], dir1=[ASC], dir2=[DESC])
  LogicalAggregate(group=[{0, 1}], EXPR$2=[COUNT()])
    LogicalProject(EXPR$0=[FLOOR($0, FLAG(YEAR))], dim2=[$3])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "timestamp_floor(\"__time\",'P1Y',null,'UTC')",
    "outputType" : "LONG"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "v0",
    "outputName" : "d0",
    "outputType" : "LONG"
  }, {
    "type" : "default",
    "dimension" : "dim2",
    "outputName" : "d1",
    "outputType" : "STRING"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "default",
    "columns" : [ {
      "dimension" : "d0",
      "direction" : "ascending",
      "dimensionOrder" : {
        "type" : "numeric"
      }
    }, {
      "dimension" : "d1",
      "direction" : "ascending",
      "dimensionOrder" : {
        "type" : "lexicographic"
      }
    }, {
      "dimension" : "a0",
      "direction" : "descending",
      "dimensionOrder" : {
        "type" : "numeric"
      }
    } ]
  },
  "context" : {
    "timestampResultField" : "d0",
    "timestampResultFieldGranularity" : "YEAR",
    "timestampResultFieldInOriginalDimensions" : 0
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[946684800000,"",2]
[946684800000,"a",1]
[978307200000,"",1]
[978307200000,"a",1]
[978307200000,"abc",1]
=== run
=== options
sqlCompatibleNulls=true
=== results
[946684800000,null,1]
[946684800000,"",1]
[946684800000,"a",1]
[978307200000,null,1]
[978307200000,"a",1]
[978307200000,"abc",1]
==============================================================
Converted from testGroupByStringLength()
Cannot vectorize due to virtual columns.
=== case
Group by string length
=== SQL
SELECT CHARACTER_LENGTH(dim1), COUNT(*)
FROM druid.foo
GROUP BY CHARACTER_LENGTH(dim1)
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 INTEGER
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
  LogicalProject(EXPR$0=[CHAR_LENGTH($2)])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "strlen(\"dim1\")",
    "outputType" : "LONG"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "v0",
    "outputName" : "d0",
    "outputType" : "LONG"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== results
[0,1]
[1,2]
[3,2]
[4,1]
==============================================================
Converted from testFilterAndGroupByLookup()
Cannot vectorize due to extraction dimension specs.
=== case
Filter and group by lookup
=== SQL
SELECT LOOKUP(dim1, 'lookyloo'), COUNT(*)
FROM foo
WHERE LOOKUP(dim1, 'lookyloo') <> 'xxx'
GROUP BY LOOKUP(dim1, 'lookyloo')
=== options
vectorize=false
=== schema
EXPR$0 VARCHAR
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
  LogicalProject(EXPR$0=[LOOKUP($2, 'lookyloo')])
    LogicalFilter(condition=[<>(LOOKUP($2, 'lookyloo'), 'xxx')])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "not",
    "field" : {
      "type" : "selector",
      "dimension" : "dim1",
      "value" : "xxx",
      "extractionFn" : {
        "type" : "registeredLookup",
        "lookup" : "lookyloo",
        "retainMissingValue" : false,
        "replaceMissingValueWith" : null,
        "injective" : null,
        "optimize" : true
      }
    }
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "extraction",
    "dimension" : "dim1",
    "outputName" : "d0",
    "outputType" : "STRING",
    "extractionFn" : {
      "type" : "registeredLookup",
      "lookup" : "lookyloo",
      "retainMissingValue" : false,
      "replaceMissingValueWith" : null,
      "injective" : null,
      "optimize" : true
    }
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
["",5]
["xabc",1]
=== run
=== options
sqlCompatibleNulls=true
=== results
[null,5]
["xabc",1]
==============================================================
Converted from testCountDistinctOfLookup()
Cannot vectorize due to extraction dimension spec.
=== case
Count distinct of lookup
=== SQL
SELECT COUNT(DISTINCT LOOKUP(dim1, 'lookyloo'))
FROM foo
=== options
vectorize=false
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0)])
  LogicalProject($f0=[LOOKUP($2, 'lookyloo')])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "cardinality",
    "name" : "a0",
    "fields" : [ {
      "type" : "extraction",
      "dimension" : "dim1",
      "outputName" : "dim1",
      "outputType" : "STRING",
      "extractionFn" : {
        "type" : "registeredLookup",
        "lookup" : "lookyloo",
        "retainMissingValue" : false,
        "replaceMissingValueWith" : null,
        "injective" : null,
        "optimize" : true
      }
    } ],
    "byRow" : false,
    "round" : true
  } ]
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[2]
=== run
=== options
sqlCompatibleNulls=true
=== results
[1]
==============================================================
Converted from testGroupByExpressionFromLookup()
Cannot vectorize direct queries on lookup tables.
=== case
Group by expression from lookup
=== SQL
SELECT SUBSTRING(v, 1, 1), COUNT(*)
FROM lookup.lookyloo
GROUP BY 1
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 VARCHAR
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
  LogicalProject(EXPR$0=[SUBSTRING($1, 1, 1)])
    LogicalTableScan(table=[[lookup, lookyloo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "lookup",
    "lookup" : "lookyloo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "extraction",
    "dimension" : "v",
    "outputName" : "d0",
    "outputType" : "STRING",
    "extractionFn" : {
      "type" : "substring",
      "index" : 0,
      "length" : 1
    }
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== results
["m",1]
["x",3]
==============================================================
Converted from testTimeseries()
=== case
Timeseries
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT floor(__time TO month) AS gran,
  cnt FROM druid.foo
  ) AS x
GROUP BY gran
ORDER BY gran
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
gran TIMESTAMP(3)
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[FLOOR($0, FLAG(MONTH))], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : "MONTH",
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "timestampResultField" : "d0"
  }
}
=== results
[3,946684800000]
[3,978307200000]
==============================================================
Converted from testFilteredTimeAggregators()
=== case
Filtered time aggregators
=== SQL
SELECT
  SUM(cnt) FILTER(WHERE __time >= TIMESTAMP '2000-01-01 00:00:00'
                    AND __time <  TIMESTAMP '2000-02-01 00:00:00'),
  SUM(cnt) FILTER(WHERE __time >= TIMESTAMP '2000-01-01 00:00:01'
                    AND __time <  TIMESTAMP '2000-02-01 00:00:00'),
  SUM(cnt) FILTER(WHERE __time >= TIMESTAMP '2001-01-01 00:00:00'
                    AND __time <  TIMESTAMP '2001-02-01 00:00:00')
FROM foo
WHERE __time >= TIMESTAMP '2000-01-01 00:00:00'
  AND __time < TIMESTAMP '2001-02-01 00:00:00'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
EXPR$2 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[SUM($0) FILTER $1], EXPR$1=[SUM($0) FILTER $2], EXPR$2=[SUM($0) FILTER $3])
  LogicalProject(cnt=[$1], $f1=[AND(>=($0, 2000-01-01 00:00:00), <($0, 2000-02-01 00:00:00))], $f2=[AND(>=($0, 2000-01-01 00:00:01), <($0, 2000-02-01 00:00:00))], $f3=[AND(>=($0, 2001-01-01 00:00:00), <($0, 2001-02-01 00:00:00))])
    LogicalFilter(condition=[AND(>=($0, 2000-01-01 00:00:00), <($0, 2001-02-01 00:00:00))])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "2000-01-01T00:00:00.000Z/2001-02-01T00:00:00.000Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "filtered",
    "aggregator" : {
      "type" : "longSum",
      "name" : "a0",
      "fieldName" : "cnt"
    },
    "filter" : {
      "type" : "bound",
      "dimension" : "__time",
      "upper" : "949363200000",
      "upperStrict" : true,
      "ordering" : {
        "type" : "numeric"
      }
    },
    "name" : "a0"
  }, {
    "type" : "filtered",
    "aggregator" : {
      "type" : "longSum",
      "name" : "a1",
      "fieldName" : "cnt"
    },
    "filter" : {
      "type" : "bound",
      "dimension" : "__time",
      "lower" : "946684801000",
      "upper" : "949363200000",
      "upperStrict" : true,
      "ordering" : {
        "type" : "numeric"
      }
    },
    "name" : "a1"
  }, {
    "type" : "filtered",
    "aggregator" : {
      "type" : "longSum",
      "name" : "a2",
      "fieldName" : "cnt"
    },
    "filter" : {
      "type" : "bound",
      "dimension" : "__time",
      "lower" : "978307200000",
      "upper" : "980985600000",
      "upperStrict" : true,
      "ordering" : {
        "type" : "numeric"
      }
    },
    "name" : "a2"
  } ]
}
=== results
[3,2,3]
==============================================================
Converted from testTimeseriesLosAngelesViaQueryContext()
=== case
Timeseries los angeles via query context
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT FLOOR(__time TO MONTH) AS gran,
  cnt FROM druid.foo
  ) AS x
GROUP BY gran
ORDER BY gran
=== context
sqlTimeZone=America/Los_Angeles
=== options
sqlCompatibleNulls=both
vectorize=true
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[FLOOR($0, FLAG(MONTH))], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "period",
    "period" : "P1M",
    "timeZone" : "America/Los_Angeles",
    "origin" : null
  },
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "sqlTimeZone" : "America/Los_Angeles",
    "timestampResultField" : "d0"
  }
}
=== results
[1,944006400000]
[2,946684800000]
[1,975628800000]
[2,978307200000]
==============================================================
Converted from testTimeseriesLosAngelesViaPlannerConfig()
=== case
Timeseries los angeles via planner config
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT
    FLOOR(__time TO MONTH) AS gran,
    cnt
  FROM druid.foo
  WHERE __time >= TIME_PARSE('1999-12-01 00:00:00')
    AND __time < TIME_PARSE('2002-01-01 00:00:00')
  ) AS x
GROUP BY gran
ORDER BY gran
=== options
planner.sqlTimeZone=America/Los_Angeles
sqlCompatibleNulls=both
vectorize=true
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[FLOOR($0, FLAG(MONTH))], cnt=[$1])
        LogicalFilter(condition=[AND(>=($0, TIME_PARSE('1999-12-01 00:00:00')), <($0, TIME_PARSE('2002-01-01 00:00:00')))])
          LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "1999-12-01T08:00:00.000Z/2002-01-01T08:00:00.000Z" ]
  },
  "granularity" : {
    "type" : "period",
    "period" : "P1M",
    "timeZone" : "America/Los_Angeles",
    "origin" : null
  },
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "timestampResultField" : "d0"
  }
}
=== results
[1,944006400000]
[2,946684800000]
[1,975628800000]
[2,978307200000]
==============================================================
Converted from testTimeseriesUsingTimeFloor()
=== case
Timeseries using time floor
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT TIME_FLOOR(__time, 'P1M') AS gran,
  cnt FROM druid.foo
  ) AS x
GROUP BY gran
ORDER BY gran
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
gran TIMESTAMP(3)
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[TIME_FLOOR($0, 'P1M')], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : "MONTH",
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "timestampResultField" : "d0"
  }
}
=== results
[3,946684800000]
[3,978307200000]
==============================================================
Converted from testTimeseriesUsingTimeFloorWithTimeShift()
Cannot vectorize due to virtual columns.
=== case
Timeseries using time floor with time shift
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT TIME_FLOOR(TIME_SHIFT(__time, 'P1D', -1), 'P1M') AS gran,
  cnt FROM druid.foo
  ) AS x
GROUP BY gran
ORDER BY gran
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 BIGINT
gran TIMESTAMP(3)
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[TIME_FLOOR(TIME_SHIFT($0, 'P1D', -1), 'P1M')], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "timestamp_floor(timestamp_shift(\"__time\",'P1D',-1,'UTC'),'P1M',null,'UTC')",
    "outputType" : "LONG"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "v0",
    "outputName" : "d0",
    "outputType" : "LONG"
  } ],
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "limitSpec" : {
    "type" : "default",
    "columns" : [ {
      "dimension" : "d0",
      "direction" : "ascending",
      "dimensionOrder" : {
        "type" : "numeric"
      }
    } ]
  }
}
=== results
[1,944006400000]
[2,946684800000]
[1,975628800000]
[2,978307200000]
==============================================================
Converted from testTimeseriesUsingTimeFloorWithTimestampAdd()
=== case
Timeseries using time floor with timestamp add
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT TIME_FLOOR(TIMESTAMPADD(DAY, -1, __time), 'P1M') AS gran,
  cnt FROM druid.foo
  ) AS x
GROUP BY gran
ORDER BY gran
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
gran TIMESTAMP(3)
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[TIME_FLOOR(+($0, *(86400000:INTERVAL DAY, -1)), 'P1M')], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "timestamp_floor((\"__time\" + -86400000),'P1M',null,'UTC')",
    "outputType" : "LONG"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "v0",
    "outputName" : "d0",
    "outputType" : "LONG"
  } ],
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "limitSpec" : {
    "type" : "default",
    "columns" : [ {
      "dimension" : "d0",
      "direction" : "ascending",
      "dimensionOrder" : {
        "type" : "numeric"
      }
    } ]
  }
}
=== results
[1,944006400000]
[2,946684800000]
[1,975628800000]
[2,978307200000]
==============================================================
Converted from testTimeseriesUsingTimeFloorWithOrigin()
=== case
Timeseries using time floor with origin
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT TIME_FLOOR(__time, 'P1M', TIMESTAMP '1970-01-01 01:02:03') AS gran,
  cnt FROM druid.foo
  ) AS x
GROUP BY gran
ORDER BY gran
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
gran TIMESTAMP(3)
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[TIME_FLOOR($0, 'P1M', 1970-01-01 01:02:03)], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "period",
    "period" : "P1M",
    "timeZone" : "UTC",
    "origin" : "1970-01-01T01:02:03.000Z"
  },
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "timestampResultField" : "d0"
  }
}
=== results
[1,944010123000]
[2,946688523000]
[1,975632523000]
[2,978310923000]
==============================================================
Converted from testTimeseriesLosAngelesUsingTimeFloorConnectionUtc()
=== case
Timeseries los angeles using time floor connection utc
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT TIME_FLOOR(__time, 'P1M', CAST(NULL AS TIMESTAMP), 'America/Los_Angeles') AS gran,
  cnt FROM druid.foo
  ) AS x
GROUP BY gran
ORDER BY gran
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
gran TIMESTAMP(3)
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[TIME_FLOOR($0, 'P1M', null:TIMESTAMP(3), 'America/Los_Angeles')], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "period",
    "period" : "P1M",
    "timeZone" : "America/Los_Angeles",
    "origin" : null
  },
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "timestampResultField" : "d0"
  }
}
=== results
[1,944035200000]
[2,946713600000]
[1,975657600000]
[2,978336000000]
==============================================================
Converted from testTimeseriesLosAngelesUsingTimeFloorConnectionLosAngeles()
=== case
Timeseries los angeles using time floor connection los angeles
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT TIME_FLOOR(__time, 'P1M') AS gran,
  cnt FROM druid.foo
  ) AS x
GROUP BY gran
ORDER BY gran
=== context
sqlTimeZone=America/Los_Angeles
=== options
sqlCompatibleNulls=both
vectorize=true
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[TIME_FLOOR($0, 'P1M')], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "period",
    "period" : "P1M",
    "timeZone" : "America/Los_Angeles",
    "origin" : null
  },
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "sqlTimeZone" : "America/Los_Angeles",
    "timestampResultField" : "d0"
  }
}
=== results
[1,944006400000]
[2,946684800000]
[1,975628800000]
[2,978307200000]
==============================================================
Converted from testTimeseriesDontSkipEmptyBuckets()

Tests that query context parameters are passed through to the
underlying query engine.
=== case
Timeseries dont skip empty buckets
=== SQL
SELECT SUM(cnt), gran
FROM (
  SELECT floor(__time TO HOUR) AS gran, cnt
  FROM druid.foo
  WHERE __time >= TIMESTAMP '2000-01-01 00:00:00'
    AND __time < TIMESTAMP '2000-01-02 00:00:00'
  ) AS x
GROUP BY gran
ORDER BY gran
=== context
skipEmptyBuckets=false
=== options
vectorize=true
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], gran=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(gran=[FLOOR($0, FLAG(HOUR))], cnt=[$1])
        LogicalFilter(condition=[AND(>=($0, 2000-01-01 00:00:00), <($0, 2000-01-02 00:00:00))])
          LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "2000-01-01T00:00:00.000Z/2000-01-02T00:00:00.000Z" ]
  },
  "granularity" : "HOUR",
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : false,
    "timestampResultField" : "d0"
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[1,946684800000]
[0,946688400000]
[0,946692000000]
[0,946695600000]
[0,946699200000]
[0,946702800000]
[0,946706400000]
[0,946710000000]
[0,946713600000]
[0,946717200000]
[0,946720800000]
[0,946724400000]
[0,946728000000]
[0,946731600000]
[0,946735200000]
[0,946738800000]
[0,946742400000]
[0,946746000000]
[0,946749600000]
[0,946753200000]
[0,946756800000]
[0,946760400000]
[0,946764000000]
[0,946767600000]
=== run
=== options
sqlCompatibleNulls=true
=== results
[1,946684800000]
[null,946688400000]
[null,946692000000]
[null,946695600000]
[null,946699200000]
[null,946702800000]
[null,946706400000]
[null,946710000000]
[null,946713600000]
[null,946717200000]
[null,946720800000]
[null,946724400000]
[null,946728000000]
[null,946731600000]
[null,946735200000]
[null,946738800000]
[null,946742400000]
[null,946746000000]
[null,946749600000]
[null,946753200000]
[null,946756800000]
[null,946760400000]
[null,946764000000]
[null,946767600000]
==============================================================
Converted from testTimeseriesUsingCastAsDate()
=== case
Timeseries using cast as date
=== SQL
SELECT SUM(cnt), dt
FROM (
  SELECT CAST(__time AS DATE) AS dt,
  cnt FROM druid.foo
  ) AS x
GROUP BY dt
ORDER BY dt
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
dt DATE
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], dt=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(dt=[CAST($0):DATE NOT NULL], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : "DAY",
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "timestampResultField" : "d0"
  }
}
=== results
[1,10957]
[1,10958]
[1,10959]
[1,11323]
[1,11324]
[1,11325]
==============================================================
Converted from testTimeseriesUsingFloorPlusCastAsDate()
=== case
Timeseries using floor plus cast as date
=== SQL
SELECT SUM(cnt), dt
FROM (
  SELECT CAST(FLOOR(__time TO QUARTER) AS DATE) AS dt,
  cnt FROM druid.foo
  ) AS x
GROUP BY dt
ORDER BY dt
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
dt DATE
=== plan
LogicalSort(sort0=[$1], dir0=[ASC])
  LogicalProject(EXPR$0=[$1], dt=[$0])
    LogicalAggregate(group=[{0}], EXPR$0=[SUM($1)])
      LogicalProject(dt=[CAST(FLOOR($0, FLAG(QUARTER))):DATE NOT NULL], cnt=[$1])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : "QUARTER",
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "a0",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true,
    "timestampResultField" : "d0"
  }
}
=== results
[3,10957]
[3,11323]
