Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
==============================================================
Converted from testGroupByWithPostAggregatorReferencingTimeFloorColumnOnTimeseries()
=== case
Group by with post aggregator referencing time floor column on timeseries
=== SQL
SELECT TIME_FORMAT("date", 'yyyy-MM'), SUM(x)
FROM (
    SELECT
        FLOOR(__time to hour) as "date",
        COUNT(*) as x
    FROM foo
    GROUP BY 1
)
GROUP BY 1
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 VARCHAR
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{0}], EXPR$1=[SUM($1)])
  LogicalProject(EXPR$0=[TIME_FORMAT($0, 'yyyy-MM')], x=[$1])
    LogicalAggregate(group=[{0}], x=[COUNT()])
      LogicalProject(date=[FLOOR($0, FLAG(HOUR))])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "query",
    "query" : {
      "queryType" : "timeseries",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "granularity" : "HOUR",
      "aggregations" : [ {
        "type" : "count",
        "name" : "a0"
      } ],
      "context" : {
        "skipEmptyBuckets" : true,
        "timestampResultField" : "d0"
      }
    }
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "timestamp_format(\"d0\",'yyyy-MM','UTC')",
    "outputType" : "STRING"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "v0",
    "outputName" : "_d0",
    "outputType" : "STRING"
  } ],
  "aggregations" : [ {
    "type" : "longSum",
    "name" : "_a0",
    "fieldName" : "a0"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== results
["2000-01",3]
["2001-01",3]
==============================================================
Converted from testInformationSchemaSchemata()
=== case
Information schema schemata
=== SQL
SELECT DISTINCT SCHEMA_NAME
FROM INFORMATION_SCHEMA.SCHEMATA
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
SCHEMA_NAME VARCHAR
=== plan
BindableAggregate(group=[{1}])
  BindableTableScan(table=[[INFORMATION_SCHEMA, SCHEMATA]])
=== results
["lookup"]
["view"]
["druid"]
["sys"]
["INFORMATION_SCHEMA"]
==============================================================
Converted from testInformationSchemaTables()
=== case
Information schema tables
=== SQL
SELECT
  TABLE_SCHEMA,
  TABLE_NAME,
  TABLE_TYPE,
  IS_JOINABLE,
  IS_BROADCAST
FROM INFORMATION_SCHEMA.TABLES
WHERE TABLE_TYPE IN ('SYSTEM_TABLE', 'TABLE', 'VIEW')
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
TABLE_SCHEMA VARCHAR
TABLE_NAME VARCHAR
TABLE_TYPE VARCHAR
IS_JOINABLE VARCHAR
IS_BROADCAST VARCHAR
=== plan
BindableProject(TABLE_SCHEMA=[$1], TABLE_NAME=[$2], TABLE_TYPE=[$3], IS_JOINABLE=[$4], IS_BROADCAST=[$5])
  BindableFilter(condition=[OR(=($3, 'SYSTEM_TABLE'), =($3, 'TABLE'), =($3, 'VIEW'))])
    BindableTableScan(table=[[INFORMATION_SCHEMA, TABLES]])
=== results
["druid","broadcast","TABLE","YES","YES"]
["druid","foo","TABLE","NO","NO"]
["druid","foo2","TABLE","NO","NO"]
["druid","foo4","TABLE","NO","NO"]
["druid","lotsocolumns","TABLE","NO","NO"]
["druid","numfoo","TABLE","NO","NO"]
["druid","some_datasource","TABLE","NO","NO"]
["druid","somexdatasource","TABLE","NO","NO"]
["druid","visits","TABLE","NO","NO"]
["INFORMATION_SCHEMA","COLUMNS","SYSTEM_TABLE","NO","NO"]
["INFORMATION_SCHEMA","SCHEMATA","SYSTEM_TABLE","NO","NO"]
["INFORMATION_SCHEMA","TABLES","SYSTEM_TABLE","NO","NO"]
["lookup","lookyloo","TABLE","YES","YES"]
["sys","segments","SYSTEM_TABLE","NO","NO"]
["sys","server_segments","SYSTEM_TABLE","NO","NO"]
["sys","servers","SYSTEM_TABLE","NO","NO"]
["sys","supervisors","SYSTEM_TABLE","NO","NO"]
["sys","tasks","SYSTEM_TABLE","NO","NO"]
["view","aview","VIEW","NO","NO"]
["view","bview","VIEW","NO","NO"]
["view","cview","VIEW","NO","NO"]
["view","dview","VIEW","NO","NO"]
["view","invalidView","VIEW","NO","NO"]
["view","restrictedView","VIEW","NO","NO"]
==============================================================
Converted from testInformationSchemaColumnsOnTable()
=== case
Information schema columns on table
=== SQL
SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_SCHEMA = 'druid'
  AND TABLE_NAME = 'foo'
=== options
vectorize=true
=== schema
COLUMN_NAME VARCHAR
DATA_TYPE VARCHAR
IS_NULLABLE VARCHAR
=== plan
BindableProject(COLUMN_NAME=[$3], DATA_TYPE=[$7], IS_NULLABLE=[$6])
  BindableFilter(condition=[AND(=($1, 'druid'), =($2, 'foo'))])
    BindableTableScan(table=[[INFORMATION_SCHEMA, COLUMNS]])
=== run
=== options
sqlCompatibleNulls=false
=== results
["__time","TIMESTAMP","NO"]
["cnt","BIGINT","NO"]
["dim1","VARCHAR","YES"]
["dim2","VARCHAR","YES"]
["dim3","VARCHAR","YES"]
["m1","FLOAT","NO"]
["m2","DOUBLE","NO"]
["unique_dim1","COMPLEX<hyperUnique>","YES"]
=== run
=== options
sqlCompatibleNulls=true
=== results
["__time","TIMESTAMP","NO"]
["cnt","BIGINT","YES"]
["dim1","VARCHAR","YES"]
["dim2","VARCHAR","YES"]
["dim3","VARCHAR","YES"]
["m1","FLOAT","YES"]
["m2","DOUBLE","YES"]
["unique_dim1","COMPLEX<hyperUnique>","YES"]
==============================================================
Converted from testInformationSchemaColumnsOnForbiddenTable()
=== case
Information schema columns on forbidden table
=== SQL
SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_SCHEMA = 'druid'
  AND TABLE_NAME = 'forbiddenDatasource'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
COLUMN_NAME VARCHAR
DATA_TYPE VARCHAR
IS_NULLABLE VARCHAR
=== plan
BindableProject(COLUMN_NAME=[$3], DATA_TYPE=[$7], IS_NULLABLE=[$6])
  BindableFilter(condition=[AND(=($1, 'druid'), =($2, 'forbiddenDatasource'))])
    BindableTableScan(table=[[INFORMATION_SCHEMA, COLUMNS]])
=== results
==============================================================
Converted from testInformationSchemaColumnsOnView()
=== case
Information schema columns on view
=== SQL
SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_SCHEMA = 'view'
  AND TABLE_NAME = 'aview'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
COLUMN_NAME VARCHAR
DATA_TYPE VARCHAR
IS_NULLABLE VARCHAR
=== plan
BindableProject(COLUMN_NAME=[$3], DATA_TYPE=[$7], IS_NULLABLE=[$6])
  BindableFilter(condition=[AND(=($1, 'view'), =($2, 'aview'))])
    BindableTableScan(table=[[INFORMATION_SCHEMA, COLUMNS]])
=== results
["dim1_firstchar","VARCHAR","YES"]
==============================================================
Converted from testInformationSchemaColumnsOnAnotherView()
=== case
Information schema columns on another view
=== SQL
SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_SCHEMA = 'view'
  AND TABLE_NAME = 'cview'
=== options
qlCompatibleNulls=both
vectorize=true
=== schema
COLUMN_NAME VARCHAR
DATA_TYPE VARCHAR
IS_NULLABLE VARCHAR
=== plan
BindableProject(COLUMN_NAME=[$3], DATA_TYPE=[$7], IS_NULLABLE=[$6])
  BindableFilter(condition=[AND(=($1, 'view'), =($2, 'cview'))])
    BindableTableScan(table=[[INFORMATION_SCHEMA, COLUMNS]])
=== results
["dim1_firstchar","VARCHAR","YES"]
["dim2","VARCHAR","YES"]
["l2","BIGINT","NO"]
==============================================================
Converted from testExplainInformationSchemaColumns()
=== case
Explain information schema columns
=== SQL
EXPLAIN PLAN FOR
SELECT COLUMN_NAME, DATA_TYPE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_SCHEMA = 'druid'
  AND TABLE_NAME = 'foo'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
PLAN VARCHAR
RESOURCES VARCHAR
=== plan
BindableProject(COLUMN_NAME=[$3], DATA_TYPE=[$7])
  BindableFilter(condition=[AND(=($1, 'druid'), =($2, 'foo'))])
    BindableTableScan(table=[[INFORMATION_SCHEMA, COLUMNS]])
=== results
["BindableProject(COLUMN_NAME=[$3], DATA_TYPE=[$7])\n  BindableFilter(condition=[AND(=($1, 'druid'), =($2, 'foo'))])\n    BindableTableScan(table=[[INFORMATION_SCHEMA, COLUMNS]])\n","[]"]
==============================================================
Converted from testAggregatorsOnInformationSchemaColumns()
Not including COUNT DISTINCT, since it isn't supported by
BindableAggregate, and so it can't work.
=== case
Aggregators on information schema columns
=== SQL
SELECT
  COUNT(JDBC_TYPE),
  SUM(JDBC_TYPE),
  AVG(JDBC_TYPE),
  MIN(JDBC_TYPE),
  MAX(JDBC_TYPE)
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_SCHEMA = 'druid'
  AND TABLE_NAME = 'foo'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
EXPR$2 BIGINT
EXPR$3 BIGINT
EXPR$4 BIGINT
=== plan
BindableProject(EXPR$0=[$0], EXPR$1=[CASE(=($0, 0), null:BIGINT, $1)], EXPR$2=[/(CASE(=($0, 0), null:BIGINT, $1), $0)], EXPR$3=[$2], EXPR$4=[$3])
  BindableAggregate(group=[{}], EXPR$0=[COUNT()], EXPR$1=[$SUM0($16)], EXPR$3=[MIN($16)], EXPR$4=[MAX($16)])
    BindableFilter(condition=[AND(=($1, 'druid'), =($2, 'foo'))])
      BindableTableScan(table=[[INFORMATION_SCHEMA, COLUMNS]])
=== results
[8,1249,156,-5,1111]
==============================================================
Converted from testTopNLimitWrapping()
=== case
Top n limit wrapping
=== SQL
SELECT dim1, COUNT(*)
FROM druid.foo
GROUP BY dim1
ORDER BY dim1 DESC
=== context
sqlOuterLimit=2
=== options
vectorize=true
=== schema
dim1 VARCHAR
EXPR$1 BIGINT
=== plan
LogicalSort(sort0=[$0], dir0=[DESC], fetch=[2:BIGINT])
  LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
    LogicalProject(dim1=[$2])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "topN",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "dimension" : {
    "type" : "default",
    "dimension" : "dim1",
    "outputName" : "d0",
    "outputType" : "STRING"
  },
  "metric" : {
    "type" : "inverted",
    "metric" : {
      "type" : "dimension",
      "ordering" : {
        "type" : "lexicographic"
      }
    }
  },
  "threshold" : 2,
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "context" : {
    "sqlOuterLimit" : 2
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
["",1]
["def",1]
=== run
=== options
sqlCompatibleNulls=true
=== results
["def",1]
["abc",1]
==============================================================
Converted from testTopNLimitWrappingOrderByAgg()
=== case
Top n limit wrapping order by agg
=== SQL
SELECT dim1, COUNT(*)
FROM druid.foo
GROUP BY 1
ORDER BY 2 DESC
=== context
sqlOuterLimit=2
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
dim1 VARCHAR
EXPR$1 BIGINT
=== plan
LogicalSort(sort0=[$1], dir0=[DESC], fetch=[2:BIGINT])
  LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
    LogicalProject(dim1=[$2])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "topN",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "dimension" : {
    "type" : "default",
    "dimension" : "dim1",
    "outputName" : "d0",
    "outputType" : "STRING"
  },
  "metric" : {
    "type" : "numeric",
    "metric" : "a0"
  },
  "threshold" : 2,
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "context" : {
    "sqlOuterLimit" : 2
  }
}
=== results
["",1]
["1",1]
==============================================================
Converted from testGroupByLimitWrapping()
=== case
Group by limit wrapping
=== SQL
SELECT dim1, dim2, COUNT(*)
FROM druid.foo
GROUP BY dim1, dim2
ORDER BY dim1 DESC
=== context
sqlOuterLimit=2
=== options
vectorize=true
=== schema
dim1 VARCHAR
dim2 VARCHAR
EXPR$2 BIGINT
=== plan
LogicalSort(sort0=[$0], dir0=[DESC], fetch=[2:BIGINT])
  LogicalAggregate(group=[{0, 1}], EXPR$2=[COUNT()])
    LogicalProject(dim1=[$2], dim2=[$3])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "dim1",
    "outputName" : "d0",
    "outputType" : "STRING"
  }, {
    "type" : "default",
    "dimension" : "dim2",
    "outputName" : "d1",
    "outputType" : "STRING"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "default",
    "columns" : [ {
      "dimension" : "d0",
      "direction" : "descending",
      "dimensionOrder" : {
        "type" : "lexicographic"
      }
    } ],
    "limit" : 2
  },
  "context" : {
    "sqlOuterLimit" : 2
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
["def","abc",1]
["abc","",1]
=== run
=== options
sqlCompatibleNulls=true
=== results
["def","abc",1]
["abc",null,1]
==============================================================
Converted from testGroupByWithForceLimitPushDown()
=== case
Group by with force limit push down
=== SQL
SELECT dim1, dim2, COUNT(*)
FROM druid.foo
GROUP BY dim1, dim2 limit 1
=== context
forceLimitPushDown=true
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
dim1 VARCHAR
dim2 VARCHAR
EXPR$2 BIGINT
=== plan
LogicalSort(fetch=[1])
  LogicalAggregate(group=[{0, 1}], EXPR$2=[COUNT()])
    LogicalProject(dim1=[$2], dim2=[$3])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "dim1",
    "outputName" : "d0",
    "outputType" : "STRING"
  }, {
    "type" : "default",
    "dimension" : "dim2",
    "outputName" : "d1",
    "outputType" : "STRING"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "default",
    "columns" : [ ],
    "limit" : 1
  },
  "context" : {
    "forceLimitPushDown" : true
  }
}
=== results
["","a",1]
==============================================================
Converted from testGroupByLimitWrappingOrderByAgg()
=== case
Group by limit wrapping order by agg
=== SQL
SELECT dim1, dim2, COUNT(*)
FROM druid.foo
GROUP BY 1, 2
ORDER BY 3 DESC
=== context
sqlOuterLimit=2
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
dim1 VARCHAR
dim2 VARCHAR
EXPR$2 BIGINT
=== plan
LogicalSort(sort0=[$2], dir0=[DESC], fetch=[2:BIGINT])
  LogicalAggregate(group=[{0, 1}], EXPR$2=[COUNT()])
    LogicalProject(dim1=[$2], dim2=[$3])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "dim1",
    "outputName" : "d0",
    "outputType" : "STRING"
  }, {
    "type" : "default",
    "dimension" : "dim2",
    "outputName" : "d1",
    "outputType" : "STRING"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "default",
    "columns" : [ {
      "dimension" : "a0",
      "direction" : "descending",
      "dimensionOrder" : {
        "type" : "numeric"
      }
    } ],
    "limit" : 2
  },
  "context" : {
    "sqlOuterLimit" : 2
  }
}
=== results
["","a",1]
["1","a",1]
==============================================================
Converted from testGroupBySingleColumnDescendingNoTopN()
=== case
Group by single column descending no top n
=== SQL
SELECT dim1
FROM druid.foo
GROUP BY dim1
ORDER BY dim1 DESC
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
dim1 VARCHAR
=== plan
LogicalSort(sort0=[$0], dir0=[DESC])
  LogicalAggregate(group=[{0}])
    LogicalProject(dim1=[$2])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "dim1",
    "outputName" : "d0",
    "outputType" : "STRING"
  } ],
  "limitSpec" : {
    "type" : "default",
    "columns" : [ {
      "dimension" : "d0",
      "direction" : "descending",
      "dimensionOrder" : {
        "type" : "lexicographic"
      }
    } ]
  }
}
=== results
["def"]
["abc"]
["2"]
["10.1"]
["1"]
[""]
==============================================================
Converted from testEarliestAggregators()
Cannot vectorize EARLIEST aggregator.
=== case
Earliest aggregators
=== SQL
SELECT
  EARLIEST(cnt),
  EARLIEST(m1),
  EARLIEST(dim1, 10),
  EARLIEST(cnt + 1),
  EARLIEST(m1 + 1),
  EARLIEST(dim1 || CAST(cnt AS VARCHAR), 10),
  EARLIEST_BY(cnt, MILLIS_TO_TIMESTAMP(l1)),
  EARLIEST_BY(m1, MILLIS_TO_TIMESTAMP(l1)),
  EARLIEST_BY(dim1, MILLIS_TO_TIMESTAMP(l1), 10),
  EARLIEST_BY(cnt + 1, MILLIS_TO_TIMESTAMP(l1)),
  EARLIEST_BY(m1 + 1, MILLIS_TO_TIMESTAMP(l1)),
  EARLIEST_BY(dim1 || CAST(cnt AS VARCHAR),
  MILLIS_TO_TIMESTAMP(l1), 10)
FROM druid.numfoo
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 FLOAT
EXPR$2 VARCHAR
EXPR$3 BIGINT
EXPR$4 FLOAT
EXPR$5 VARCHAR
EXPR$6 BIGINT
EXPR$7 FLOAT
EXPR$8 VARCHAR
EXPR$9 BIGINT
EXPR$10 FLOAT
EXPR$11 VARCHAR
=== plan
LogicalAggregate(group=[{}], EXPR$0=[EARLIEST($0)], EXPR$1=[EARLIEST($1)], EXPR$2=[EARLIEST($2, $3)], EXPR$3=[EARLIEST($4)], EXPR$4=[EARLIEST($5)], EXPR$5=[EARLIEST($6, $3)], EXPR$6=[EARLIEST_BY($0, $7)], EXPR$7=[EARLIEST_BY($1, $7)], EXPR$8=[EARLIEST_BY($2, $7, $3)], EXPR$9=[EARLIEST_BY($4, $7)], EXPR$10=[EARLIEST_BY($5, $7)], EXPR$11=[EARLIEST_BY($6, $7, $3)])
  LogicalProject(cnt=[$1], m1=[$14], dim1=[$4], $f3=[10], $f4=[+($1, 1)], $f5=[+($14, 1)], $f6=[||($4, CAST($1):VARCHAR NOT NULL)], $f7=[MILLIS_TO_TIMESTAMP($12)])
    LogicalTableScan(table=[[druid, numfoo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "numfoo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "(\"cnt\" + 1)",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v1",
    "expression" : "(\"m1\" + 1)",
    "outputType" : "FLOAT"
  }, {
    "type" : "expression",
    "name" : "v2",
    "expression" : "concat(\"dim1\",CAST(\"cnt\", 'STRING'))",
    "outputType" : "STRING"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "longFirst",
    "name" : "a0",
    "fieldName" : "cnt",
    "timeColumn" : "__time"
  }, {
    "type" : "floatFirst",
    "name" : "a1",
    "fieldName" : "m1",
    "timeColumn" : "__time"
  }, {
    "type" : "stringFirst",
    "name" : "a2",
    "fieldName" : "dim1",
    "timeColumn" : "__time",
    "maxStringBytes" : 10
  }, {
    "type" : "longFirst",
    "name" : "a3",
    "fieldName" : "v0",
    "timeColumn" : "__time"
  }, {
    "type" : "floatFirst",
    "name" : "a4",
    "fieldName" : "v1",
    "timeColumn" : "__time"
  }, {
    "type" : "stringFirst",
    "name" : "a5",
    "fieldName" : "v2",
    "timeColumn" : "__time",
    "maxStringBytes" : 10
  }, {
    "type" : "longFirst",
    "name" : "a6",
    "fieldName" : "cnt",
    "timeColumn" : "l1"
  }, {
    "type" : "floatFirst",
    "name" : "a7",
    "fieldName" : "m1",
    "timeColumn" : "l1"
  }, {
    "type" : "stringFirst",
    "name" : "a8",
    "fieldName" : "dim1",
    "timeColumn" : "l1",
    "maxStringBytes" : 10
  }, {
    "type" : "longFirst",
    "name" : "a9",
    "fieldName" : "v0",
    "timeColumn" : "l1"
  }, {
    "type" : "floatFirst",
    "name" : "a10",
    "fieldName" : "v1",
    "timeColumn" : "l1"
  }, {
    "type" : "stringFirst",
    "name" : "a11",
    "fieldName" : "v2",
    "timeColumn" : "l1",
    "maxStringBytes" : 10
  } ]
}
=== results
[1,1.0,"",2,2.0,"1",1,3.0,"2",2,4.0,"21"]
==============================================================
Converted from testLatestVectorAggregators()
=== case
Latest vector aggregators
=== SQL
SELECT LATEST(cnt), LATEST(cnt + 1), LATEST(m1), LATEST(m1+1)
FROM druid.numfoo
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
EXPR$2 FLOAT
EXPR$3 FLOAT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[LATEST($0)], EXPR$1=[LATEST($1)], EXPR$2=[LATEST($2)], EXPR$3=[LATEST($3)])
  LogicalProject(cnt=[$1], $f1=[+($1, 1)], m1=[$14], $f3=[+($14, 1)])
    LogicalTableScan(table=[[druid, numfoo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "numfoo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "(\"cnt\" + 1)",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v1",
    "expression" : "(\"m1\" + 1)",
    "outputType" : "FLOAT"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "longLast",
    "name" : "a0",
    "fieldName" : "cnt",
    "timeColumn" : "__time"
  }, {
    "type" : "longLast",
    "name" : "a1",
    "fieldName" : "v0",
    "timeColumn" : "__time"
  }, {
    "type" : "floatLast",
    "name" : "a2",
    "fieldName" : "m1",
    "timeColumn" : "__time"
  }, {
    "type" : "floatLast",
    "name" : "a3",
    "fieldName" : "v1",
    "timeColumn" : "__time"
  } ]
}
=== results
[1,2,6.0,7.0]
==============================================================
Converted from testLatestAggregators()
Cannot vectorize until StringLast is vectorized
=== case
Latest aggregators
=== SQL
SELECT
  LATEST(cnt),
  LATEST(m1),
  LATEST(dim1, 10),
  LATEST(cnt + 1),
  LATEST(m1 + 1),
  LATEST(dim1 || CAST(cnt AS VARCHAR), 10),
  LATEST_BY(cnt, MILLIS_TO_TIMESTAMP(l1)),
  LATEST_BY(m1, MILLIS_TO_TIMESTAMP(l1)),
  LATEST_BY(dim1, MILLIS_TO_TIMESTAMP(l1), 10),
  LATEST_BY(cnt + 1, MILLIS_TO_TIMESTAMP(l1)),
  LATEST_BY(m1 + 1, MILLIS_TO_TIMESTAMP(l1)),
  LATEST_BY(dim1 || CAST(cnt AS VARCHAR),
  MILLIS_TO_TIMESTAMP(l1), 10)
FROM druid.numfoo
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 FLOAT
EXPR$2 VARCHAR
EXPR$3 BIGINT
EXPR$4 FLOAT
EXPR$5 VARCHAR
EXPR$6 BIGINT
EXPR$7 FLOAT
EXPR$8 VARCHAR
EXPR$9 BIGINT
EXPR$10 FLOAT
EXPR$11 VARCHAR
=== plan
LogicalAggregate(group=[{}], EXPR$0=[LATEST($0)], EXPR$1=[LATEST($1)], EXPR$2=[LATEST($2, $3)], EXPR$3=[LATEST($4)], EXPR$4=[LATEST($5)], EXPR$5=[LATEST($6, $3)], EXPR$6=[LATEST_BY($0, $7)], EXPR$7=[LATEST_BY($1, $7)], EXPR$8=[LATEST_BY($2, $7, $3)], EXPR$9=[LATEST_BY($4, $7)], EXPR$10=[LATEST_BY($5, $7)], EXPR$11=[LATEST_BY($6, $7, $3)])
  LogicalProject(cnt=[$1], m1=[$14], dim1=[$4], $f3=[10], $f4=[+($1, 1)], $f5=[+($14, 1)], $f6=[||($4, CAST($1):VARCHAR NOT NULL)], $f7=[MILLIS_TO_TIMESTAMP($12)])
    LogicalTableScan(table=[[druid, numfoo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "numfoo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "(\"cnt\" + 1)",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v1",
    "expression" : "(\"m1\" + 1)",
    "outputType" : "FLOAT"
  }, {
    "type" : "expression",
    "name" : "v2",
    "expression" : "concat(\"dim1\",CAST(\"cnt\", 'STRING'))",
    "outputType" : "STRING"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "longLast",
    "name" : "a0",
    "fieldName" : "cnt",
    "timeColumn" : "__time"
  }, {
    "type" : "floatLast",
    "name" : "a1",
    "fieldName" : "m1",
    "timeColumn" : "__time"
  }, {
    "type" : "stringLast",
    "name" : "a2",
    "fieldName" : "dim1",
    "timeColumn" : "__time",
    "maxStringBytes" : 10
  }, {
    "type" : "longLast",
    "name" : "a3",
    "fieldName" : "v0",
    "timeColumn" : "__time"
  }, {
    "type" : "floatLast",
    "name" : "a4",
    "fieldName" : "v1",
    "timeColumn" : "__time"
  }, {
    "type" : "stringLast",
    "name" : "a5",
    "fieldName" : "v2",
    "timeColumn" : "__time",
    "maxStringBytes" : 10
  }, {
    "type" : "longLast",
    "name" : "a6",
    "fieldName" : "cnt",
    "timeColumn" : "l1"
  }, {
    "type" : "floatLast",
    "name" : "a7",
    "fieldName" : "m1",
    "timeColumn" : "l1"
  }, {
    "type" : "stringLast",
    "name" : "a8",
    "fieldName" : "dim1",
    "timeColumn" : "l1",
    "maxStringBytes" : 10
  }, {
    "type" : "longLast",
    "name" : "a9",
    "fieldName" : "v0",
    "timeColumn" : "l1"
  }, {
    "type" : "floatLast",
    "name" : "a10",
    "fieldName" : "v1",
    "timeColumn" : "l1"
  }, {
    "type" : "stringLast",
    "name" : "a11",
    "fieldName" : "v2",
    "timeColumn" : "l1",
    "maxStringBytes" : 10
  } ]
}
=== results
[1,6.0,"abc",2,7.0,"abc1",1,2.0,"10.1",2,3.0,"10.11"]
==============================================================
Converted from testEarliestByInvalidTimestamp()
=== case
Earliest by invalid timestamp
=== SQL
SELECT EARLIEST_BY(m1, l1) FROM druid.numfoo
=== error
!.* Cannot apply 'EARLIEST_BY' to arguments of type 'EARLIEST_BY\(<FLOAT>, <BIGINT>\)'. Supported form\(s\): 'EARLIEST\(expr, timeColumn\)'
**
==============================================================
Converted from testLatestByInvalidTimestamp()
=== case
Latest by invalid timestamp
=== SQL
SELECT LATEST_BY(m1, l1) FROM druid.numfoo
=== error
!.* Cannot apply 'LATEST_BY' to arguments of type 'LATEST_BY\(<FLOAT>, <BIGINT>\)'. Supported form\(s\): 'LATEST\(expr, timeColumn\)'
**
==============================================================
Converted from testAnyAggregator()
This test the on-heap version of the AnyAggregator (Double/Float/Long/String)
Cannot vectorize virtual expressions.
=== case
Any aggregator
=== SQL
SELECT
  ANY_VALUE(cnt),
  ANY_VALUE(m1),
  ANY_VALUE(m2),
  ANY_VALUE(dim1, 10),
  ANY_VALUE(cnt + 1),
  ANY_VALUE(m1 + 1),
  ANY_VALUE(dim1 || CAST(cnt AS VARCHAR), 10)
FROM druid.foo
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 FLOAT
EXPR$2 DOUBLE
EXPR$3 VARCHAR
EXPR$4 BIGINT
EXPR$5 FLOAT
EXPR$6 VARCHAR
=== plan
LogicalAggregate(group=[{}], EXPR$0=[ANY_VALUE($0)], EXPR$1=[ANY_VALUE($1)], EXPR$2=[ANY_VALUE($2)], EXPR$3=[ANY_VALUE($3, $4)], EXPR$4=[ANY_VALUE($5)], EXPR$5=[ANY_VALUE($6)], EXPR$6=[ANY_VALUE($7, $4)])
  LogicalProject(cnt=[$1], m1=[$5], m2=[$6], dim1=[$2], $f4=[10], $f5=[+($1, 1)], $f6=[+($5, 1)], $f7=[||($2, CAST($1):VARCHAR NOT NULL)])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "(\"cnt\" + 1)",
    "outputType" : "LONG"
  }, {
    "type" : "expression",
    "name" : "v1",
    "expression" : "(\"m1\" + 1)",
    "outputType" : "FLOAT"
  }, {
    "type" : "expression",
    "name" : "v2",
    "expression" : "concat(\"dim1\",CAST(\"cnt\", 'STRING'))",
    "outputType" : "STRING"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "longAny",
    "name" : "a0",
    "fieldName" : "cnt"
  }, {
    "type" : "floatAny",
    "name" : "a1",
    "fieldName" : "m1"
  }, {
    "type" : "doubleAny",
    "name" : "a2",
    "fieldName" : "m2"
  }, {
    "type" : "stringAny",
    "name" : "a3",
    "fieldName" : "dim1",
    "maxStringBytes" : 10
  }, {
    "type" : "longAny",
    "name" : "a4",
    "fieldName" : "v0"
  }, {
    "type" : "floatAny",
    "name" : "a5",
    "fieldName" : "v1"
  }, {
    "type" : "stringAny",
    "name" : "a6",
    "fieldName" : "v2",
    "maxStringBytes" : 10
  } ]
}
=== results
[1,1.0,1.0,"",2,2.0,"1"]
==============================================================
Converted from testAnyAggregatorsOnHeapNumericNulls()
This test the on-heap version of the AnyAggregator (Double/Float/Long)
against numeric columns that have null values (when run in SQL
compatible null mode)
=== case
Any aggregators on heap numeric nulls
=== SQL
SELECT ANY_VALUE(l1), ANY_VALUE(d1), ANY_VALUE(f1)
FROM druid.numfoo
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 DOUBLE
EXPR$2 FLOAT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[ANY_VALUE($0)], EXPR$1=[ANY_VALUE($1)], EXPR$2=[ANY_VALUE($2)])
  LogicalProject(l1=[$12], d1=[$2], f1=[$10])
    LogicalTableScan(table=[[druid, numfoo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "numfoo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "longAny",
    "name" : "a0",
    "fieldName" : "l1"
  }, {
    "type" : "doubleAny",
    "name" : "a1",
    "fieldName" : "d1"
  }, {
    "type" : "floatAny",
    "name" : "a2",
    "fieldName" : "f1"
  } ]
}
=== results
[7,1.0,1.0]
==============================================================
Converted from testAnyAggregatorsOffHeapNumericNulls()
This test the off-heap (buffer) version of the AnyAggregator
(Double/Float/Long) against numeric columnsthat have null values
(when run in SQL compatible null mode)
=== case
Any aggregators off heap numeric nulls
=== SQL
SELECT ANY_VALUE(l1), ANY_VALUE(d1), ANY_VALUE(f1)
FROM druid.numfoo
GROUP BY dim2
=== options
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 DOUBLE
EXPR$2 FLOAT
=== plan
LogicalProject(EXPR$0=[$1], EXPR$1=[$2], EXPR$2=[$3])
  LogicalAggregate(group=[{0}], EXPR$0=[ANY_VALUE($1)], EXPR$1=[ANY_VALUE($2)], EXPR$2=[ANY_VALUE($3)])
    LogicalProject(dim2=[$5], l1=[$12], d1=[$2], f1=[$10])
      LogicalTableScan(table=[[druid, numfoo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "numfoo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "dim2",
    "outputName" : "_d0",
    "outputType" : "STRING"
  } ],
  "aggregations" : [ {
    "type" : "longAny",
    "name" : "a0",
    "fieldName" : "l1"
  }, {
    "type" : "doubleAny",
    "name" : "a1",
    "fieldName" : "d1"
  }, {
    "type" : "floatAny",
    "name" : "a2",
    "fieldName" : "f1"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[325323,1.7,0.1]
[7,1.0,1.0]
[0,0.0,0.0]
=== run
=== options
sqlCompatibleNulls=true
=== results
[325323,1.7,0.1]
[0,0.0,0.0]
[7,1.0,1.0]
[null,null,null]
==============================================================
Converted from testPrimitiveLatestInSubquery()
This test the off-heap (buffer) version of the LatestAggregator
(Double/Float/Long)
=== case
Primitive latest in subquery
=== SQL
SELECT
  SUM(val1),
  SUM(val2),
  SUM(val3)
FROM (
  SELECT
    dim2,
    LATEST(m1) AS val1,
    LATEST(cnt) AS val2,
    LATEST(m2) AS val3
  FROM foo
  GROUP BY dim2
  )
=== options
vectorize=true
=== schema
EXPR$0 DOUBLE
EXPR$1 BIGINT
EXPR$2 DOUBLE
=== plan
LogicalAggregate(group=[{}], EXPR$0=[SUM($0)], EXPR$1=[SUM($1)], EXPR$2=[SUM($2)])
  LogicalProject(val1=[$1], val2=[$2], val3=[$3])
    LogicalAggregate(group=[{0}], val1=[LATEST($1)], val2=[LATEST($2)], val3=[LATEST($3)])
      LogicalProject(dim2=[$3], m1=[$5], cnt=[$1], m2=[$6])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "query",
    "query" : {
      "queryType" : "groupBy",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "granularity" : {
        "type" : "all"
      },
      "dimensions" : [ {
        "type" : "default",
        "dimension" : "dim2",
        "outputName" : "d0",
        "outputType" : "STRING"
      } ],
      "aggregations" : [ {
        "type" : "floatLast",
        "name" : "a0:a",
        "fieldName" : "m1",
        "timeColumn" : "__time"
      }, {
        "type" : "longLast",
        "name" : "a1:a",
        "fieldName" : "cnt",
        "timeColumn" : "__time"
      }, {
        "type" : "doubleLast",
        "name" : "a2:a",
        "fieldName" : "m2",
        "timeColumn" : "__time"
      } ],
      "postAggregations" : [ {
        "type" : "finalizingFieldAccess",
        "name" : "a0",
        "fieldName" : "a0:a"
      }, {
        "type" : "finalizingFieldAccess",
        "name" : "a1",
        "fieldName" : "a1:a"
      }, {
        "type" : "finalizingFieldAccess",
        "name" : "a2",
        "fieldName" : "a2:a"
      } ],
      "limitSpec" : {
        "type" : "NoopLimitSpec"
      }
    }
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ ],
  "aggregations" : [ {
    "type" : "doubleSum",
    "name" : "_a0",
    "fieldName" : "a0"
  }, {
    "type" : "longSum",
    "name" : "_a1",
    "fieldName" : "a1"
  }, {
    "type" : "doubleSum",
    "name" : "_a2",
    "fieldName" : "a2"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[15.0,3,15.0]
=== run
=== options
sqlCompatibleNulls=true
=== results
[18.0,4,18.0]
