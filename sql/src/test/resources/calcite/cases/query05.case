Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
==============================================================
Converted from testDoublePredicateFilterNulls()
=== case
Double predicate filter nulls
=== SQL
SELECT COUNT(*)
FROM druid.numfoo
WHERE d1 > 0
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[>($2, 0)])
      LogicalTableScan(table=[[druid, numfoo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "numfoo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "bound",
    "dimension" : "d1",
    "lower" : "0",
    "lowerStrict" : true,
    "ordering" : {
      "type" : "numeric"
    }
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[2]
==============================================================
Converted from testFloatPredicateFilterNulls()
=== case
Float predicate filter nulls
=== SQL
SELECT COUNT(*)
FROM druid.numfoo
WHERE f1 > 0
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[>($10, 0)])
      LogicalTableScan(table=[[druid, numfoo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "numfoo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "bound",
    "dimension" : "f1",
    "lower" : "0",
    "lowerStrict" : true,
    "ordering" : {
      "type" : "numeric"
    }
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[2]
==============================================================
Converted from testEmptyStringEquality()
=== case
Empty string equality
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE NULLIF(dim2, 'a') = ''
=== options
sqlCompatibleNulls=false
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[=(CASE(=($3, 'a'), null:VARCHAR, $3), '')])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "in",
    "dimension" : "dim2",
    "values" : [ null, "a" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[5]
==============================================================
Converted from testEmptyStringEquality()
=== case
Empty string equality
=== SQL copy
=== options
sqlCompatibleNulls=true
vectorize=true
=== schema copy
=== plan copy
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "selector",
    "dimension" : "dim2",
    "value" : ""
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "context" : {
    "defaultTimeout" : 300000,
    "maxScatterGatherBytes" : 9223372036854775807,
    "sqlCurrentTimestamp" : "2000-01-01T00:00:00Z",
    "sqlQueryId" : "dummy"
  }
}
=== results
[1]
==============================================================
Converted from testNullStringEquality()
=== case
Null string equality
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE NULLIF(dim2, 'a') = null
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[=(CASE(=($3, 'a'), null:VARCHAR, $3), null)])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "scan",
  "dataSource" : {
    "type" : "inline",
    "columnNames" : [ "EXPR$0" ],
    "columnTypes" : [ "LONG" ],
    "rows" : [ [ 0 ] ]
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "resultFormat" : "compactedList",
  "columns" : [ "EXPR$0" ],
  "granularity" : {
    "type" : "all"
  }
}
=== results
[0]
==============================================================
Converted from testCoalesceColumns()
Doesn't conform to the SQL standard, but it's how we do it.
This example is used in the sql.md doc.

Cannot vectorize due to virtual columns.
=== case
Coalesce columns
=== SQL
SELECT COALESCE(dim2, dim1), COUNT(*)
FROM druid.foo
GROUP BY COALESCE(dim2, dim1)
=== options
vectorize=false
=== schema
EXPR$0 VARCHAR
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
  LogicalProject(EXPR$0=[CASE(IS NOT NULL($3), $3, $2)])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "virtualColumns" : [ {
    "type" : "expression",
    "name" : "v0",
    "expression" : "case_searched(notnull(\"dim2\"),\"dim2\",\"dim1\")",
    "outputType" : "STRING"
  } ],
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "v0",
    "outputName" : "d0",
    "outputType" : "STRING"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
["10.1",1]
["2",1]
["a",2]
["abc",2]
=== run
=== options
sqlCompatibleNulls=true
=== results
["",1]
["10.1",1]
["a",2]
["abc",2]
==============================================================
Converted from testColumnIsNull()
Doesn't conform to the SQL standard, but it's how we do it.
This example is used in the sql.md doc.
=== case
Column is null
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE dim2 IS NULL
=== options
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[IS NULL($3)])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "selector",
    "dimension" : "dim2"
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[3]
=== run
=== options
sqlCompatibleNulls=true
=== results
[2]
==============================================================
Converted from testSelfJoin()
Cannot vectorize due to virtual columns.
=== case
Self join
=== SQL
SELECT COUNT(*)
FROM druid.foo x, druid.foo y
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalJoin(condition=[true], joinType=[inner])
      LogicalTableScan(table=[[druid, foo]])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "join",
    "left" : {
      "type" : "table",
      "name" : "foo"
    },
    "right" : {
      "type" : "query",
      "query" : {
        "queryType" : "scan",
        "dataSource" : {
          "type" : "table",
          "name" : "foo"
        },
        "intervals" : {
          "type" : "intervals",
          "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
        },
        "resultFormat" : "compactedList",
        "columns" : [ "__time", "cnt", "dim1", "dim2", "dim3", "m1", "m2", "unique_dim1" ],
        "granularity" : {
          "type" : "all"
        }
      }
    },
    "rightPrefix" : "j0.",
    "condition" : "1",
    "joinType" : "INNER"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[36]
==============================================================
Converted from testGroupingWithNullInFilter()
HashJoinSegmentStorageAdapter is not vectorizable
=== case
Grouping with null in filter
=== SQL
SELECT COUNT(*)
FROM foo
WHERE dim1 IN (NULL)
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalJoin(condition=[=($2, $8)], joinType=[inner])
      LogicalTableScan(table=[[druid, foo]])
      LogicalAggregate(group=[{0}])
        LogicalValues(type=[RecordType(VARCHAR ROW_VALUE)], tuples=[[{ null }]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "join",
    "left" : {
      "type" : "table",
      "name" : "foo"
    },
    "right" : {
      "type" : "inline",
      "columnNames" : [ "ROW_VALUE" ],
      "columnTypes" : [ "STRING" ],
      "rows" : [ [ null ] ]
    },
    "rightPrefix" : "j0.",
    "condition" : "(\"dim1\" == \"j0.ROW_VALUE\")",
    "joinType" : "INNER"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[0]
==============================================================
Converted from testTwoExactCountDistincts()
=== case
Two exact count distincts
=== SQL
SELECT COUNT(distinct dim1), COUNT(distinct dim2)
FROM druid.foo
=== options
planner.useApproximateCountDistinct=false
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0)], EXPR$1=[COUNT(DISTINCT $1)])
  LogicalProject(dim1=[$2], dim2=[$3])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "scan",
  "dataSource" : {
    "type" : "join",
    "left" : {
      "type" : "query",
      "query" : {
        "queryType" : "groupBy",
        "dataSource" : {
          "type" : "query",
          "query" : {
            "queryType" : "groupBy",
            "dataSource" : {
              "type" : "table",
              "name" : "foo"
            },
            "intervals" : {
              "type" : "intervals",
              "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
            },
            "granularity" : {
              "type" : "all"
            },
            "dimensions" : [ {
              "type" : "default",
              "dimension" : "dim1",
              "outputName" : "d0",
              "outputType" : "STRING"
            } ],
            "limitSpec" : {
              "type" : "NoopLimitSpec"
            }
          }
        },
        "intervals" : {
          "type" : "intervals",
          "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
        },
        "granularity" : {
          "type" : "all"
        },
        "dimensions" : [ ],
        "aggregations" : [ {
          "type" : "filtered",
          "aggregator" : {
            "type" : "count",
            "name" : "a0"
          },
          "filter" : {
            "type" : "not",
            "field" : {
              "type" : "selector",
              "dimension" : "d0"
            }
          },
          "name" : "a0"
        } ],
        "limitSpec" : {
          "type" : "NoopLimitSpec"
        }
      }
    },
    "right" : {
      "type" : "query",
      "query" : {
        "queryType" : "groupBy",
        "dataSource" : {
          "type" : "query",
          "query" : {
            "queryType" : "groupBy",
            "dataSource" : {
              "type" : "table",
              "name" : "foo"
            },
            "intervals" : {
              "type" : "intervals",
              "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
            },
            "granularity" : {
              "type" : "all"
            },
            "dimensions" : [ {
              "type" : "default",
              "dimension" : "dim2",
              "outputName" : "d0",
              "outputType" : "STRING"
            } ],
            "limitSpec" : {
              "type" : "NoopLimitSpec"
            }
          }
        },
        "intervals" : {
          "type" : "intervals",
          "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
        },
        "granularity" : {
          "type" : "all"
        },
        "dimensions" : [ ],
        "aggregations" : [ {
          "type" : "filtered",
          "aggregator" : {
            "type" : "count",
            "name" : "a0"
          },
          "filter" : {
            "type" : "not",
            "field" : {
              "type" : "selector",
              "dimension" : "d0"
            }
          },
          "name" : "a0"
        } ],
        "limitSpec" : {
          "type" : "NoopLimitSpec"
        }
      }
    },
    "rightPrefix" : "j0.",
    "condition" : "1",
    "joinType" : "INNER"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "resultFormat" : "compactedList",
  "columns" : [ "a0", "j0.a0" ],
  "granularity" : {
    "type" : "all"
  }
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[5,2]
=== run
=== options
sqlCompatibleNulls=true
=== results
[6,3]
==============================================================
Converted from testGroupByNothingWithLiterallyFalseFilter()
=== case
Group by nothing with literally false filter
=== SQL
SELECT COUNT(*), MAX(cnt)
FROM druid.foo
WHERE 1 = 0
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()], EXPR$1=[MAX($0)])
  LogicalProject(cnt=[$1])
    LogicalFilter(condition=[=(1, 0)])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "scan",
  "dataSource" : {
    "type" : "inline",
    "columnNames" : [ "EXPR$0", "EXPR$1" ],
    "columnTypes" : [ "LONG", "LONG" ],
    "rows" : [ [ 0, null ] ]
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "resultFormat" : "compactedList",
  "columns" : [ "EXPR$0", "EXPR$1" ],
  "granularity" : {
    "type" : "all"
  }
}
=== results
[0,null]
==============================================================
Converted from testGroupByNothingWithImpossibleTimeFilter()
Regression test for https://github.com/apache/druid/issues/7671
=== case
Group by nothing with impossible time filter
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE FLOOR(__time TO DAY) = TIMESTAMP '2000-01-02 01:00:00'
   OR FLOOR(__time TO DAY) = TIMESTAMP '2000-01-02 02:00:00'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[OR(=(FLOOR($0, FLAG(DAY)), 2000-01-02 01:00:00), =(FLOOR($0, FLAG(DAY)), 2000-01-02 02:00:00))])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[0]
==============================================================
Converted from testGroupByWithImpossibleTimeFilter()
This gets optimized into 'false'
=== case
Group by with impossible time filter
=== SQL
SELECT dim1, COUNT(*)
FROM druid.foo
WHERE FLOOR(__time TO DAY) = TIMESTAMP '2000-01-02 01:00:00'
   OR FLOOR(__time TO DAY) = TIMESTAMP '2000-01-02 02:00:00'
GROUP BY 1
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
dim1 VARCHAR
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{0}], EXPR$1=[COUNT()])
  LogicalProject(dim1=[$2])
    LogicalFilter(condition=[OR(=(FLOOR($0, FLAG(DAY)), 2000-01-02 01:00:00), =(FLOOR($0, FLAG(DAY)), 2000-01-02 02:00:00))])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "groupBy",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ ]
  },
  "granularity" : {
    "type" : "all"
  },
  "dimensions" : [ {
    "type" : "default",
    "dimension" : "dim1",
    "outputName" : "d0",
    "outputType" : "STRING"
  } ],
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "limitSpec" : {
    "type" : "NoopLimitSpec"
  }
}
=== results
==============================================================
Converted from testGroupByOneColumnWithLiterallyFalseFilter()
=== case
Group by one column with literally false filter
=== SQL
SELECT COUNT(*), MAX(cnt)
FROM druid.foo
WHERE 1 = 0
GROUP BY dim1
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
=== plan
LogicalProject(EXPR$0=[$1], EXPR$1=[$2])
  LogicalAggregate(group=[{0}], EXPR$0=[COUNT()], EXPR$1=[MAX($1)])
    LogicalProject(dim1=[$2], cnt=[$1])
      LogicalFilter(condition=[=(1, 0)])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "scan",
  "dataSource" : {
    "type" : "inline",
    "columnNames" : [ "EXPR$0", "EXPR$1" ],
    "columnTypes" : [ "LONG", "LONG" ],
    "rows" : [ ]
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "resultFormat" : "compactedList",
  "columns" : [ "EXPR$0", "EXPR$1" ],
  "granularity" : {
    "type" : "all"
  }
}
=== results
==============================================================
Converted from testGroupByWithFilterMatchingNothing()
=== case
Group by with filter matching nothing
=== SQL
SELECT COUNT(*), MAX(cnt)
FROM druid.foo
WHERE dim1 = 'foobar'
=== options
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()], EXPR$1=[MAX($0)])
  LogicalProject(cnt=[$1])
    LogicalFilter(condition=[=($2, 'foobar')])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "selector",
    "dimension" : "dim1",
    "value" : "foobar"
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  }, {
    "type" : "longMax",
    "name" : "a1",
    "fieldName" : "cnt"
  } ]
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[0,-9223372036854775808]
=== run
=== options
sqlCompatibleNulls=true
=== results
[0,null]
==============================================================
Converted from testGroupByWithGroupByEmpty()
=== case
Group by with group by empty
=== SQL
SELECT COUNT(*), SUM(cnt), MIN(cnt)
FROM druid.foo
GROUP BY ()
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
EXPR$2 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()], EXPR$1=[SUM($0)], EXPR$2=[MIN($0)])
  LogicalProject(cnt=[$1])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  }, {
    "type" : "longSum",
    "name" : "a1",
    "fieldName" : "cnt"
  }, {
    "type" : "longMin",
    "name" : "a2",
    "fieldName" : "cnt"
  } ]
}
=== results
[6,6,1]
==============================================================
Converted from testGroupByWithFilterMatchingNothingWithGroupByLiteral()
=== case
Group by with filter matching nothing with group by literal
=== SQL
SELECT COUNT(*), MAX(cnt)
FROM druid.foo
WHERE dim1 = 'foobar'
GROUP BY 'dummy'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
EXPR$1 BIGINT
=== plan
LogicalProject(EXPR$0=[$1], EXPR$1=[$2])
  LogicalAggregate(group=[{0}], EXPR$0=[COUNT()], EXPR$1=[MAX($1)])
    LogicalProject($f0=['dummy'], cnt=[$1])
      LogicalFilter(condition=[=($2, 'foobar')])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "selector",
    "dimension" : "dim1",
    "value" : "foobar"
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  }, {
    "type" : "longMax",
    "name" : "a1",
    "fieldName" : "cnt"
  } ],
  "context" : {
    "skipEmptyBuckets" : true
  }
}
=== results
==============================================================
Converted from testCountNonNullColumn()
=== case
Count non null column
=== SQL
SELECT COUNT(cnt)
FROM druid.foo
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject(cnt=[$1])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[6]
==============================================================
Converted from testCountNonNullColumn()
=== case
Count non null column
=== SQL copy
=== options
sqlCompatibleNulls=true
vectorize=true
=== schema copy
=== plan copy
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "filtered",
    "aggregator" : {
      "type" : "count",
      "name" : "a0"
    },
    "filter" : {
      "type" : "not",
      "field" : {
        "type" : "selector",
        "dimension" : "cnt"
      }
    },
    "name" : "a0"
  } ],
  "context" : {
    "defaultTimeout" : 300000,
    "maxScatterGatherBytes" : 9223372036854775807,
    "sqlCurrentTimestamp" : "2000-01-01T00:00:00Z",
    "sqlQueryId" : "dummy"
  }
}
=== results copy
==============================================================
Converted from testCountNullableColumn()
=== case
Count nullable column
=== SQL
SELECT COUNT(dim2)
FROM druid.foo
=== options
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT($0)])
  LogicalProject(dim2=[$3])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "filtered",
    "aggregator" : {
      "type" : "count",
      "name" : "a0"
    },
    "filter" : {
      "type" : "not",
      "field" : {
        "type" : "selector",
        "dimension" : "dim2"
      }
    },
    "name" : "a0"
  } ]
}
=== run
=== options
sqlCompatibleNulls=false
=== results
[3]
=== run
=== options
sqlCompatibleNulls=true
=== results
[4]
==============================================================
Converted from testCountNullableExpression()
=== case
Count nullable expression
=== SQL
SELECT COUNT(CASE WHEN dim2 = 'abc' THEN 'yes' WHEN dim2 = 'def' THEN 'yes' END)
FROM druid.foo
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT($0)])
  LogicalProject($f0=[CASE(OR(=($3, 'abc'), =($3, 'def')), 'yes', null:CHAR(3))])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "filtered",
    "aggregator" : {
      "type" : "count",
      "name" : "a0"
    },
    "filter" : {
      "type" : "in",
      "dimension" : "dim2",
      "values" : [ "abc", "def" ]
    },
    "name" : "a0"
  } ]
}
=== results
[1]
==============================================================
Converted from testCountStar()
=== case
Count star
=== SQL
SELECT COUNT(*) FROM druid.foo
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[6]
==============================================================
Converted from testCountStarOnCommonTableExpression()
=== case
Count star on common table expression
=== SQL
WITH  beep (dim1_firstchar) AS (
  SELECT SUBSTRING(dim1, 1, 1)
  FROM foo WHERE dim2 = 'a'
  )
SELECT COUNT(*)
FROM beep
WHERE dim1_firstchar <> 'z'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[<>($0, 'z')])
      LogicalProject(EXPR$0=[SUBSTRING($2, 1, 1)])
        LogicalFilter(condition=[=($3, 'a')])
          LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "and",
    "fields" : [ {
      "type" : "selector",
      "dimension" : "dim2",
      "value" : "a"
    }, {
      "type" : "not",
      "field" : {
        "type" : "selector",
        "dimension" : "dim1",
        "value" : "z",
        "extractionFn" : {
          "type" : "substring",
          "index" : 0,
          "length" : 1
        }
      }
    } ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[2]
==============================================================
Converted from testCountStarOnView()
=== case
Count star on view
=== SQL
SELECT COUNT(*)
FROM view.aview
WHERE dim1_firstchar <> 'z'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[<>($0, 'z')])
      LogicalProject(dim1_firstchar=[SUBSTRING($2, 1, 1)])
        LogicalFilter(condition=[=($3, 'a')])
          LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "and",
    "fields" : [ {
      "type" : "selector",
      "dimension" : "dim2",
      "value" : "a"
    }, {
      "type" : "not",
      "field" : {
        "type" : "selector",
        "dimension" : "dim1",
        "value" : "z",
        "extractionFn" : {
          "type" : "substring",
          "index" : 0,
          "length" : 1
        }
      }
    } ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[2]
==============================================================
Converted from testConfusedView()
=== case
Confused view
=== SQL
SELECT COUNT(*)
FROM view.dview as druid
WHERE druid.numfoo <> 'z'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[<>($0, 'z')])
      LogicalProject(numfoo=[SUBSTRING($2, 1, 1)])
        LogicalFilter(condition=[=($3, 'a')])
          LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "and",
    "fields" : [ {
      "type" : "selector",
      "dimension" : "dim2",
      "value" : "a"
    }, {
      "type" : "not",
      "field" : {
        "type" : "selector",
        "dimension" : "dim1",
        "value" : "z",
        "extractionFn" : {
          "type" : "substring",
          "index" : 0,
          "length" : 1
        }
      }
    } ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[2]
==============================================================
Converted from testViewAndJoin()
=== case
View and join
=== SQL
SELECT COUNT(*)
FROM view.cview as a
INNER JOIN druid.foo d on d.dim2 = a.dim2
WHERE a.dim1_firstchar <> 'z'
=== context
enableJoinLeftTableScanDirect=true
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[<>($0, 'z')])
      LogicalJoin(condition=[=($6, $1)], joinType=[inner])
        LogicalProject(dim1_firstchar=[SUBSTRING($2, 1, 1)], dim2=[$3], l2=[$21])
          LogicalJoin(condition=[=($3, $13)], joinType=[inner])
            LogicalProject(__time=[$0], cnt=[$1], dim1=[$2], dim2=[$3], dim3=[$4], m1=[$5], m2=[$6], unique_dim1=[$7])
              LogicalFilter(condition=[=($3, 'a')])
                LogicalTableScan(table=[[druid, foo]])
            LogicalTableScan(table=[[druid, numfoo]])
        LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "join",
    "left" : {
      "type" : "join",
      "left" : {
        "type" : "table",
        "name" : "foo"
      },
      "right" : {
        "type" : "query",
        "query" : {
          "queryType" : "scan",
          "dataSource" : {
            "type" : "table",
            "name" : "numfoo"
          },
          "intervals" : {
            "type" : "intervals",
            "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
          },
          "resultFormat" : "compactedList",
          "columns" : [ "dim2" ],
          "context" : {
            "enableJoinLeftTableScanDirect" : true
          },
          "granularity" : {
            "type" : "all"
          }
        }
      },
      "rightPrefix" : "j0.",
      "condition" : "(\"dim2\" == \"j0.dim2\")",
      "joinType" : "INNER",
      "leftFilter" : {
        "type" : "bound",
        "dimension" : "dim2",
        "lower" : "a",
        "upper" : "a",
        "ordering" : {
          "type" : "lexicographic"
        }
      }
    },
    "right" : {
      "type" : "query",
      "query" : {
        "queryType" : "scan",
        "dataSource" : {
          "type" : "table",
          "name" : "foo"
        },
        "intervals" : {
          "type" : "intervals",
          "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
        },
        "resultFormat" : "compactedList",
        "columns" : [ "dim2" ],
        "context" : {
          "enableJoinLeftTableScanDirect" : true
        },
        "granularity" : {
          "type" : "all"
        }
      }
    },
    "rightPrefix" : "_j0.",
    "condition" : "('a' == \"_j0.dim2\")",
    "joinType" : "INNER"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "not",
    "field" : {
      "type" : "selector",
      "dimension" : "dim1",
      "value" : "z",
      "extractionFn" : {
        "type" : "substring",
        "index" : 0,
        "length" : 1
      }
    }
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "context" : {
    "enableJoinLeftTableScanDirect" : true
  }
}
=== results
[8]
==============================================================
Converted from testExplainCountStarOnView()

No vectorize for EXPLAIN tests so that the explain context
is constant. (The context contains the vectorize option.)
=== case
Explain count star on view
=== SQL
EXPLAIN PLAN FOR SELECT COUNT(*) FROM view.aview WHERE dim1_firstchar <> 'z'
=== options
sqlCompatibleNulls=both
vectorize=false
=== schema
PLAN VARCHAR
RESOURCES VARCHAR
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[<>($0, 'z')])
      LogicalProject(dim1_firstchar=[SUBSTRING($2, 1, 1)])
        LogicalFilter(condition=[=($3, 'a')])
          LogicalTableScan(table=[[druid, foo]])
=== explain
DruidQueryRel(query=[(
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "and",
    "fields" : [ {
      "type" : "selector",
      "dimension" : "dim2",
      "value" : "a"
    }, {
      "type" : "not",
      "field" : {
        "type" : "selector",
        "dimension" : "dim1",
        "value" : "z",
        "extractionFn" : {
          "type" : "substring",
          "index" : 0,
          "length" : 1
        }
      }
    } ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ],
  "context" : {
!    "sqlQueryId" : ".*"
  }
},
signature=[(
  {a0:LONG}
])
---
[ {
  "name" : "aview",
  "type" : "VIEW"
} ]
=== results
["DruidQueryRel(query=[{\"queryType\":\"timeseries\",\"dataSource\":{\"type\":\"table\",\"name\":\"foo\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"filter\":{\"type\":\"and\",\"fields\":[{\"type\":\"selector\",\"dimension\":\"dim2\",\"value\":\"a\"},{\"type\":\"not\",\"field\":{\"type\":\"selector\",\"dimension\":\"dim1\",\"value\":\"z\",\"extractionFn\":{\"type\":\"substring\",\"index\":0,\"length\":1}}}]},\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}],\"context\":{\"defaultTimeout\":300000,\"maxScatterGatherBytes\":9223372036854775807,\"sqlCurrentTimestamp\":\"2000-01-01T00:00:00Z\",\"sqlQueryId\":\"dummy\",\"vectorize\":\"FALSE\",\"vectorizeVirtualColumns\":\"FALSE\"}}], signature=[{a0:LONG}])\n","[{\"name\":\"aview\",\"type\":\"VIEW\"}]"]
==============================================================
Converted from testCountStarWithLikeFilter()
=== case
Count star with like filter
=== SQL
SELECT COUNT(*)
FROM druid.foo
WHERE dim1 like 'a%'
   OR dim2 like '%xb%' escape 'x'
=== options
sqlCompatibleNulls=both
vectorize=true
=== schema
EXPR$0 BIGINT
=== plan
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
  LogicalProject($f0=[0])
    LogicalFilter(condition=[OR(LIKE($2, 'a%'), LIKE($3, '%xb%', 'x'))])
      LogicalTableScan(table=[[druid, foo]])
=== native
{
  "queryType" : "timeseries",
  "dataSource" : {
    "type" : "table",
    "name" : "foo"
  },
  "intervals" : {
    "type" : "intervals",
    "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
  },
  "filter" : {
    "type" : "or",
    "fields" : [ {
      "type" : "like",
      "dimension" : "dim1",
      "pattern" : "a%",
      "escape" : null,
      "extractionFn" : null
    }, {
      "type" : "like",
      "dimension" : "dim2",
      "pattern" : "%xb%",
      "escape" : "x",
      "extractionFn" : null
    } ]
  },
  "granularity" : {
    "type" : "all"
  },
  "aggregations" : [ {
    "type" : "count",
    "name" : "a0"
  } ]
}
=== results
[2]
