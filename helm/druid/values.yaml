# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default values for druid.

image:
  repository: apache/druid
  tag: 0.21.1
  pullPolicy: IfNotPresent
  pullSecrets: []

configMap:
  ## If false, configMap will not be applied
  ##
  enabled: true

## The configurations below would be set as environment variable across all pod
globalConfig:
  ## DRUID env vars. ref: https://github.com/apache/druid/blob/master/distribution/docker/druid.sh#L29
  DRUID_USE_CONTAINER_IP: 'true'

  ## Druid Common Configurations. ref: https://druid.apache.org/docs/latest/configuration/index.html#common-configurations
  druid_extensions_loadList: '["druid-datasketches", "druid-lookups-cached-global", "postgresql-metadata-storage"]'

  druid_metadata_storage_type: derby

  druid_storage_type: local

  druid_indexer_logs_type: file

  ## Druid Emitting Metrics. ref: https://druid.apache.org/docs/latest/configuration/index.html#emitting-metrics
  druid_emitter: noop
  druid_emitter_logging_logLevel: info

gCloudStorage:
  enabled: false
  secretName: google-cloud-key

hdfs:
  # Enable hdfs as deep-storage reference: http://druid.apache.org/docs/latest/development/extensions-core/hdfs.html
  # Need to do:
  # 1. enabled: true
  # 2. add `"druid-hdfs-storage"` to `druid_extensions_loadList`
  # 3. supply configmap's name which contains hdfs-site.xml and core-site.xml
  enabled: false
  configMapName: ''

broker:
  ## If false, broker will not be installed
  ##
  enabled: true
  name: broker
  replicaCount: 1
  port: 8082
  serviceType: ClusterIP

  config:
    DRUID_XMX: 512m
    DRUID_XMS: 512m

    # In fact, broker need to merge result, which means more direct memory
    DRUID_MAXDIRECTMEMORYSIZE: 400m
    # druid_monitoring_monitors: '["org.apache.druid.client.cache.CacheMonitor", "org.apache.druid.server.metrics.QueryCountStatsMonitor"]'

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  resources: {}
    # limits:
    #   cpu: 1
    #   memory: 1Gi
    # requests:
    #   cpu: 250m
    #   memory: 512Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  podAnnotations: {}

coordinator:
  ## If false, coordinator will not be installed
  ##
  enabled: true
  name: coordinator
  replicaCount: 1
  port: 8081
  serviceType: ClusterIP

  config:
    DRUID_XMX: 256m
    DRUID_XMS: 256m

    # Every coordinator process can act like an overload process as well
    druid_coordinator_asOverlord_enabled: 'false'
    
    # druid_monitoring_monitors: '["org.apache.druid.server.metrics.TaskCountStatsMonitor"]'

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  resources: {}
    # limits:
    #   cpu: 500m
    #   memory: 1Gi
    # requests:
    #   cpu: 250m
    #   memory: 512Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  podAnnotations: {}

overlord:
  ## If true, the **separate** overlord will be installed
  ##
  enabled: false
  name: overlord
  replicaCount: 1
  port: 8081
  serviceType: ClusterIP

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  resources: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  podAnnotations: {}

historical:
  ## If false, historical will not be installed
  ##
  enabled: true
  name: historical
  replicaCount: 1
  port: 8083
  serviceType: ClusterIP

  config:
    DRUID_XMX: 512m
    DRUID_XMS: 512m
    DRUID_MAXDIRECTMEMORYSIZE: 400m
    # druid_monitoring_monitors: '["org.apache.druid.client.cache.CacheMonitor", "org.apache.druid.server.metrics.HistoricalMetricsMonitor", "org.apache.druid.server.metrics.QueryCountStatsMonitor"]'
    druid_segmentCache_locations: '[{"path":"/opt/druid/var/druid/segment-cache","maxSize":300000000000}]'

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: "4Gi"
    # storageClass: "ssd"

  antiAffinity: "soft"

  nodeAffinity: {}

  nodeSelector: {}

  securityContext:
    fsGroup: 1000

  tolerations: []

  resources: {}
    # limits:
    #   cpu: 2
    #   memory: 2Gi
    # requests:
    #   cpu: 500m
    #   memory: 512Mi

  ## (dict) If specified, apply these annotations to each master Pod
  podAnnotations: {}

  podDisruptionBudget:
    enabled: false
    # minAvailable: 2
    maxUnavailable: 1

  updateStrategy:
    type: RollingUpdate

middleManager:
  ## If false, middleManager will not be installed
  ##
  enabled: true
  name: middle-manager
  replicaCount: 1
  port: 8091
  serviceType: ClusterIP

  config:
    DRUID_XMX: 64m
    DRUID_XMS: 64m

    # druid_processing_* configured here would be passed to peons

    druid_indexer_runner_javaOptsArray: '["-server", "-Xms256m", "-Xmx256m", "-XX:MaxDirectMemorySize=300m", "-Duser.timezone=UTC", "-Dfile.encoding=UTF-8", "-XX:+ExitOnOutOfMemoryError", "-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager"]'

  autoscaling:
    enabled: false
    minReplicas: 2
    maxReplicas: 5
    metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 60

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: "4Gi"
    # storageClass: "ssd"

  antiAffinity: "soft"

  nodeAffinity: {}

  nodeSelector: {}

  securityContext:
    fsGroup: 1000

  tolerations: []

  resources: {}
    # limits:
    #   cpu: 500m
    #   memory: 1Gi
    # requests:
    #   cpu: 250m
    #   memory: 256Mi

  ## (dict) If specified, apply these annotations to each master Pod
  podAnnotations: {}

  podDisruptionBudget:
    enabled: false
    # minAvailable: 2
    maxUnavailable: 1

  updateStrategy:
    type: RollingUpdate

router:
  ## If false, router will not be installed
  ##
  enabled: true
  name: router
  replicaCount: 1
  port: 8888
  serviceType: ClusterIP

  config:
    DRUID_XMX: 128m
    DRUID_XMS: 128m
    DRUID_MAXDIRECTMEMORYSIZE: 128m

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  resources: {}
    # limits:
    #   cpu: 250m
    #   memory: 256Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  podAnnotations: {}
