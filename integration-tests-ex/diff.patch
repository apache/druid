diff --git a/integration-tests-ex/cases/cluster/BackwardCompatibility/docker-compose.yaml b/integration-tests-ex/cases/cluster/BackwardCompatibility/docker-compose.yaml
new file mode 100644
index 0000000000..fe71ad2b25
--- /dev/null
+++ b/integration-tests-ex/cases/cluster/BackwardCompatibility/docker-compose.yaml
@@ -0,0 +1,107 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+networks:
+  druid-it-net:
+    name: druid-it-net
+    ipam:
+      config:
+        - subnet: 172.172.172.0/24
+
+services:
+  zookeeper:
+    extends:
+      file: ../Common/dependencies.yaml
+      service: zookeeper
+
+  metadata:
+    extends:
+      file: ../Common/dependencies.yaml
+      service: metadata
+
+  coordinator:
+    extends:
+      file: ../Common/druid.yaml
+      service: coordinator
+    image: ${DRUID_PREVIOUS_IT_IMAGE_NAME}
+    container_name: coordinator
+    environment:
+      - DRUID_INTEGRATION_TEST_GROUP=${DRUID_INTEGRATION_TEST_GROUP}
+      # The frequency with which the coordinator polls the database
+      # for changes. The DB population code has to wait at least this
+      # long for the coordinator to notice changes.
+      - druid_manager_segments_pollDuration=PT5S
+      - druid_coordinator_period=PT10S
+    depends_on:
+      - zookeeper
+      - metadata
+
+  overlord:
+    extends:
+      file: ../Common/druid.yaml
+      service: overlord
+    image: ${DRUID_PREVIOUS_IT_IMAGE_NAME}
+    container_name: overlord
+    environment:
+      - DRUID_INTEGRATION_TEST_GROUP=${DRUID_INTEGRATION_TEST_GROUP}
+    depends_on:
+      - zookeeper
+      - metadata
+
+  broker:
+    extends:
+      file: ../Common/druid.yaml
+      service: broker
+    environment:
+      - DRUID_INTEGRATION_TEST_GROUP=${DRUID_INTEGRATION_TEST_GROUP}
+    depends_on:
+      - zookeeper
+
+  router:
+    extends:
+      file: ../Common/druid.yaml
+      service: router
+    environment:
+      - DRUID_INTEGRATION_TEST_GROUP=${DRUID_INTEGRATION_TEST_GROUP}
+    depends_on:
+      - zookeeper
+
+  historical:
+    extends:
+      file: ../Common/druid.yaml
+      service: historical
+    environment:
+      - DRUID_INTEGRATION_TEST_GROUP=${DRUID_INTEGRATION_TEST_GROUP}
+    depends_on:
+      - zookeeper
+
+  middlemanager:
+    extends:
+      file: ../Common/druid.yaml
+      service: middlemanager
+    environment:
+      - DRUID_INTEGRATION_TEST_GROUP=${DRUID_INTEGRATION_TEST_GROUP}
+    volumes:
+      # Test data
+      - ../../resources:/resources
+    depends_on:
+      - zookeeper
+
+  kafka:
+    extends:
+      file: ../Common/dependencies.yaml
+      service: kafka
+    depends_on:
+      - zookeeper
diff --git a/integration-tests-ex/cases/cluster/Common/dependencies.yaml b/integration-tests-ex/cases/cluster/Common/dependencies.yaml
index 0409c30bf5..0adf603ff9 100644
--- a/integration-tests-ex/cases/cluster/Common/dependencies.yaml
+++ b/integration-tests-ex/cases/cluster/Common/dependencies.yaml
@@ -26,7 +26,7 @@ services:
   # See https://hub.docker.com/_/zookeeper
   zookeeper:
     # Uncomment the following when running on Apple Silicon processors:
-    # platform: linux/x86_64
+    platform: linux/x86_64
     image: zookeeper:${ZK_VERSION}
     container_name: zookeeper
     labels:
@@ -46,7 +46,7 @@ services:
   kafka:
     image: bitnami/kafka:${KAFKA_VERSION}
     container_name: kafka
-    # platform: linux/x86_64
+    platform: linux/x86_64
     labels:
       druid-int-test: "true"
     ports:
@@ -73,7 +73,7 @@ services:
   # The image will intialize the user and DB upon first start.
   metadata:
     # Uncomment the following when running on Apple Silicon processors:
-    # platform: linux/x86_64
+    platform: linux/x86_64
     image: mysql:$MYSQL_IMAGE_VERSION
     container_name: metadata
     labels:
diff --git a/integration-tests-ex/cases/pom.xml b/integration-tests-ex/cases/pom.xml
index 40461dd8ef..a07dad2d14 100644
--- a/integration-tests-ex/cases/pom.xml
+++ b/integration-tests-ex/cases/pom.xml
@@ -459,6 +459,15 @@
                 <it.category>GcsDeepStorage</it.category>
             </properties>
         </profile>
+        <profile>
+            <id>IT-BackwardCompatibility</id>
+            <activation>
+                <activeByDefault>false</activeByDefault>
+            </activation>
+            <properties>
+                <it.category>BackwardCompatibility</it.category>
+            </properties>
+        </profile>
         <profile>
             <id>docker-tests</id>
             <activation>
diff --git a/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/BackwardCompatibility/ITBackwardCompatibilityIndexerTest.java b/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/BackwardCompatibility/ITBackwardCompatibilityIndexerTest.java
new file mode 100644
index 0000000000..7d07d51892
--- /dev/null
+++ b/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/BackwardCompatibility/ITBackwardCompatibilityIndexerTest.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.testsEx.BackwardCompatibility;
+
+import org.apache.druid.testsEx.categories.BackwardCompatibility;
+import org.apache.druid.testsEx.config.DruidTestRunner;
+import org.apache.druid.testsEx.indexer.IndexerTest;
+import org.junit.experimental.categories.Category;
+import org.junit.runner.RunWith;
+
+@RunWith(DruidTestRunner.class)
+@Category({BackwardCompatibility.class})
+public class ITBackwardCompatibilityIndexerTest extends IndexerTest
+{
+}
diff --git a/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/categories/BackwardCompatibility.java b/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/categories/BackwardCompatibility.java
new file mode 100644
index 0000000000..7e357e6df8
--- /dev/null
+++ b/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/categories/BackwardCompatibility.java
@@ -0,0 +1,24 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.testsEx.categories;
+
+public class BackwardCompatibility
+{
+}
diff --git a/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/indexer/ITIndexerTest.java b/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/indexer/ITIndexerTest.java
index 65b8dc0b1a..06a097d608 100644
--- a/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/indexer/ITIndexerTest.java
+++ b/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/indexer/ITIndexerTest.java
@@ -19,368 +19,13 @@
 
 package org.apache.druid.testsEx.indexer;
 
-import com.fasterxml.jackson.core.JsonProcessingException;
-import com.google.inject.Inject;
-import org.apache.druid.java.util.common.Intervals;
-import org.apache.druid.java.util.common.Pair;
-import org.apache.druid.java.util.common.StringUtils;
-import org.apache.druid.server.coordinator.CoordinatorDynamicConfig;
-import org.apache.druid.testing.clients.CoordinatorResourceTestClient;
-import org.apache.druid.testing.utils.ITRetryUtil;
 import org.apache.druid.testsEx.categories.BatchIndex;
 import org.apache.druid.testsEx.config.DruidTestRunner;
-import org.joda.time.Interval;
-import org.junit.Assert;
-import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.junit.runner.RunWith;
 
-import java.io.Closeable;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.function.Function;
-
 @RunWith(DruidTestRunner.class)
-@Category(BatchIndex.class)
-public class ITIndexerTest extends AbstractITBatchIndexTest
+@Category({BatchIndex.class})
+public class ITIndexerTest extends IndexerTest
 {
-  private static final String INDEX_TASK = "/indexer/wikipedia_index_task.json";
-  private static final String INDEX_QUERIES_RESOURCE = "/indexer/wikipedia_index_queries.json";
-  private static final String INDEX_DATASOURCE = "wikipedia_index_test";
-
-  private static final String INDEX_WITH_TIMESTAMP_TASK = "/indexer/wikipedia_with_timestamp_index_task.json";
-  // TODO: add queries that validate timestamp is different from the __time column since it is a dimension
-  // TODO: https://github.com/apache/druid/issues/9565
-  private static final String INDEX_WITH_TIMESTAMP_QUERIES_RESOURCE = "/indexer/wikipedia_index_queries.json";
-  private static final String INDEX_WITH_TIMESTAMP_DATASOURCE = "wikipedia_with_timestamp_index_test";
-
-  private static final String REINDEX_TASK = "/indexer/wikipedia_reindex_task.json";
-  private static final String REINDEX_TASK_WITH_DRUID_INPUT_SOURCE = "/indexer/wikipedia_reindex_druid_input_source_task.json";
-  private static final String REINDEX_QUERIES_RESOURCE = "/indexer/wikipedia_reindex_queries.json";
-  private static final String REINDEX_DATASOURCE = "wikipedia_reindex_test";
-
-  private static final String MERGE_INDEX_TASK = "/indexer/wikipedia_merge_index_task.json";
-  private static final String MERGE_INDEX_QUERIES_RESOURCE = "/indexer/wikipedia_merge_index_queries.json";
-  private static final String MERGE_INDEX_DATASOURCE = "wikipedia_merge_index_test";
-
-  private static final String MERGE_REINDEX_TASK = "/indexer/wikipedia_merge_reindex_task.json";
-  private static final String MERGE_REINDEX_TASK_WITH_DRUID_INPUT_SOURCE = "/indexer/wikipedia_merge_reindex_druid_input_source_task.json";
-  private static final String MERGE_REINDEX_QUERIES_RESOURCE = "/indexer/wikipedia_merge_index_queries.json";
-  private static final String MERGE_REINDEX_DATASOURCE = "wikipedia_merge_reindex_test";
-
-  private static final String INDEX_WITH_MERGE_COLUMN_LIMIT_TASK = "/indexer/wikipedia_index_with_merge_column_limit_task.json";
-  private static final String INDEX_WITH_MERGE_COLUMN_LIMIT_DATASOURCE = "wikipedia_index_with_merge_column_limit_test";
-
-  private static final String GET_LOCKED_INTERVALS = "wikipedia_index_get_locked_intervals_test";
-
-  private static final CoordinatorDynamicConfig DYNAMIC_CONFIG_PAUSED =
-      CoordinatorDynamicConfig.builder().withPauseCoordination(true).build();
-  private static final CoordinatorDynamicConfig DYNAMIC_CONFIG_DEFAULT =
-      CoordinatorDynamicConfig.builder().build();
-
-  @Inject
-  CoordinatorResourceTestClient coordinatorClient;
-
-  @Test
-  public void testIndexData() throws Exception
-  {
-    final String reindexDatasource = REINDEX_DATASOURCE + "-testIndexData";
-    final String reindexDatasourceWithDruidInputSource = REINDEX_DATASOURCE + "-testIndexData-druidInputSource";
-    try (
-        final Closeable ignored1 = unloader(INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix());
-        final Closeable ignored2 = unloader(reindexDatasource + config.getExtraDatasourceNameSuffix());
-        final Closeable ignored3 = unloader(reindexDatasourceWithDruidInputSource + config.getExtraDatasourceNameSuffix())
-    ) {
-
-      final Function<String, String> transform = spec -> {
-        try {
-          return StringUtils.replace(
-              spec,
-              "%%SEGMENT_AVAIL_TIMEOUT_MILLIS%%",
-              jsonMapper.writeValueAsString("0")
-          );
-        }
-        catch (JsonProcessingException e) {
-          throw new RuntimeException(e);
-        }
-      };
-
-      doIndexTest(
-          INDEX_DATASOURCE,
-          INDEX_TASK,
-          transform,
-          INDEX_QUERIES_RESOURCE,
-          false,
-          true,
-          true,
-          new Pair<>(false, false)
-      );
-      doReindexTest(
-          INDEX_DATASOURCE,
-          reindexDatasource,
-          REINDEX_TASK,
-          REINDEX_QUERIES_RESOURCE,
-          new Pair<>(false, false)
-      );
-      doReindexTest(
-          INDEX_DATASOURCE,
-          reindexDatasourceWithDruidInputSource,
-          REINDEX_TASK_WITH_DRUID_INPUT_SOURCE,
-          REINDEX_QUERIES_RESOURCE,
-          new Pair<>(false, false)
-      );
-    }
-  }
-
-  @Test
-  public void testReIndexDataWithTimestamp() throws Exception
-  {
-    final String reindexDatasource = REINDEX_DATASOURCE + "-testReIndexDataWithTimestamp";
-    final String reindexDatasourceWithDruidInputSource = REINDEX_DATASOURCE + "-testReIndexDataWithTimestamp-druidInputSource";
-    try (
-        final Closeable ignored1 = unloader(INDEX_WITH_TIMESTAMP_DATASOURCE + config.getExtraDatasourceNameSuffix());
-        final Closeable ignored2 = unloader(reindexDatasource + config.getExtraDatasourceNameSuffix());
-        final Closeable ignored3 = unloader(reindexDatasourceWithDruidInputSource + config.getExtraDatasourceNameSuffix())
-    ) {
-      doIndexTest(
-          INDEX_WITH_TIMESTAMP_DATASOURCE,
-          INDEX_WITH_TIMESTAMP_TASK,
-          INDEX_WITH_TIMESTAMP_QUERIES_RESOURCE,
-          false,
-          true,
-          true,
-          new Pair<>(false, false)
-      );
-      doReindexTest(
-          INDEX_WITH_TIMESTAMP_DATASOURCE,
-          reindexDatasource,
-          REINDEX_TASK,
-          REINDEX_QUERIES_RESOURCE,
-          new Pair<>(false, false)
-      );
-      doReindexTest(
-          INDEX_WITH_TIMESTAMP_DATASOURCE,
-          reindexDatasourceWithDruidInputSource,
-          REINDEX_TASK_WITH_DRUID_INPUT_SOURCE,
-          REINDEX_QUERIES_RESOURCE,
-          new Pair<>(false, false)
-      );
-    }
-  }
-
-  @Test
-  public void testReIndexWithNonExistingDatasource() throws Exception
-  {
-    Pair<Boolean, Boolean> dummyPair = new Pair<>(false, false);
-    final String fullBaseDatasourceName = "nonExistingDatasource2904";
-    final String fullReindexDatasourceName = "newDatasource123";
-
-    String taskSpec = StringUtils.replace(
-        getResourceAsString(REINDEX_TASK_WITH_DRUID_INPUT_SOURCE),
-        "%%DATASOURCE%%",
-        fullBaseDatasourceName
-    );
-    taskSpec = StringUtils.replace(
-        taskSpec,
-        "%%REINDEX_DATASOURCE%%",
-        fullReindexDatasourceName
-    );
-
-    // This method will also verify task is successful after task finish running
-    // We expect task to be successful even if the datasource to reindex does not exist
-    submitTaskAndWait(
-        taskSpec,
-        fullReindexDatasourceName,
-        false,
-        false,
-        dummyPair
-    );
-  }
-
-  @Test
-  public void testMERGEIndexData() throws Exception
-  {
-    final String reindexDatasource = MERGE_REINDEX_DATASOURCE + "-testMergeIndexData";
-    final String reindexDatasourceWithDruidInputSource = MERGE_REINDEX_DATASOURCE + "-testMergeReIndexData-druidInputSource";
-    try (
-        final Closeable ignored1 = unloader(MERGE_INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix());
-        final Closeable ignored2 = unloader(reindexDatasource + config.getExtraDatasourceNameSuffix());
-        final Closeable ignored3 = unloader(reindexDatasourceWithDruidInputSource + config.getExtraDatasourceNameSuffix())
-    ) {
-      doIndexTest(
-          MERGE_INDEX_DATASOURCE,
-          MERGE_INDEX_TASK,
-          MERGE_INDEX_QUERIES_RESOURCE,
-          false,
-          true,
-          true,
-          new Pair<>(false, false)
-      );
-      doReindexTest(
-          MERGE_INDEX_DATASOURCE,
-          reindexDatasource,
-          MERGE_REINDEX_TASK,
-          MERGE_REINDEX_QUERIES_RESOURCE,
-          new Pair<>(false, false)
-      );
-      doReindexTest(
-          MERGE_INDEX_DATASOURCE,
-          reindexDatasourceWithDruidInputSource,
-          MERGE_REINDEX_TASK_WITH_DRUID_INPUT_SOURCE,
-          MERGE_INDEX_QUERIES_RESOURCE,
-          new Pair<>(false, false)
-      );
-    }
-  }
-
-  /**
-   * Test that task reports indicate the ingested segments were loaded before the configured timeout expired.
-   *
-   * @throws Exception
-   */
-  @Test
-  public void testIndexDataAwaitSegmentAvailability() throws Exception
-  {
-    try (
-        final Closeable ignored1 = unloader(INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix());
-    ) {
-      final Function<String, String> transform = spec -> {
-        try {
-          return StringUtils.replace(
-              spec,
-              "%%SEGMENT_AVAIL_TIMEOUT_MILLIS%%",
-              jsonMapper.writeValueAsString("600000")
-          );
-        }
-        catch (JsonProcessingException e) {
-          throw new RuntimeException(e);
-        }
-      };
-
-      doIndexTest(
-          INDEX_DATASOURCE,
-          INDEX_TASK,
-          transform,
-          INDEX_QUERIES_RESOURCE,
-          false,
-          true,
-          true,
-          new Pair<>(true, true)
-      );
-    }
-  }
-
-  /**
-   * Test that the task still succeeds if the segments do not become available before the configured wait timeout
-   * expires.
-   *
-   * @throws Exception
-   */
-  @Test
-  public void testIndexDataAwaitSegmentAvailabilityFailsButTaskSucceeds() throws Exception
-  {
-    try (
-        final Closeable ignored1 = unloader(INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix());
-    ) {
-      coordinatorClient.postDynamicConfig(DYNAMIC_CONFIG_PAUSED);
-      final Function<String, String> transform = spec -> {
-        try {
-          return StringUtils.replace(
-              spec,
-              "%%SEGMENT_AVAIL_TIMEOUT_MILLIS%%",
-              jsonMapper.writeValueAsString("1")
-          );
-        }
-        catch (JsonProcessingException e) {
-          throw new RuntimeException(e);
-        }
-      };
-
-      doIndexTest(
-          INDEX_DATASOURCE,
-          INDEX_TASK,
-          transform,
-          INDEX_QUERIES_RESOURCE,
-          false,
-          false,
-          false,
-          new Pair<>(true, false)
-      );
-      coordinatorClient.postDynamicConfig(DYNAMIC_CONFIG_DEFAULT);
-      ITRetryUtil.retryUntilTrue(
-          () -> coordinator.areSegmentsLoaded(INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix()), "Segment Load"
-      );
-    }
-  }
-
-
-  @Test
-  public void testIndexWithMergeColumnLimitData() throws Exception
-  {
-    try (
-        final Closeable ignored1 = unloader(INDEX_WITH_MERGE_COLUMN_LIMIT_DATASOURCE + config.getExtraDatasourceNameSuffix());
-    ) {
-      doIndexTest(
-          INDEX_WITH_MERGE_COLUMN_LIMIT_DATASOURCE,
-          INDEX_WITH_MERGE_COLUMN_LIMIT_TASK,
-          INDEX_QUERIES_RESOURCE,
-          false,
-          true,
-          true,
-          new Pair<>(false, false)
-      );
-    }
-  }
-
-  @Test
-  public void testGetLockedIntervals() throws Exception
-  {
-    final String datasourceName = GET_LOCKED_INTERVALS + config.getExtraDatasourceNameSuffix();
-    try (final Closeable ignored = unloader(datasourceName)) {
-      // Submit an Indexing Task
-      submitIndexTask(INDEX_TASK, datasourceName);
-
-      // Wait until it acquires a lock
-      final Map<String, Integer> minTaskPriority = Collections.singletonMap(datasourceName, 0);
-      final Map<String, List<Interval>> lockedIntervals = new HashMap<>();
-      ITRetryUtil.retryUntilFalse(
-          () -> {
-            lockedIntervals.clear();
-            lockedIntervals.putAll(indexer.getLockedIntervals(minTaskPriority));
-            return lockedIntervals.isEmpty();
-          },
-          "Verify Intervals are Locked"
-      );
-
-      // Verify the locked intervals for this datasource
-      Assert.assertEquals(lockedIntervals.size(), 1);
-      Assert.assertEquals(
-          lockedIntervals.get(datasourceName),
-          Collections.singletonList(Intervals.of("2013-08-31/2013-09-02"))
-      );
-
-      ITRetryUtil.retryUntilTrue(
-          () -> coordinator.areSegmentsLoaded(datasourceName),
-          "Segment Load"
-      );
-    }
-  }
-
-  @Test
-  public void testJsonFunctions() throws Exception
-  {
-    final String taskSpec = getResourceAsString("/indexer/json_path_index_task.json");
-
-    submitTaskAndWait(
-        taskSpec,
-        "json_path_index_test",
-        false,
-        true,
-        new Pair<>(false, false)
-    );
-
-    doTestQuery("json_path_index_test", "/indexer/json_path_index_queries.json");
-  }
 }
diff --git a/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/indexer/IndexerTest.java b/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/indexer/IndexerTest.java
new file mode 100644
index 0000000000..50eeb511c1
--- /dev/null
+++ b/integration-tests-ex/cases/src/test/java/org/apache/druid/testsEx/indexer/IndexerTest.java
@@ -0,0 +1,384 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.testsEx.indexer;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.google.inject.Inject;
+import org.apache.druid.java.util.common.Intervals;
+import org.apache.druid.java.util.common.Pair;
+import org.apache.druid.java.util.common.StringUtils;
+import org.apache.druid.server.coordinator.CoordinatorDynamicConfig;
+import org.apache.druid.testing.clients.CoordinatorResourceTestClient;
+import org.apache.druid.testing.utils.ITRetryUtil;
+import org.apache.druid.testsEx.categories.BackwardCompatibility;
+import org.apache.druid.testsEx.config.DruidTestRunner;
+import org.joda.time.Interval;
+import org.junit.Assert;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.runner.RunWith;
+
+import java.io.Closeable;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Function;
+
+public class IndexerTest extends AbstractITBatchIndexTest
+{
+  private static final String INDEX_TASK = "/indexer/wikipedia_index_task.json";
+  private static final String INDEX_QUERIES_RESOURCE = "/indexer/wikipedia_index_queries.json";
+  private static final String INDEX_DATASOURCE = "wikipedia_index_test";
+
+  private static final String INDEX_WITH_TIMESTAMP_TASK = "/indexer/wikipedia_with_timestamp_index_task.json";
+  // TODO: add queries that validate timestamp is different from the __time column since it is a dimension
+  // TODO: https://github.com/apache/druid/issues/9565
+  private static final String INDEX_WITH_TIMESTAMP_QUERIES_RESOURCE = "/indexer/wikipedia_index_queries.json";
+  private static final String INDEX_WITH_TIMESTAMP_DATASOURCE = "wikipedia_with_timestamp_index_test";
+
+  private static final String REINDEX_TASK = "/indexer/wikipedia_reindex_task.json";
+  private static final String REINDEX_TASK_WITH_DRUID_INPUT_SOURCE = "/indexer/wikipedia_reindex_druid_input_source_task.json";
+  private static final String REINDEX_QUERIES_RESOURCE = "/indexer/wikipedia_reindex_queries.json";
+  private static final String REINDEX_DATASOURCE = "wikipedia_reindex_test";
+
+  private static final String MERGE_INDEX_TASK = "/indexer/wikipedia_merge_index_task.json";
+  private static final String MERGE_INDEX_QUERIES_RESOURCE = "/indexer/wikipedia_merge_index_queries.json";
+  private static final String MERGE_INDEX_DATASOURCE = "wikipedia_merge_index_test";
+
+  private static final String MERGE_REINDEX_TASK = "/indexer/wikipedia_merge_reindex_task.json";
+  private static final String MERGE_REINDEX_TASK_WITH_DRUID_INPUT_SOURCE = "/indexer/wikipedia_merge_reindex_druid_input_source_task.json";
+  private static final String MERGE_REINDEX_QUERIES_RESOURCE = "/indexer/wikipedia_merge_index_queries.json";
+  private static final String MERGE_REINDEX_DATASOURCE = "wikipedia_merge_reindex_test";
+
+  private static final String INDEX_WITH_MERGE_COLUMN_LIMIT_TASK = "/indexer/wikipedia_index_with_merge_column_limit_task.json";
+  private static final String INDEX_WITH_MERGE_COLUMN_LIMIT_DATASOURCE = "wikipedia_index_with_merge_column_limit_test";
+
+  private static final String GET_LOCKED_INTERVALS = "wikipedia_index_get_locked_intervals_test";
+
+  private static final CoordinatorDynamicConfig DYNAMIC_CONFIG_PAUSED =
+      CoordinatorDynamicConfig.builder().withPauseCoordination(true).build();
+  private static final CoordinatorDynamicConfig DYNAMIC_CONFIG_DEFAULT =
+      CoordinatorDynamicConfig.builder().build();
+
+  @Inject
+  CoordinatorResourceTestClient coordinatorClient;
+
+  @Test
+  public void testIndexData() throws Exception
+  {
+    final String reindexDatasource = REINDEX_DATASOURCE + "-testIndexData";
+    final String reindexDatasourceWithDruidInputSource = REINDEX_DATASOURCE + "-testIndexData-druidInputSource";
+    try (
+        final Closeable ignored1 = unloader(INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix());
+        final Closeable ignored2 = unloader(reindexDatasource + config.getExtraDatasourceNameSuffix());
+        final Closeable ignored3 = unloader(reindexDatasourceWithDruidInputSource + config.getExtraDatasourceNameSuffix())
+    ) {
+
+      final Function<String, String> transform = spec -> {
+        try {
+          return StringUtils.replace(
+              spec,
+              "%%SEGMENT_AVAIL_TIMEOUT_MILLIS%%",
+              jsonMapper.writeValueAsString("0")
+          );
+        }
+        catch (JsonProcessingException e) {
+          throw new RuntimeException(e);
+        }
+      };
+
+      doIndexTest(
+          INDEX_DATASOURCE,
+          INDEX_TASK,
+          transform,
+          INDEX_QUERIES_RESOURCE,
+          false,
+          true,
+          true,
+          new Pair<>(false, false)
+      );
+      doReindexTest(
+          INDEX_DATASOURCE,
+          reindexDatasource,
+          REINDEX_TASK,
+          REINDEX_QUERIES_RESOURCE,
+          new Pair<>(false, false)
+      );
+      doReindexTest(
+          INDEX_DATASOURCE,
+          reindexDatasourceWithDruidInputSource,
+          REINDEX_TASK_WITH_DRUID_INPUT_SOURCE,
+          REINDEX_QUERIES_RESOURCE,
+          new Pair<>(false, false)
+      );
+    }
+  }
+
+  @Test
+  public void testReIndexDataWithTimestamp() throws Exception
+  {
+    final String reindexDatasource = REINDEX_DATASOURCE + "-testReIndexDataWithTimestamp";
+    final String reindexDatasourceWithDruidInputSource = REINDEX_DATASOURCE + "-testReIndexDataWithTimestamp-druidInputSource";
+    try (
+        final Closeable ignored1 = unloader(INDEX_WITH_TIMESTAMP_DATASOURCE + config.getExtraDatasourceNameSuffix());
+        final Closeable ignored2 = unloader(reindexDatasource + config.getExtraDatasourceNameSuffix());
+        final Closeable ignored3 = unloader(reindexDatasourceWithDruidInputSource + config.getExtraDatasourceNameSuffix())
+    ) {
+      doIndexTest(
+          INDEX_WITH_TIMESTAMP_DATASOURCE,
+          INDEX_WITH_TIMESTAMP_TASK,
+          INDEX_WITH_TIMESTAMP_QUERIES_RESOURCE,
+          false,
+          true,
+          true,
+          new Pair<>(false, false)
+      );
+      doReindexTest(
+          INDEX_WITH_TIMESTAMP_DATASOURCE,
+          reindexDatasource,
+          REINDEX_TASK,
+          REINDEX_QUERIES_RESOURCE,
+          new Pair<>(false, false)
+      );
+      doReindexTest(
+          INDEX_WITH_TIMESTAMP_DATASOURCE,
+          reindexDatasourceWithDruidInputSource,
+          REINDEX_TASK_WITH_DRUID_INPUT_SOURCE,
+          REINDEX_QUERIES_RESOURCE,
+          new Pair<>(false, false)
+      );
+    }
+  }
+
+  @Test
+  public void testReIndexWithNonExistingDatasource() throws Exception
+  {
+    Pair<Boolean, Boolean> dummyPair = new Pair<>(false, false);
+    final String fullBaseDatasourceName = "nonExistingDatasource2904";
+    final String fullReindexDatasourceName = "newDatasource123";
+
+    String taskSpec = StringUtils.replace(
+        getResourceAsString(REINDEX_TASK_WITH_DRUID_INPUT_SOURCE),
+        "%%DATASOURCE%%",
+        fullBaseDatasourceName
+    );
+    taskSpec = StringUtils.replace(
+        taskSpec,
+        "%%REINDEX_DATASOURCE%%",
+        fullReindexDatasourceName
+    );
+
+    // This method will also verify task is successful after task finish running
+    // We expect task to be successful even if the datasource to reindex does not exist
+    submitTaskAndWait(
+        taskSpec,
+        fullReindexDatasourceName,
+        false,
+        false,
+        dummyPair
+    );
+  }
+
+  @Test
+  public void testMERGEIndexData() throws Exception
+  {
+    final String reindexDatasource = MERGE_REINDEX_DATASOURCE + "-testMergeIndexData";
+    final String reindexDatasourceWithDruidInputSource = MERGE_REINDEX_DATASOURCE + "-testMergeReIndexData-druidInputSource";
+    try (
+        final Closeable ignored1 = unloader(MERGE_INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix());
+        final Closeable ignored2 = unloader(reindexDatasource + config.getExtraDatasourceNameSuffix());
+        final Closeable ignored3 = unloader(reindexDatasourceWithDruidInputSource + config.getExtraDatasourceNameSuffix())
+    ) {
+      doIndexTest(
+          MERGE_INDEX_DATASOURCE,
+          MERGE_INDEX_TASK,
+          MERGE_INDEX_QUERIES_RESOURCE,
+          false,
+          true,
+          true,
+          new Pair<>(false, false)
+      );
+      doReindexTest(
+          MERGE_INDEX_DATASOURCE,
+          reindexDatasource,
+          MERGE_REINDEX_TASK,
+          MERGE_REINDEX_QUERIES_RESOURCE,
+          new Pair<>(false, false)
+      );
+      doReindexTest(
+          MERGE_INDEX_DATASOURCE,
+          reindexDatasourceWithDruidInputSource,
+          MERGE_REINDEX_TASK_WITH_DRUID_INPUT_SOURCE,
+          MERGE_INDEX_QUERIES_RESOURCE,
+          new Pair<>(false, false)
+      );
+    }
+  }
+
+  /**
+   * Test that task reports indicate the ingested segments were loaded before the configured timeout expired.
+   *
+   * @throws Exception
+   */
+  @Test
+  public void testIndexDataAwaitSegmentAvailability() throws Exception
+  {
+    try (
+        final Closeable ignored1 = unloader(INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix());
+    ) {
+      final Function<String, String> transform = spec -> {
+        try {
+          return StringUtils.replace(
+              spec,
+              "%%SEGMENT_AVAIL_TIMEOUT_MILLIS%%",
+              jsonMapper.writeValueAsString("600000")
+          );
+        }
+        catch (JsonProcessingException e) {
+          throw new RuntimeException(e);
+        }
+      };
+
+      doIndexTest(
+          INDEX_DATASOURCE,
+          INDEX_TASK,
+          transform,
+          INDEX_QUERIES_RESOURCE,
+          false,
+          true,
+          true,
+          new Pair<>(true, true)
+      );
+    }
+  }
+
+  /**
+   * Test that the task still succeeds if the segments do not become available before the configured wait timeout
+   * expires.
+   *
+   * @throws Exception
+   */
+  @Test
+  public void testIndexDataAwaitSegmentAvailabilityFailsButTaskSucceeds() throws Exception
+  {
+    try (
+        final Closeable ignored1 = unloader(INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix());
+    ) {
+      coordinatorClient.postDynamicConfig(DYNAMIC_CONFIG_PAUSED);
+      final Function<String, String> transform = spec -> {
+        try {
+          return StringUtils.replace(
+              spec,
+              "%%SEGMENT_AVAIL_TIMEOUT_MILLIS%%",
+              jsonMapper.writeValueAsString("1")
+          );
+        }
+        catch (JsonProcessingException e) {
+          throw new RuntimeException(e);
+        }
+      };
+
+      doIndexTest(
+          INDEX_DATASOURCE,
+          INDEX_TASK,
+          transform,
+          INDEX_QUERIES_RESOURCE,
+          false,
+          false,
+          false,
+          new Pair<>(true, false)
+      );
+      coordinatorClient.postDynamicConfig(DYNAMIC_CONFIG_DEFAULT);
+      ITRetryUtil.retryUntilTrue(
+          () -> coordinator.areSegmentsLoaded(INDEX_DATASOURCE + config.getExtraDatasourceNameSuffix()), "Segment Load"
+      );
+    }
+  }
+
+
+  @Test
+  public void testIndexWithMergeColumnLimitData() throws Exception
+  {
+    try (
+        final Closeable ignored1 = unloader(INDEX_WITH_MERGE_COLUMN_LIMIT_DATASOURCE + config.getExtraDatasourceNameSuffix());
+    ) {
+      doIndexTest(
+          INDEX_WITH_MERGE_COLUMN_LIMIT_DATASOURCE,
+          INDEX_WITH_MERGE_COLUMN_LIMIT_TASK,
+          INDEX_QUERIES_RESOURCE,
+          false,
+          true,
+          true,
+          new Pair<>(false, false)
+      );
+    }
+  }
+
+  @Test
+  public void testGetLockedIntervals() throws Exception
+  {
+    final String datasourceName = GET_LOCKED_INTERVALS + config.getExtraDatasourceNameSuffix();
+    try (final Closeable ignored = unloader(datasourceName)) {
+      // Submit an Indexing Task
+      submitIndexTask(INDEX_TASK, datasourceName);
+
+      // Wait until it acquires a lock
+      final Map<String, Integer> minTaskPriority = Collections.singletonMap(datasourceName, 0);
+      final Map<String, List<Interval>> lockedIntervals = new HashMap<>();
+      ITRetryUtil.retryUntilFalse(
+          () -> {
+            lockedIntervals.clear();
+            lockedIntervals.putAll(indexer.getLockedIntervals(minTaskPriority));
+            return lockedIntervals.isEmpty();
+          },
+          "Verify Intervals are Locked"
+      );
+
+      // Verify the locked intervals for this datasource
+      Assert.assertEquals(lockedIntervals.size(), 1);
+      Assert.assertEquals(
+          lockedIntervals.get(datasourceName),
+          Collections.singletonList(Intervals.of("2013-08-31/2013-09-02"))
+      );
+
+      ITRetryUtil.retryUntilTrue(
+          () -> coordinator.areSegmentsLoaded(datasourceName),
+          "Segment Load"
+      );
+    }
+  }
+
+  @Test
+  public void testJsonFunctions() throws Exception
+  {
+    final String taskSpec = getResourceAsString("/indexer/json_path_index_task.json");
+
+    submitTaskAndWait(
+        taskSpec,
+        "json_path_index_test",
+        false,
+        true,
+        new Pair<>(false, false)
+    );
+
+    doTestQuery("json_path_index_test", "/indexer/json_path_index_queries.json");
+  }
+}
diff --git a/integration-tests-ex/cases/src/test/resources/cluster/BackwardCompatibility/docker.yaml b/integration-tests-ex/cases/src/test/resources/cluster/BackwardCompatibility/docker.yaml
new file mode 100644
index 0000000000..d676f530e9
--- /dev/null
+++ b/integration-tests-ex/cases/src/test/resources/cluster/BackwardCompatibility/docker.yaml
@@ -0,0 +1,40 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#-------------------------------------------------------------------------
+
+# Definition of the batch index test cluster.
+# See https://yaml.org/spec/1.2.2 for more about YAML
+include:
+  - /cluster/Common/zk-metastore.yaml
+
+druid:
+  coordinator:
+    instances:
+      - port: 8081
+  overlord:
+    instances:
+      - port: 8090
+  broker:
+    instances:
+      - port: 8082
+  router:
+    instances:
+      - port: 8888
+  historical:
+    instances:
+      - port: 8083
+  indexer:
+    instances:
+      - port: 8091
diff --git a/integration-tests-ex/image/build-image.sh b/integration-tests-ex/image/build-image.sh
index 4a31a78419..2a63796318 100755
--- a/integration-tests-ex/image/build-image.sh
+++ b/integration-tests-ex/image/build-image.sh
@@ -43,6 +43,9 @@ export CONFLUENT_VERSION=$CONFLUENT_VERSION
 export MARIADB_VERSION=$MARIADB_VERSION
 export HADOOP_VERSION=$HADOOP_VERSION
 export DRUID_IT_IMAGE_NAME=$DRUID_IT_IMAGE_NAME
+export DRUID_PREV_VERSION=$DRUID_PREV_VERSION
+export DRUID_PREVIOUS_VERSION_DOWNLOAD_URL=$DRUID_PREVIOUS_VERSION_DOWNLOAD_URL
+export DRUID_PREVIOUS_IT_IMAGE_NAME=$DRUID_PREVIOUS_IT_IMAGE_NAME
 EOF
 
 exec bash $SCRIPT_DIR/docker-build.sh
diff --git a/integration-tests-ex/image/docker-build.sh b/integration-tests-ex/image/docker-build.sh
index 6a945aa612..62b017a554 100755
--- a/integration-tests-ex/image/docker-build.sh
+++ b/integration-tests-ex/image/docker-build.sh
@@ -53,4 +53,24 @@ docker build -t $DRUID_IT_IMAGE_NAME \
 	--build-arg CONFLUENT_VERSION=$CONFLUENT_VERSION \
 	--build-arg HADOOP_VERSION=$HADOOP_VERSION \
 	--build-arg MYSQL_DRIVER_CLASSNAME=$MYSQL_DRIVER_CLASSNAME \
+	--build-arg DRUID_TESTING_TOOLS_VERSION=$DRUID_VERSION \
+	.
+
+if [ $BACKWARD_COMPATIBILITY_IT_ENABLED != "true" ]; then
+  exit 1
+fi
+
+# Download the previous druid tar
+curl -L $DRUID_PREVIOUS_VERSION_DOWNLOAD_URL --output apache-druid-$DRUID_PREV_VERSION-bin.tar.gz
+
+echo "Downloaded previous version tar"
+
+docker build -t $DRUID_PREVIOUS_IT_IMAGE_NAME \
+	--build-arg DRUID_VERSION=$DRUID_PREV_VERSION \
+	--build-arg MYSQL_VERSION=$MYSQL_VERSION \
+	--build-arg MARIADB_VERSION=$MARIADB_VERSION \
+	--build-arg CONFLUENT_VERSION=$CONFLUENT_VERSION \
+	--build-arg HADOOP_VERSION=$HADOOP_VERSION \
+	--build-arg MYSQL_DRIVER_CLASSNAME=$MYSQL_DRIVER_CLASSNAME \
+	--build-arg DRUID_TESTING_TOOLS_VERSION=$DRUID_VERSION \
 	.
diff --git a/integration-tests-ex/image/docker/Dockerfile b/integration-tests-ex/image/docker/Dockerfile
index a77a5c2d02..99b2da9476 100644
--- a/integration-tests-ex/image/docker/Dockerfile
+++ b/integration-tests-ex/image/docker/Dockerfile
@@ -46,13 +46,15 @@ ARG MARIADB_VERSION
 ENV MARIADB_VERSION=$MARIADB_VERSION
 ARG MYSQL_DRIVER_CLASSNAME=com.mysql.jdbc.Driver
 ENV MYSQL_DRIVER_CLASSNAME=$MYSQL_DRIVER_CLASSNAME
+ARG DRUID_TESTING_TOOLS_VERSION
+ENV DRUID_TESTING_TOOLS_VERSION=$DRUID_TESTING_TOOLS_VERSION
 
 ENV DRUID_HOME=/usr/local/druid
 
 # Populate build artifacts
 
 COPY apache-druid-${DRUID_VERSION}-bin.tar.gz /usr/local/
-COPY druid-it-tools-${DRUID_VERSION}.jar /tmp/druid/extensions/druid-it-tools/
+COPY druid-it-tools-${DRUID_TESTING_TOOLS_VERSION}.jar /tmp/druid/extensions/druid-it-tools/
 COPY kafka-protobuf-provider-${CONFLUENT_VERSION}.jar /tmp/druid/lib/
 COPY mysql-connector-j-${MYSQL_VERSION}.jar /tmp/druid/lib/
 COPY mariadb-java-client-${MARIADB_VERSION}.jar /tmp/druid/lib/
@@ -71,3 +73,4 @@ RUN bash /test-setup.sh
 USER druid:druid
 WORKDIR /
 ENTRYPOINT [ "bash", "/launch.sh" ]
+
diff --git a/integration-tests-ex/image/pom.xml b/integration-tests-ex/image/pom.xml
index 600501edb9..7cac87dfd5 100644
--- a/integration-tests-ex/image/pom.xml
+++ b/integration-tests-ex/image/pom.xml
@@ -55,6 +55,10 @@ Reference: https://dzone.com/articles/build-docker-image-from-maven
              image name will typically be provided by the build enfironment, and will
              override this default name. -->
         <druid.it.image-name>${project.groupId}/test:${project.version}</druid.it.image-name>
+        <druid.it.prev.version>30.0.0</druid.it.prev.version>
+        <druid.it.prev.version.download.url>https://dlcdn.apache.org/druid/30.0.0/apache-druid-30.0.0-bin.tar.gz</druid.it.prev.version.download.url>
+        <druid.it.prev.image-name>${project.groupId}/test:${druid.it.prev.version}</druid.it.prev.image-name>
+        <druid.it.backward-compatibility.enabled>true</druid.it.backward-compatibility.enabled>
         <confluent-version>5.5.1</confluent-version>
         <mariadb.version>2.7.3</mariadb.version>
         <mysql.image.version>5.7-debian</mysql.image.version>
@@ -209,6 +213,10 @@ Reference: https://dzone.com/articles/build-docker-image-from-maven
                                         <HADOOP_VERSION>${hadoop.compile.version}</HADOOP_VERSION>
                                         <DRUID_VERSION>${project.version}</DRUID_VERSION>
                                         <DRUID_IT_IMAGE_NAME>${druid.it.image-name}</DRUID_IT_IMAGE_NAME>
+                                        <DRUID_PREV_VERSION>${druid.it.prev.version}</DRUID_PREV_VERSION>
+                                        <DRUID_PREVIOUS_VERSION_DOWNLOAD_URL>${druid.it.prev.version.download.url}</DRUID_PREVIOUS_VERSION_DOWNLOAD_URL>
+                                        <DRUID_PREVIOUS_IT_IMAGE_NAME>${druid.it.prev.image-name}</DRUID_PREVIOUS_IT_IMAGE_NAME>
+                                        <BACKWARD_COMPATIBILITY_IT_ENABLED>${druid.it.backward-compatibility.enabled}</BACKWARD_COMPATIBILITY_IT_ENABLED>
                                         <TARGET_DIR>${project.build.directory}</TARGET_DIR>
                                         <!-- Maven has no good way to get the root directory, so
                                              this is as close as we can get. -->
