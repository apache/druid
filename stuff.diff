diff --git a/benchmarks/src/test/java/org/apache/druid/server/coordinator/NewestSegmentFirstPolicyBenchmark.java b/benchmarks/src/test/java/org/apache/druid/server/coordinator/NewestSegmentFirstPolicyBenchmark.java
index a0a1da9369..c9c4599fad 100644
--- a/benchmarks/src/test/java/org/apache/druid/server/coordinator/NewestSegmentFirstPolicyBenchmark.java
+++ b/benchmarks/src/test/java/org/apache/druid/server/coordinator/NewestSegmentFirstPolicyBenchmark.java
@@ -21,11 +21,9 @@ package org.apache.druid.server.coordinator;
 
 import com.google.common.collect.ImmutableList;
 import org.apache.druid.client.DataSourcesSnapshot;
-import org.apache.druid.jackson.DefaultObjectMapper;
 import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.server.compaction.CompactionCandidateSearchPolicy;
 import org.apache.druid.server.compaction.CompactionSegmentIterator;
-import org.apache.druid.server.compaction.CompactionStatusTracker;
 import org.apache.druid.server.compaction.NewestSegmentFirstPolicy;
 import org.apache.druid.server.compaction.PriorityBasedCompactionSegmentIterator;
 import org.apache.druid.timeline.DataSegment;
@@ -137,8 +135,7 @@ public class NewestSegmentFirstPolicyBenchmark
         policy,
         compactionConfigs,
         dataSources,
-        Collections.emptyMap(),
-        new CompactionStatusTracker(new DefaultObjectMapper())
+        Collections.emptyMap()
     );
     for (int i = 0; i < numCompactionTaskSlots && iterator.hasNext(); i++) {
       blackhole.consume(iterator.next());
diff --git a/embedded-tests/src/test/java/org/apache/druid/testing/embedded/compact/CompactionSupervisorTest.java b/embedded-tests/src/test/java/org/apache/druid/testing/embedded/compact/CompactionSupervisorTest.java
new file mode 100644
index 0000000000..35ba77274f
--- /dev/null
+++ b/embedded-tests/src/test/java/org/apache/druid/testing/embedded/compact/CompactionSupervisorTest.java
@@ -0,0 +1,345 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.testing.embedded.compact;
+
+import org.apache.druid.catalog.guice.CatalogClientModule;
+import org.apache.druid.catalog.guice.CatalogCoordinatorModule;
+import org.apache.druid.common.utils.IdUtils;
+import org.apache.druid.indexing.common.task.IndexTask;
+import org.apache.druid.indexing.compact.CascadingCompactionTemplate;
+import org.apache.druid.indexing.compact.CompactionJobTemplate;
+import org.apache.druid.indexing.compact.CompactionRule;
+import org.apache.druid.indexing.compact.CompactionStateMatcher;
+import org.apache.druid.indexing.compact.CompactionSupervisorSpec;
+import org.apache.druid.indexing.compact.InlineCompactionJobTemplate;
+import org.apache.druid.indexing.compact.MSQCompactionJobTemplate;
+import org.apache.druid.indexing.overlord.Segments;
+import org.apache.druid.java.util.common.DateTimes;
+import org.apache.druid.java.util.common.granularity.Granularities;
+import org.apache.druid.java.util.common.granularity.Granularity;
+import org.apache.druid.msq.guice.IndexerMemoryManagementModule;
+import org.apache.druid.msq.guice.MSQDurableStorageModule;
+import org.apache.druid.msq.guice.MSQIndexingModule;
+import org.apache.druid.msq.guice.MSQSqlModule;
+import org.apache.druid.msq.guice.SqlTaskModule;
+import org.apache.druid.query.DruidMetrics;
+import org.apache.druid.query.http.ClientSqlQuery;
+import org.apache.druid.rpc.UpdateResponse;
+import org.apache.druid.server.coordinator.ClusterCompactionConfig;
+import org.apache.druid.server.coordinator.DataSourceCompactionConfig;
+import org.apache.druid.server.coordinator.InlineSchemaDataSourceCompactionConfig;
+import org.apache.druid.server.coordinator.UserCompactionTaskGranularityConfig;
+import org.apache.druid.testing.embedded.EmbeddedBroker;
+import org.apache.druid.testing.embedded.EmbeddedCoordinator;
+import org.apache.druid.testing.embedded.EmbeddedDruidCluster;
+import org.apache.druid.testing.embedded.EmbeddedHistorical;
+import org.apache.druid.testing.embedded.EmbeddedIndexer;
+import org.apache.druid.testing.embedded.EmbeddedOverlord;
+import org.apache.druid.testing.embedded.EmbeddedRouter;
+import org.apache.druid.testing.embedded.indexing.MoreResources;
+import org.apache.druid.testing.embedded.junit5.EmbeddedClusterTestBase;
+import org.hamcrest.Matcher;
+import org.hamcrest.Matchers;
+import org.joda.time.DateTime;
+import org.joda.time.Period;
+import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.BeforeAll;
+import org.junit.jupiter.api.Test;
+
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Embedded test that runs compaction supervisors of various types.
+ */
+public class CompactionSupervisorTest extends EmbeddedClusterTestBase
+{
+  private final EmbeddedBroker broker = new EmbeddedBroker();
+  private final EmbeddedIndexer indexer = new EmbeddedIndexer()
+      .setServerMemory(2_000_000_000L)
+      .addProperty("druid.worker.capacity", "20");
+  private final EmbeddedOverlord overlord = new EmbeddedOverlord()
+      .addProperty("druid.manager.segments.pollDuration", "PT1s")
+      .addProperty("druid.manager.segments.useIncrementalCache", "always");
+  private final EmbeddedHistorical historical = new EmbeddedHistorical();
+  private final EmbeddedCoordinator coordinator = new EmbeddedCoordinator()
+      .addProperty("druid.manager.segments.useIncrementalCache", "always");
+
+  @Override
+  public EmbeddedDruidCluster createCluster()
+  {
+    return EmbeddedDruidCluster.withEmbeddedDerbyAndZookeeper()
+                               .useLatchableEmitter()
+                               .useDefaultTimeoutForLatchableEmitter(600)
+                               .addExtensions(
+                                   CatalogClientModule.class,
+                                   CatalogCoordinatorModule.class,
+                                   IndexerMemoryManagementModule.class,
+                                   MSQDurableStorageModule.class,
+                                   MSQIndexingModule.class,
+                                   MSQSqlModule.class,
+                                   SqlTaskModule.class
+                               )
+                               .addServer(coordinator)
+                               .addServer(overlord)
+                               .addServer(indexer)
+                               .addServer(historical)
+                               .addServer(broker)
+                               .addServer(new EmbeddedRouter());
+  }
+
+  @BeforeAll
+  public void enableCompactionSupervisors()
+  {
+    final UpdateResponse updateResponse = cluster.callApi().onLeaderOverlord(
+        o -> o.updateClusterCompactionConfig(new ClusterCompactionConfig(1.0, 100, null, true, null))
+    );
+    Assertions.assertTrue(updateResponse.isSuccess());
+  }
+
+  @Test
+  public void test_ingestDayGranularity_andCompactToMonthGranularity_withInlineConfig()
+  {
+    // Ingest data at DAY granularity and verify
+    runIngestionAtGranularity(
+        "DAY",
+        "2025-06-01T00:00:00.000Z,shirt,105"
+        + "\n2025-06-02T00:00:00.000Z,trousers,210"
+        + "\n2025-06-03T00:00:00.000Z,jeans,150"
+    );
+    Assertions.assertEquals(3, getNumSegmentsWith(Granularities.DAY));
+
+    // Create a compaction config with MONTH granularity
+    InlineSchemaDataSourceCompactionConfig compactionConfig =
+        InlineSchemaDataSourceCompactionConfig
+            .builder()
+            .forDataSource(dataSource)
+            .withSkipOffsetFromLatest(Period.seconds(0))
+            .withGranularitySpec(
+                new UserCompactionTaskGranularityConfig(Granularities.MONTH, null, null)
+            )
+            .build();
+
+    runCompactionWithSpec(compactionConfig);
+    waitForAllCompactionTasksToFinish();
+
+    Assertions.assertEquals(0, getNumSegmentsWith(Granularities.DAY));
+    Assertions.assertEquals(1, getNumSegmentsWith(Granularities.MONTH));
+  }
+
+  @Test
+  public void test_ingestHourGranularity_andCompactToDayAndMonth_withInlineTemplates()
+  {
+    // Create a cascading template with DAY and MONTH granularity
+    CascadingCompactionTemplate cascadingTemplate = new CascadingCompactionTemplate(
+        dataSource,
+        List.of(
+            new CompactionRule(Period.days(1), new InlineCompactionJobTemplate(createMatcher(Granularities.DAY))),
+            new CompactionRule(Period.days(50), new InlineCompactionJobTemplate(createMatcher(Granularities.MONTH)))
+        )
+    );
+
+    ingestHourSegments(1000);
+    runCompactionWithSpec(cascadingTemplate);
+    waitForAllCompactionTasksToFinish();
+
+    Assertions.assertEquals(0, getNumSegmentsWith(Granularities.HOUR));
+    Assertions.assertTrue(getNumSegmentsWith(Granularities.DAY) >= 1);
+    Assertions.assertTrue(getNumSegmentsWith(Granularities.MONTH) >= 1);
+  }
+
+  @Test
+  public void test_ingestHourGranularity_andCompactToDayAndMonth_withMSQTemplates()
+  {
+    ingestHourSegments(1200);
+
+    // Add compaction templates to catalog
+    final String sqlDayGranularity =
+        "REPLACE INTO ${dataSource}"
+        + " OVERWRITE WHERE __time >= TIMESTAMP '${startTimestamp}' AND __time < TIMESTAMP '${endTimestamp}'"
+        + " SELECT * FROM ${dataSource}"
+        + " WHERE __time BETWEEN '${startTimestamp}' AND '${endTimestamp}'"
+        + " PARTITIONED BY DAY";
+    final CompactionJobTemplate dayGranularityTemplate = new MSQCompactionJobTemplate(
+        new ClientSqlQuery(sqlDayGranularity, null, false, false, false, null, null),
+        createMatcher(Granularities.DAY)
+    );
+    final String sqlMonthGranularity =
+        "REPLACE INTO ${dataSource}"
+        + " OVERWRITE WHERE __time >= TIMESTAMP '${startTimestamp}' AND __time < TIMESTAMP '${endTimestamp}'"
+        + " SELECT * FROM ${dataSource}"
+        + " WHERE __time >= TIMESTAMP '${startTimestamp}' AND __time < TIMESTAMP '${endTimestamp}'"
+        + " PARTITIONED BY MONTH";
+    final CompactionJobTemplate monthGranularityTemplate = new MSQCompactionJobTemplate(
+        new ClientSqlQuery(sqlMonthGranularity, null, false, false, false, null, null),
+        createMatcher(Granularities.MONTH)
+    );
+
+    // Create a cascading template with DAY and MONTH granularity
+    CascadingCompactionTemplate cascadingTemplate = new CascadingCompactionTemplate(
+        dataSource,
+        List.of(
+            new CompactionRule(Period.days(1), dayGranularityTemplate),
+            new CompactionRule(Period.days(50), monthGranularityTemplate)
+        )
+    );
+
+    runCompactionWithSpec(cascadingTemplate);
+    waitForAllCompactionTasksToFinish();
+
+    Assertions.assertEquals(0, getNumSegmentsWith(Granularities.HOUR));
+    Assertions.assertTrue(getNumSegmentsWith(Granularities.DAY) >= 1);
+    Assertions.assertTrue(getNumSegmentsWith(Granularities.MONTH) >= 1);
+  }
+
+  @Test
+  public void test_ingestHourGranularity_andCompactToDayAndMonth_withMixedTemplates()
+  {
+    ingestHourSegments(1200);
+
+    // Add compaction templates to catalog
+    final String sqlDayGranularity =
+        "REPLACE INTO ${dataSource}"
+        + " OVERWRITE WHERE __time >= TIMESTAMP '${startTimestamp}' AND __time < TIMESTAMP '${endTimestamp}'"
+        + " SELECT * FROM ${dataSource}"
+        + " WHERE __time BETWEEN '${startTimestamp}' AND '${endTimestamp}'"
+        + " PARTITIONED BY DAY";
+    final MSQCompactionJobTemplate dayTemplate = new MSQCompactionJobTemplate(
+        new ClientSqlQuery(sqlDayGranularity, null, false, false, false, null, null),
+        createMatcher(Granularities.DAY)
+    );
+    final CompactionJobTemplate weekTemplate =
+        new InlineCompactionJobTemplate(createMatcher(Granularities.WEEK));
+    final InlineCompactionJobTemplate monthTemplate =
+        new InlineCompactionJobTemplate(createMatcher(Granularities.MONTH));
+
+    // Compact last 1 day to DAY, next 14 days to WEEK, then 1 more DAY, rest to MONTH
+    CascadingCompactionTemplate cascadingTemplate = new CascadingCompactionTemplate(
+        dataSource,
+        List.of(
+            new CompactionRule(Period.days(1), dayTemplate),
+            new CompactionRule(Period.days(15), weekTemplate),
+            new CompactionRule(Period.days(16), dayTemplate),
+            new CompactionRule(Period.ZERO, monthTemplate)
+        )
+    );
+
+    runCompactionWithSpec(cascadingTemplate);
+    waitForAllCompactionTasksToFinish();
+
+    Assertions.assertEquals(0, getNumSegmentsWith(Granularities.HOUR));
+    Assertions.assertTrue(getNumSegmentsWith(Granularities.DAY) >= 1);
+    Assertions.assertTrue(getNumSegmentsWith(Granularities.WEEK) >= 1);
+    Assertions.assertTrue(getNumSegmentsWith(Granularities.MONTH) >= 1);
+  }
+
+  private void ingestHourSegments(int numSegments)
+  {
+    runIngestionAtGranularity(
+        "HOUR",
+        createHourlyInlineDataCsv(DateTimes.nowUtc(), numSegments)
+    );
+  }
+
+  private void runCompactionWithSpec(DataSourceCompactionConfig config)
+  {
+    final CompactionSupervisorSpec compactionSupervisor
+        = new CompactionSupervisorSpec(config, false, null);
+    cluster.callApi().postSupervisor(compactionSupervisor);
+  }
+
+  private void waitForAllCompactionTasksToFinish()
+  {
+    // Wait for all intervals to be compacted
+    overlord.latchableEmitter().waitForEvent(
+        event -> event.hasMetricName("interval/waitCompact/count")
+                      .hasDimension(DruidMetrics.DATASOURCE, dataSource)
+                      .hasValueMatching(Matchers.equalTo(0L))
+    );
+
+    // Wait for all submitted compaction jobs to finish
+    int numSubmittedTasks = overlord.latchableEmitter().getMetricValues(
+        "compact/task/count",
+        Map.of(DruidMetrics.DATASOURCE, dataSource)
+    ).stream().mapToInt(Number::intValue).sum();
+
+    final Matcher<Object> taskTypeMatcher = Matchers.anyOf(
+        Matchers.equalTo("query_controller"),
+        Matchers.equalTo("compact")
+    );
+    overlord.latchableEmitter().waitForEventAggregate(
+        event -> event.hasMetricName("task/run/time")
+                      .hasDimensionMatching(DruidMetrics.TASK_TYPE, taskTypeMatcher)
+                      .hasDimension(DruidMetrics.DATASOURCE, dataSource),
+        agg -> agg.hasCountAtLeast(numSubmittedTasks)
+    );
+  }
+
+  private int getNumSegmentsWith(Granularity granularity)
+  {
+    return (int) overlord
+        .bindings()
+        .segmentsMetadataStorage()
+        .retrieveAllUsedSegments(dataSource, Segments.ONLY_VISIBLE)
+        .stream()
+        .filter(segment -> granularity.isAligned(segment.getInterval()))
+        .count();
+  }
+
+  private void runIngestionAtGranularity(
+      String granularity,
+      String inlineDataCsv
+  )
+  {
+    final IndexTask task = MoreResources.Task.BASIC_INDEX
+        .get()
+        .segmentGranularity(granularity)
+        .inlineInputSourceWithData(inlineDataCsv)
+        .dataSource(dataSource)
+        .withId(IdUtils.getRandomId());
+    cluster.callApi().runTask(task, overlord);
+  }
+
+  private String createHourlyInlineDataCsv(DateTime latestRecordTimestamp, int numRecords)
+  {
+    final StringBuilder builder = new StringBuilder();
+    for (int i = 0; i < numRecords; ++i) {
+      builder.append(latestRecordTimestamp.minusHours(i))
+             .append(",").append("item_").append(IdUtils.getRandomId())
+             .append(",").append(0)
+             .append("\n");
+    }
+
+    return builder.toString();
+  }
+
+  private static CompactionStateMatcher createMatcher(Granularity segmentGranularity)
+  {
+    return new CompactionStateMatcher(
+        null,
+        null,
+        null,
+        null,
+        null,
+        new UserCompactionTaskGranularityConfig(segmentGranularity, null, null),
+        null
+    );
+  }
+}
diff --git a/embedded-tests/src/test/java/org/apache/druid/testing/embedded/indexing/KafkaClusterMetricsTest.java b/embedded-tests/src/test/java/org/apache/druid/testing/embedded/indexing/KafkaClusterMetricsTest.java
index 44c9be9a9c..11e25120aa 100644
--- a/embedded-tests/src/test/java/org/apache/druid/testing/embedded/indexing/KafkaClusterMetricsTest.java
+++ b/embedded-tests/src/test/java/org/apache/druid/testing/embedded/indexing/KafkaClusterMetricsTest.java
@@ -237,7 +237,7 @@ public class KafkaClusterMetricsTest extends EmbeddedClusterTestBase
         event -> event.hasMetricName("task/run/time")
                       .hasDimension(DruidMetrics.TASK_TYPE, "compact")
                       .hasDimension(DruidMetrics.TASK_STATUS, "SUCCESS"),
-        agg -> agg.hasCountAtLeast(2)
+        agg -> agg.hasCountAtLeast(10)
     );
 
     // Verify that some segments have been upgraded due to Concurrent Append and Replace
diff --git a/embedded-tests/src/test/java/org/apache/druid/testing/embedded/server/HighAvailabilityTest.java b/embedded-tests/src/test/java/org/apache/druid/testing/embedded/server/HighAvailabilityTest.java
index 303ad16afa..2a0f9d3cbe 100644
--- a/embedded-tests/src/test/java/org/apache/druid/testing/embedded/server/HighAvailabilityTest.java
+++ b/embedded-tests/src/test/java/org/apache/druid/testing/embedded/server/HighAvailabilityTest.java
@@ -43,6 +43,7 @@ import org.apache.druid.testing.embedded.EmbeddedOverlord;
 import org.apache.druid.testing.embedded.EmbeddedRouter;
 import org.apache.druid.testing.embedded.indexing.Resources;
 import org.apache.druid.testing.embedded.junit5.EmbeddedClusterTestBase;
+import org.hamcrest.Matchers;
 import org.jboss.netty.handler.codec.http.HttpMethod;
 import org.jboss.netty.handler.codec.http.HttpResponseStatus;
 import org.junit.jupiter.api.Assertions;
@@ -128,7 +129,7 @@ public class HighAvailabilityTest extends EmbeddedClusterTestBase
     coordinator1.latchableEmitter().waitForEvent(
         event -> event.hasMetricName("segment/metadataCache/used/count")
                       .hasDimension(DruidMetrics.DATASOURCE, dataSource)
-                      .hasValueAtLeast(10)
+                      .hasValueMatching(Matchers.greaterThanOrEqualTo(10L))
     );
 
     // Run sys queries, switch leaders, repeat
diff --git a/extensions-core/druid-catalog/pom.xml b/extensions-core/druid-catalog/pom.xml
index 3227fe6216..2c36306634 100644
--- a/extensions-core/druid-catalog/pom.xml
+++ b/extensions-core/druid-catalog/pom.xml
@@ -193,6 +193,11 @@
             <artifactId>easymock</artifactId>
             <scope>test</scope>
         </dependency>
+        <dependency>
+            <groupId>org.hamcrest</groupId>
+            <artifactId>hamcrest-all</artifactId>
+            <scope>test</scope>
+        </dependency>
         <dependency>
             <groupId>org.apache.curator</groupId>
             <artifactId>curator-test</artifactId>
diff --git a/extensions-core/druid-catalog/src/test/java/org/apache/druid/catalog/compact/CatalogCompactionTest.java b/extensions-core/druid-catalog/src/test/java/org/apache/druid/catalog/compact/CatalogCompactionTest.java
index 9f3106b884..ece6a9ef05 100644
--- a/extensions-core/druid-catalog/src/test/java/org/apache/druid/catalog/compact/CatalogCompactionTest.java
+++ b/extensions-core/druid-catalog/src/test/java/org/apache/druid/catalog/compact/CatalogCompactionTest.java
@@ -32,6 +32,7 @@ import org.apache.druid.indexing.compact.CompactionSupervisorSpec;
 import org.apache.druid.indexing.overlord.Segments;
 import org.apache.druid.java.util.common.StringUtils;
 import org.apache.druid.java.util.common.granularity.Granularities;
+import org.apache.druid.query.DruidMetrics;
 import org.apache.druid.rpc.UpdateResponse;
 import org.apache.druid.server.coordinator.CatalogDataSourceCompactionConfig;
 import org.apache.druid.server.coordinator.ClusterCompactionConfig;
@@ -52,7 +53,8 @@ import java.util.List;
 public class CatalogCompactionTest extends EmbeddedClusterTestBase
 {
   private final EmbeddedOverlord overlord = new EmbeddedOverlord()
-      .addProperty("druid.catalog.client.maxSyncRetries", "0");
+      .addProperty("druid.catalog.client.maxSyncRetries", "0")
+      .addProperty("druid.manager.segments.pollDuration", "PT1s");
   private final EmbeddedCoordinator coordinator = new EmbeddedCoordinator()
       .addProperty("druid.manager.segments.useIncrementalCache", "always");
   private final EmbeddedBroker broker = new EmbeddedBroker()
@@ -119,7 +121,8 @@ public class CatalogCompactionTest extends EmbeddedClusterTestBase
     // Wait for compaction to finish
     overlord.latchableEmitter().waitForEvent(
         event -> event.hasMetricName("task/run/time")
-                      .hasDimension("taskType", "compact")
+                      .hasDimension(DruidMetrics.TASK_TYPE, "compact")
+                      .hasDimension(DruidMetrics.DATASOURCE, dataSource)
     );
 
     // Verify that segments are now compacted to MONTH granularity
diff --git a/indexing-service/src/main/java/org/apache/druid/guice/SupervisorModule.java b/indexing-service/src/main/java/org/apache/druid/guice/SupervisorModule.java
index 73e8e06e89..5642fe1126 100644
--- a/indexing-service/src/main/java/org/apache/druid/guice/SupervisorModule.java
+++ b/indexing-service/src/main/java/org/apache/druid/guice/SupervisorModule.java
@@ -25,7 +25,10 @@ import com.fasterxml.jackson.databind.jsontype.NamedType;
 import com.fasterxml.jackson.databind.module.SimpleModule;
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Binder;
+import org.apache.druid.indexing.compact.CascadingCompactionTemplate;
 import org.apache.druid.indexing.compact.CompactionSupervisorSpec;
+import org.apache.druid.indexing.compact.InlineCompactionJobTemplate;
+import org.apache.druid.indexing.compact.MSQCompactionJobTemplate;
 import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManagerConfig;
 import org.apache.druid.indexing.scheduledbatch.ScheduledBatchSupervisorSpec;
 import org.apache.druid.initialization.DruidModule;
@@ -46,6 +49,9 @@ public class SupervisorModule implements DruidModule
     return ImmutableList.of(
         new SimpleModule(getClass().getSimpleName())
             .registerSubtypes(
+                new NamedType(InlineCompactionJobTemplate.class, InlineCompactionJobTemplate.TYPE),
+                new NamedType(MSQCompactionJobTemplate.class, MSQCompactionJobTemplate.TYPE),
+                new NamedType(CascadingCompactionTemplate.class, CascadingCompactionTemplate.TYPE),
                 new NamedType(CompactionSupervisorSpec.class, CompactionSupervisorSpec.TYPE),
                 new NamedType(ScheduledBatchSupervisorSpec.class, ScheduledBatchSupervisorSpec.TYPE)
             )
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java
index 1c65264f6b..75b148d6e5 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java
@@ -27,7 +27,6 @@ import com.fasterxml.jackson.annotation.JsonInclude.Include;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Preconditions;
-import com.google.common.base.Verify;
 import com.google.common.collect.BiMap;
 import com.google.common.collect.HashBiMap;
 import com.google.common.collect.ImmutableList;
@@ -95,8 +94,8 @@ import org.apache.druid.segment.realtime.appenderator.AppenderatorsManager;
 import org.apache.druid.segment.transform.CompactionTransformSpec;
 import org.apache.druid.segment.transform.TransformSpec;
 import org.apache.druid.segment.writeout.SegmentWriteOutMediumFactory;
+import org.apache.druid.server.compaction.CompactionSlotManager;
 import org.apache.druid.server.coordinator.CompactionConfigValidationResult;
-import org.apache.druid.server.coordinator.duty.CompactSegments;
 import org.apache.druid.server.lookup.cache.LookupLoadingSpec;
 import org.apache.druid.server.security.ResourceAction;
 import org.apache.druid.timeline.DataSegment;
@@ -133,7 +132,7 @@ import java.util.stream.IntStream;
  */
 public class CompactionTask extends AbstractBatchIndexTask implements PendingSegmentAllocatingTask
 {
-  public static final String TYPE = "compact";
+  public static final String TYPE = CompactionSlotManager.COMPACTION_TASK_TYPE;
   private static final Logger log = new Logger(CompactionTask.class);
 
   /**
@@ -148,10 +147,6 @@ public class CompactionTask extends AbstractBatchIndexTask implements PendingSeg
    */
   public static final String CTX_KEY_APPENDERATOR_TRACKING_TASK_ID = "appenderatorTrackingTaskId";
 
-  static {
-    Verify.verify(TYPE.equals(CompactSegments.COMPACTION_TASK_TYPE));
-  }
-
   private final CompactionIOConfig ioConfig;
   @Nullable
   private final DimensionsSpec dimensionsSpec;
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/Tasks.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/Tasks.java
index 90fb67116b..b45eb45dc0 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/Tasks.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/Tasks.java
@@ -47,7 +47,6 @@ public class Tasks
   public static final long DEFAULT_SUB_TASK_TIMEOUT_MILLIS = 0;
   public static final boolean DEFAULT_FORCE_TIME_CHUNK_LOCK = true;
   public static final boolean DEFAULT_STORE_COMPACTION_STATE = false;
-  public static final boolean DEFAULT_USE_MAX_MEMORY_ESTIMATES = false;
   public static final TaskLockType DEFAULT_TASK_LOCK_TYPE = TaskLockType.EXCLUSIVE;
   public static final boolean DEFAULT_USE_CONCURRENT_LOCKS = false;
 
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CascadingCompactionTemplate.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CascadingCompactionTemplate.java
new file mode 100644
index 0000000000..89d3897436
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CascadingCompactionTemplate.java
@@ -0,0 +1,246 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.apache.druid.data.input.impl.AggregateProjectionSpec;
+import org.apache.druid.error.InvalidInput;
+import org.apache.druid.indexer.CompactionEngine;
+import org.apache.druid.indexing.input.DruidInputSource;
+import org.apache.druid.java.util.common.DateTimes;
+import org.apache.druid.java.util.common.granularity.Granularity;
+import org.apache.druid.query.aggregation.AggregatorFactory;
+import org.apache.druid.segment.transform.CompactionTransformSpec;
+import org.apache.druid.server.coordinator.DataSourceCompactionConfig;
+import org.apache.druid.server.coordinator.UserCompactionTaskDimensionsConfig;
+import org.apache.druid.server.coordinator.UserCompactionTaskGranularityConfig;
+import org.apache.druid.server.coordinator.UserCompactionTaskIOConfig;
+import org.apache.druid.server.coordinator.UserCompactionTaskQueryTuningConfig;
+import org.joda.time.DateTime;
+import org.joda.time.Interval;
+import org.joda.time.Period;
+
+import javax.annotation.Nullable;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * Template to perform period-based cascading compaction. Contains a list of
+ * {@link CompactionRule} which divide the segment timeline into compactible
+ * intervals. Each rule specifies a period relative to the current time which is
+ * used to determine its applicable interval:
+ * <ul>
+ * <li>Rule 1: range = [now - p1, +inf)</li>
+ * <li>Rule 2: range = [now - p2, now - p1)</li>
+ * <li>...</li>
+ * <li>Rule n: range = (-inf, now - p(n - 1))</li>
+ * </ul>
+ *
+ * If two adjacent rules explicitly specify a segment granularity, the boundary
+ * between them may be {@linkplain CompactionRule#computeStartTime adjusted}
+ * to ensure that there are no uncompacted gaps in the timeline.
+ * <p>
+ * This template never needs to be deserialized as a {@code BatchIndexingJobTemplate},
+ * only as a {@link DataSourceCompactionConfig} in {@link CompactionSupervisorSpec}.
+ */
+public class CascadingCompactionTemplate implements CompactionJobTemplate, DataSourceCompactionConfig
+{
+  public static final String TYPE = "compactCascade";
+
+  private final String dataSource;
+  private final List<CompactionRule> rules;
+
+  @JsonCreator
+  public CascadingCompactionTemplate(
+      @JsonProperty("dataSource") String dataSource,
+      @JsonProperty("rules") List<CompactionRule> rules
+  )
+  {
+    this.rules = rules;
+    this.dataSource = Objects.requireNonNull(dataSource, "'dataSource' cannot be null");
+
+    InvalidInput.conditionalException(rules != null && !rules.isEmpty(), "'rules' cannot be empty");
+  }
+
+  @Override
+  @JsonProperty
+  public String getDataSource()
+  {
+    return dataSource;
+  }
+
+  @JsonProperty
+  public List<CompactionRule> getRules()
+  {
+    return rules;
+  }
+
+  @Override
+  public List<CompactionJob> createCompactionJobs(
+      DruidInputSource source,
+      CompactionJobParams jobParams
+  )
+  {
+    final List<CompactionJob> allJobs = new ArrayList<>();
+
+    // Include future dates in the first rule
+    final DateTime currentTime = jobParams.getScheduleStartTime();
+    DateTime previousRuleStartTime = DateTimes.MAX;
+    for (int i = 0; i < rules.size() - 1; ++i) {
+      final CompactionRule rule = rules.get(i);
+      final DateTime ruleStartTime = rule.computeStartTime(currentTime, rules.get(i + 1));
+      final Interval ruleInterval = new Interval(ruleStartTime, previousRuleStartTime);
+
+      allJobs.addAll(
+          createJobsForSearchInterval(rule.getTemplate(), ruleInterval, source, jobParams)
+      );
+
+      previousRuleStartTime = ruleStartTime;
+    }
+
+    // Include past dates in the last rule
+    final CompactionRule lastRule = rules.get(rules.size() - 1);
+    final Interval lastRuleInterval = new Interval(DateTimes.MIN, previousRuleStartTime);
+    allJobs.addAll(
+        createJobsForSearchInterval(lastRule.getTemplate(), lastRuleInterval, source, jobParams)
+    );
+
+    return allJobs;
+  }
+
+  private List<CompactionJob> createJobsForSearchInterval(
+      CompactionJobTemplate template,
+      Interval searchInterval,
+      DruidInputSource inputSource,
+      CompactionJobParams jobParams
+  )
+  {
+    return template.createCompactionJobs(
+        inputSource.withInterval(searchInterval),
+        jobParams
+    );
+  }
+
+  @Override
+  public String getType()
+  {
+    return TYPE;
+  }
+
+  // Legacy fields from DataSourceCompactionConfig that are not used by this template
+
+  @Nullable
+  @Override
+  public CompactionEngine getEngine()
+  {
+    return null;
+  }
+
+  @Override
+  public int getTaskPriority()
+  {
+    return 0;
+  }
+
+  @Override
+  public long getInputSegmentSizeBytes()
+  {
+    return 0;
+  }
+
+  @Nullable
+  @Override
+  public Integer getMaxRowsPerSegment()
+  {
+    return 0;
+  }
+
+  @Override
+  public Period getSkipOffsetFromLatest()
+  {
+    return null;
+  }
+
+  @Nullable
+  @Override
+  public UserCompactionTaskQueryTuningConfig getTuningConfig()
+  {
+    return null;
+  }
+
+  @Nullable
+  @Override
+  public UserCompactionTaskIOConfig getIoConfig()
+  {
+    return null;
+  }
+
+  @Nullable
+  @Override
+  public Map<String, Object> getTaskContext()
+  {
+    return Map.of();
+  }
+
+  @Nullable
+  @Override
+  public Granularity getSegmentGranularity()
+  {
+    return null;
+  }
+
+  @Nullable
+  @Override
+  public UserCompactionTaskGranularityConfig getGranularitySpec()
+  {
+    return null;
+  }
+
+  @Nullable
+  @Override
+  public List<AggregateProjectionSpec> getProjections()
+  {
+    return List.of();
+  }
+
+  @Nullable
+  @Override
+  public CompactionTransformSpec getTransformSpec()
+  {
+    return null;
+  }
+
+  @Nullable
+  @Override
+  public UserCompactionTaskDimensionsConfig getDimensionsSpec()
+  {
+    return null;
+  }
+
+  @Nullable
+  @Override
+  public AggregatorFactory[] getMetricsSpec()
+  {
+    return new AggregatorFactory[0];
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionConfigBasedJobTemplate.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionConfigBasedJobTemplate.java
new file mode 100644
index 0000000000..c8563290a6
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionConfigBasedJobTemplate.java
@@ -0,0 +1,156 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import org.apache.druid.client.indexing.ClientCompactionTaskQuery;
+import org.apache.druid.error.DruidException;
+import org.apache.druid.error.InvalidInput;
+import org.apache.druid.indexing.input.DruidInputSource;
+import org.apache.druid.java.util.common.Intervals;
+import org.apache.druid.java.util.common.granularity.Granularity;
+import org.apache.druid.server.compaction.CompactionCandidate;
+import org.apache.druid.server.compaction.CompactionSlotManager;
+import org.apache.druid.server.compaction.DataSourceCompactibleSegmentIterator;
+import org.apache.druid.server.compaction.NewestSegmentFirstPolicy;
+import org.apache.druid.server.coordinator.DataSourceCompactionConfig;
+import org.apache.druid.server.coordinator.InlineSchemaDataSourceCompactionConfig;
+import org.apache.druid.server.coordinator.duty.CompactSegments;
+import org.apache.druid.timeline.SegmentTimeline;
+import org.joda.time.Interval;
+import org.joda.time.Period;
+
+import javax.annotation.Nullable;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+
+/**
+ * This template never needs to be deserialized as a {@code BatchIndexingJobTemplate}.
+ * It is just a delegating template that uses a {@link DataSourceCompactionConfig}
+ * to create compaction jobs.
+ */
+public class CompactionConfigBasedJobTemplate implements CompactionJobTemplate
+{
+  private final DataSourceCompactionConfig config;
+
+  public CompactionConfigBasedJobTemplate(DataSourceCompactionConfig config)
+  {
+    this.config = config;
+  }
+
+  public static CompactionConfigBasedJobTemplate create(String dataSource, CompactionStateMatcher stateMatcher)
+  {
+    return new CompactionConfigBasedJobTemplate(
+        InlineSchemaDataSourceCompactionConfig
+            .builder()
+            .forDataSource(dataSource)
+            .withSkipOffsetFromLatest(Period.ZERO)
+            .withTransformSpec(stateMatcher.getTransformSpec())
+            .withProjections(stateMatcher.getProjections())
+            .withMetricsSpec(stateMatcher.getMetricsSpec())
+            .withGranularitySpec(stateMatcher.getGranularitySpec())
+            .build()
+    );
+  }
+
+  @Nullable
+  @Override
+  public Granularity getSegmentGranularity()
+  {
+    return config.getSegmentGranularity();
+  }
+
+  @Override
+  public List<CompactionJob> createCompactionJobs(
+      DruidInputSource source,
+      CompactionJobParams params
+  )
+  {
+    final DataSourceCompactibleSegmentIterator segmentIterator = getCompactibleCandidates(source, params);
+
+    final List<CompactionJob> jobs = new ArrayList<>();
+
+    // Create a job for each CompactionCandidate
+    while (segmentIterator.hasNext()) {
+      final CompactionCandidate candidate = segmentIterator.next();
+
+      ClientCompactionTaskQuery taskPayload
+          = CompactSegments.createCompactionTask(candidate, config, params.getClusterCompactionConfig().getEngine());
+      jobs.add(
+          new CompactionJob(
+              taskPayload,
+              candidate,
+              CompactionSlotManager.getMaxTaskSlotsForNativeCompactionTask(taskPayload.getTuningConfig())
+          )
+      );
+    }
+
+    return jobs;
+  }
+
+  @Override
+  public String getType()
+  {
+    throw DruidException.defensive(
+        "This template cannot be serialized. It is an adapter used to create jobs"
+        + " using a legacy DataSourceCompactionConfig. Do not use this template"
+        + " in a supervisor spec directly. Use types [compactCatalog], [compactMsq]"
+        + " or [compactInline] instead."
+    );
+  }
+
+  /**
+   * Creates an iterator over the compactible candidate segments for the given
+   * params. Adds stats for segments that are already compacted to the
+   * {@link CompactionJobParams#getSnapshotBuilder()}.
+   */
+  DataSourceCompactibleSegmentIterator getCompactibleCandidates(
+      DruidInputSource source,
+      CompactionJobParams params
+  )
+  {
+    validateInput(source);
+
+    final Interval searchInterval = Objects.requireNonNull(source.getInterval());
+
+    final SegmentTimeline timeline = params.getTimeline(config.getDataSource());
+    final DataSourceCompactibleSegmentIterator iterator = new DataSourceCompactibleSegmentIterator(
+        config,
+        timeline,
+        Intervals.complementOf(searchInterval),
+        new NewestSegmentFirstPolicy(null)
+    );
+
+    // Collect stats for segments that are already compacted
+    iterator.getCompactedSegments().forEach(entry -> params.getSnapshotBuilder().addToComplete(entry));
+
+    return iterator;
+  }
+
+  private void validateInput(DruidInputSource druidInputSource)
+  {
+    if (!druidInputSource.getDataSource().equals(config.getDataSource())) {
+      throw InvalidInput.exception(
+          "Datasource[%s] in compaction config does not match datasource[%s] in input source",
+          config.getDataSource(), druidInputSource.getDataSource()
+      );
+    }
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJob.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJob.java
new file mode 100644
index 0000000000..7a7e7fdc1e
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJob.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import org.apache.druid.client.indexing.ClientCompactionTaskQuery;
+import org.apache.druid.indexing.template.BatchIndexingJob;
+import org.apache.druid.query.http.ClientSqlQuery;
+import org.apache.druid.server.compaction.CompactionCandidate;
+
+/**
+ * {@link BatchIndexingJob} to compact an interval of a datasource.
+ */
+public class CompactionJob extends BatchIndexingJob
+{
+  private final CompactionCandidate candidate;
+  private final int maxRequiredTaskSlots;
+
+  public CompactionJob(
+      ClientCompactionTaskQuery task,
+      CompactionCandidate candidate,
+      int maxRequiredTaskSlots
+  )
+  {
+    super(task, null);
+    this.candidate = candidate;
+    this.maxRequiredTaskSlots = maxRequiredTaskSlots;
+  }
+
+  public CompactionJob(
+      ClientSqlQuery msqQuery,
+      CompactionCandidate candidate,
+      int maxRequiredTaskSlots
+  )
+  {
+    super(null, msqQuery);
+    this.candidate = candidate;
+    this.maxRequiredTaskSlots = maxRequiredTaskSlots;
+  }
+
+  public String getDataSource()
+  {
+    return candidate.getDataSource();
+  }
+
+  public CompactionCandidate getCandidate()
+  {
+    return candidate;
+  }
+
+  public int getMaxRequiredTaskSlots()
+  {
+    return maxRequiredTaskSlots;
+  }
+
+  @Override
+  public String toString()
+  {
+    return "CompactionJob{" +
+           super.toString() +
+           ", candidate=" + candidate +
+           ", maxRequiredTaskSlots=" + maxRequiredTaskSlots +
+           '}';
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobParams.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobParams.java
new file mode 100644
index 0000000000..0113f1b78b
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobParams.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import org.apache.druid.server.compaction.CompactionSnapshotBuilder;
+import org.apache.druid.server.coordinator.ClusterCompactionConfig;
+import org.apache.druid.timeline.SegmentTimeline;
+import org.joda.time.DateTime;
+
+/**
+ * Parameters used while creating a {@link CompactionJob} using a {@link CompactionJobTemplate}.
+ */
+public class CompactionJobParams
+{
+  private final DateTime scheduleStartTime;
+  private final TimelineProvider timelineProvider;
+  private final ClusterCompactionConfig clusterCompactionConfig;
+  private final CompactionSnapshotBuilder snapshotBuilder;
+
+  public CompactionJobParams(
+      DateTime scheduleStartTime,
+      ClusterCompactionConfig clusterCompactionConfig,
+      TimelineProvider timelineProvider,
+      CompactionSnapshotBuilder snapshotBuilder
+  )
+  {
+    this.scheduleStartTime = scheduleStartTime;
+    this.clusterCompactionConfig = clusterCompactionConfig;
+    this.timelineProvider = timelineProvider;
+    this.snapshotBuilder = snapshotBuilder;
+  }
+
+  /**
+   * Timestamp denoting the start of the current run of the scheduler which has
+   * triggered creation of jobs using these {@link CompactionJobParams}.
+   */
+  public DateTime getScheduleStartTime()
+  {
+    return scheduleStartTime;
+  }
+
+  /**
+   * Cluster-level compaction config containing details such as the engine,
+   * compaction search policy, etc. to use while creating {@link CompactionJob}.
+   */
+  public ClusterCompactionConfig getClusterCompactionConfig()
+  {
+    return clusterCompactionConfig;
+  }
+
+  /**
+   * Provides the full {@link SegmentTimeline} of used segments for the given
+   * datasource. This timeline is used to identify eligible intervals for which
+   * compaction jobs should be created.
+   */
+  public SegmentTimeline getTimeline(String dataSource)
+  {
+    return timelineProvider.getTimelineForDataSource(dataSource);
+  }
+
+  /**
+   * Used to build an {@link org.apache.druid.server.coordinator.AutoCompactionSnapshot}
+   * for all the datasources at the end of the current run. During the run, as
+   * candidate intervals are identified as compacted, skipped or pending, they
+   * should be updated in this snapshot builder by invoking
+   * {@link CompactionSnapshotBuilder#addToComplete}, {@link CompactionSnapshotBuilder#addToSkipped}
+   * and {@link CompactionSnapshotBuilder#addToPending} respectively.
+   */
+  public CompactionSnapshotBuilder getSnapshotBuilder()
+  {
+    return snapshotBuilder;
+  }
+
+  @FunctionalInterface
+  public interface TimelineProvider
+  {
+    SegmentTimeline getTimelineForDataSource(String dataSource);
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobQueue.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobQueue.java
new file mode 100644
index 0000000000..77886af1a0
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobQueue.java
@@ -0,0 +1,373 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.druid.client.DataSourcesSnapshot;
+import org.apache.druid.client.broker.BrokerClient;
+import org.apache.druid.client.indexing.ClientCompactionTaskQuery;
+import org.apache.druid.client.indexing.ClientTaskQuery;
+import org.apache.druid.common.guava.FutureUtils;
+import org.apache.druid.indexer.TaskState;
+import org.apache.druid.indexer.TaskStatus;
+import org.apache.druid.indexing.common.actions.TaskActionClientFactory;
+import org.apache.druid.indexing.common.task.Task;
+import org.apache.druid.indexing.input.DruidInputSource;
+import org.apache.druid.indexing.overlord.GlobalTaskLockbox;
+import org.apache.druid.indexing.template.BatchIndexingJob;
+import org.apache.druid.java.util.common.DateTimes;
+import org.apache.druid.java.util.common.Stopwatch;
+import org.apache.druid.java.util.common.logger.Logger;
+import org.apache.druid.rpc.indexing.OverlordClient;
+import org.apache.druid.server.compaction.CompactionCandidate;
+import org.apache.druid.server.compaction.CompactionCandidateSearchPolicy;
+import org.apache.druid.server.compaction.CompactionSlotManager;
+import org.apache.druid.server.compaction.CompactionSnapshotBuilder;
+import org.apache.druid.server.compaction.CompactionStatus;
+import org.apache.druid.server.compaction.CompactionStatusTracker;
+import org.apache.druid.server.coordinator.AutoCompactionSnapshot;
+import org.apache.druid.server.coordinator.ClusterCompactionConfig;
+import org.apache.druid.server.coordinator.CompactionConfigValidationResult;
+import org.apache.druid.server.coordinator.stats.CoordinatorRunStats;
+import org.apache.druid.server.coordinator.stats.Dimension;
+import org.apache.druid.server.coordinator.stats.RowKey;
+import org.apache.druid.server.coordinator.stats.Stats;
+
+import javax.annotation.Nullable;
+import javax.annotation.concurrent.NotThreadSafe;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/**
+ * Iterates over all eligible compaction jobs in order of their priority.
+ * A fresh instance of this class must be used in every run of the
+ * {@link CompactionScheduler}.
+ * <p>
+ * Unlike the Coordinator duty {@code CompactSegments}, the job queue currently
+ * does not cancel running compaction tasks even if their target segment
+ * granularity has changed. This has not been done here for simplicity since the
+ * {@code CompactionJobQueue} uses compaction templates and may have a different
+ * target segment granulariy for different intervals of the same datasource.
+ * The cancellation of invalid tasks has been left as a future enhancement.
+ */
+@NotThreadSafe
+public class CompactionJobQueue
+{
+  private static final Logger log = new Logger(CompactionJobQueue.class);
+
+  private final CompactionJobParams jobParams;
+  private final CompactionCandidateSearchPolicy searchPolicy;
+  private final ClusterCompactionConfig clusterCompactionConfig;
+
+  private final ObjectMapper objectMapper;
+  private final CompactionStatusTracker statusTracker;
+  private final TaskActionClientFactory taskActionClientFactory;
+  private final OverlordClient overlordClient;
+  private final GlobalTaskLockbox taskLockbox;
+  private final BrokerClient brokerClient;
+
+  private final CompactionSnapshotBuilder snapshotBuilder;
+  private final PriorityQueue<CompactionJob> queue;
+  private final CoordinatorRunStats runStats;
+
+  private final Set<String> activeSupervisors;
+  private final Map<String, CompactionJob> submittedTaskIdToJob;
+
+  public CompactionJobQueue(
+      DataSourcesSnapshot dataSourcesSnapshot,
+      ClusterCompactionConfig clusterCompactionConfig,
+      CompactionStatusTracker statusTracker,
+      TaskActionClientFactory taskActionClientFactory,
+      GlobalTaskLockbox taskLockbox,
+      OverlordClient overlordClient,
+      BrokerClient brokerClient,
+      ObjectMapper objectMapper
+  )
+  {
+    this.runStats = new CoordinatorRunStats();
+    this.snapshotBuilder = new CompactionSnapshotBuilder(runStats);
+    this.clusterCompactionConfig = clusterCompactionConfig;
+    this.searchPolicy = clusterCompactionConfig.getCompactionPolicy();
+    this.queue = new PriorityQueue<>(
+        (o1, o2) -> searchPolicy.compareCandidates(o1.getCandidate(), o2.getCandidate())
+    );
+    this.submittedTaskIdToJob = new HashMap<>();
+    this.activeSupervisors = new HashSet<>();
+    this.jobParams = new CompactionJobParams(
+        DateTimes.nowUtc(),
+        clusterCompactionConfig,
+        dataSourcesSnapshot.getUsedSegmentsTimelinesPerDataSource()::get,
+        snapshotBuilder
+    );
+
+    this.taskActionClientFactory = taskActionClientFactory;
+    this.overlordClient = overlordClient;
+    this.brokerClient = brokerClient;
+    this.statusTracker = statusTracker;
+    this.objectMapper = objectMapper;
+    this.taskLockbox = taskLockbox;
+  }
+
+  /**
+   * Creates jobs for the given {@link CompactionSupervisor} and adds them to
+   * the job queue.
+   * <p>
+   * This method is idempotent. If jobs for the given supervisor already exist
+   * in the queue, the method does nothing.
+   */
+  public void createAndEnqueueJobs(
+      CompactionSupervisor supervisor,
+      DruidInputSource source
+  )
+  {
+    final Stopwatch jobCreationTime = Stopwatch.createStarted();
+    final String supervisorId = supervisor.getSpec().getId();
+    try {
+      if (supervisor.shouldCreateJobs() && !activeSupervisors.contains(supervisorId)) {
+        // Queue fresh jobs
+        final List<CompactionJob> jobs = supervisor.createJobs(source, jobParams);
+        jobs.forEach(job -> snapshotBuilder.addToPending(job.getCandidate()));
+
+        queue.addAll(jobs);
+        activeSupervisors.add(supervisorId);
+
+        runStats.add(
+            Stats.Compaction.CREATED_JOBS,
+            RowKey.of(Dimension.DATASOURCE, source.getDataSource()),
+            jobs.size()
+        );
+      } else {
+        log.debug("Skipping job creation for supervisor[%s]", supervisorId);
+      }
+    }
+    catch (Exception e) {
+      log.error(e, "Error while creating jobs for supervisor[%s]", supervisorId);
+    }
+    finally {
+      runStats.add(
+          Stats.Compaction.JOB_CREATION_TIME,
+          RowKey.of(Dimension.DATASOURCE, source.getDataSource()),
+          jobCreationTime.millisElapsed()
+      );
+    }
+  }
+
+  /**
+   * Removes all existing jobs for the given datasource from the queue.
+   */
+  public void removeJobs(String dataSource)
+  {
+    final List<CompactionJob> jobsToRemove = queue
+        .stream()
+        .filter(job -> job.getDataSource().equals(dataSource))
+        .collect(Collectors.toList());
+
+    queue.removeAll(jobsToRemove);
+    log.info("Removed [%d] jobs for datasource[%s] from queue.", jobsToRemove.size(), dataSource);
+  }
+
+  /**
+   * Submits jobs which are ready to either the Overlord or a Broker (if it is
+   * an MSQ SQL job).
+   */
+  public void runReadyJobs()
+  {
+    final CompactionSlotManager slotManager = new CompactionSlotManager(
+        overlordClient,
+        statusTracker,
+        clusterCompactionConfig
+    );
+    slotManager.reserveTaskSlotsForRunningCompactionTasks();
+
+    final List<CompactionJob> pendingJobs = new ArrayList<>();
+    while (!queue.isEmpty()) {
+      final CompactionJob job = queue.poll();
+      if (startJobIfPendingAndReady(job, searchPolicy, pendingJobs, slotManager)) {
+        runStats.add(Stats.Compaction.SUBMITTED_TASKS, RowKey.of(Dimension.DATASOURCE, job.getDataSource()), 1);
+      }
+    }
+
+    // Requeue pending jobs so that they can be launched when slots become available
+    queue.addAll(pendingJobs);
+  }
+
+  /**
+   * Notifies completion of the given so that the compaction snapshots may be
+   * updated.
+   */
+  public void onTaskFinished(String taskId, TaskStatus taskStatus)
+  {
+    final CompactionJob job = submittedTaskIdToJob.remove(taskId);
+    if (job == null || !taskStatus.getStatusCode().isComplete()) {
+      // This is an unknown task ID
+      return;
+    }
+
+    if (taskStatus.getStatusCode() == TaskState.FAILED) {
+      // Add this job back to the queue
+      queue.add(job);
+    } else {
+      snapshotBuilder.moveFromPendingToCompleted(job.getCandidate());
+    }
+  }
+
+  public CoordinatorRunStats getRunStats()
+  {
+    return runStats;
+  }
+
+  /**
+   * Builds compaction snapshots for all the datasources being tracked by this
+   * queue.
+   */
+  public Map<String, AutoCompactionSnapshot> getSnapshots()
+  {
+    return snapshotBuilder.build();
+  }
+
+  /**
+   * Starts a job if it is ready and is not already in progress.
+   *
+   * @return true if the job was submitted successfully for execution
+   */
+  private boolean startJobIfPendingAndReady(
+      CompactionJob job,
+      CompactionCandidateSearchPolicy policy,
+      List<CompactionJob> pendingJobs,
+      CompactionSlotManager slotManager
+  )
+  {
+    // Check if the job is a valid compaction job
+    final CompactionCandidate candidate = job.getCandidate();
+    final CompactionConfigValidationResult validationResult = validateCompactionJob(job);
+    if (!validationResult.isValid()) {
+      log.error("Skipping invalid compaction job[%s] due to reason[%s].", job, validationResult.getReason());
+      snapshotBuilder.moveFromPendingToSkipped(candidate);
+      return false;
+    }
+
+    // Check if the job is already running, completed or skipped
+    final CompactionStatus compactionStatus = getCurrentStatusForJob(job, policy);
+    switch (compactionStatus.getState()) {
+      case RUNNING:
+        return false;
+      case COMPLETE:
+        snapshotBuilder.moveFromPendingToCompleted(candidate);
+        return false;
+      case SKIPPED:
+        snapshotBuilder.moveFromPendingToSkipped(candidate);
+        return false;
+      default:
+        break;
+    }
+
+    // Check if enough compaction task slots are available
+    if (job.getMaxRequiredTaskSlots() > slotManager.getNumAvailableTaskSlots()) {
+      pendingJobs.add(job);
+      return false;
+    }
+
+    // Reserve task slots and try to start the task
+    slotManager.reserveTaskSlots(job.getMaxRequiredTaskSlots());
+    final String taskId = startTaskIfReady(job);
+    if (taskId == null) {
+      // Mark the job as skipped for now as the intervals might be locked by other tasks
+      snapshotBuilder.moveFromPendingToSkipped(candidate);
+      return false;
+    } else {
+      statusTracker.onTaskSubmitted(taskId, job.getCandidate());
+      submittedTaskIdToJob.put(taskId, job);
+      return true;
+    }
+  }
+
+  /**
+   * Starts the given job if the underlying Task is able to acquire locks.
+   *
+   * @return Non-null taskId if the Task was submitted successfully.
+   */
+  @Nullable
+  private String startTaskIfReady(CompactionJob job)
+  {
+    // Assume MSQ jobs to be always ready
+    if (job.isMsq()) {
+      try {
+        return FutureUtils.getUnchecked(brokerClient.submitSqlTask(job.getNonNullMsqQuery()), true)
+                          .getTaskId();
+      }
+      catch (Exception e) {
+        log.error(e, "Error while submitting query[%s] to Broker", job.getNonNullMsqQuery());
+      }
+    }
+
+    final ClientTaskQuery taskQuery = job.getNonNullTask();
+    final Task task = objectMapper.convertValue(taskQuery, Task.class);
+
+    log.debug(
+        "Checking readiness of task[%s] with interval[%s]",
+        task.getId(), job.getCandidate().getCompactionInterval()
+    );
+    try {
+      taskLockbox.add(task);
+      if (task.isReady(taskActionClientFactory.create(task))) {
+        // Hold the locks acquired by task.isReady() as we will reacquire them anyway
+        FutureUtils.getUnchecked(overlordClient.runTask(task.getId(), task), true);
+        return task.getId();
+      } else {
+        taskLockbox.unlockAll(task);
+        return null;
+      }
+    }
+    catch (Exception e) {
+      log.error(e, "Error while submitting task[%s] to Overlord", task.getId());
+      taskLockbox.unlockAll(task);
+      return null;
+    }
+  }
+
+  public CompactionStatus getCurrentStatusForJob(CompactionJob job, CompactionCandidateSearchPolicy policy)
+  {
+    final CompactionStatus compactionStatus = statusTracker.computeCompactionStatus(job.getCandidate(), policy);
+    final CompactionCandidate candidatesWithStatus = job.getCandidate().withCurrentStatus(null);
+    statusTracker.onCompactionStatusComputed(candidatesWithStatus, null);
+    return compactionStatus;
+  }
+
+  public static CompactionConfigValidationResult validateCompactionJob(BatchIndexingJob job)
+  {
+    // For MSQ jobs, do not perform any validation
+    if (job.isMsq()) {
+      return CompactionConfigValidationResult.success();
+    }
+
+    final ClientTaskQuery task = job.getNonNullTask();
+    if (!(task instanceof ClientCompactionTaskQuery)) {
+      return CompactionConfigValidationResult.failure("Invalid task type[%s]", task.getType());
+    }
+
+    return CompactionConfigValidationResult.success();
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobTemplate.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobTemplate.java
new file mode 100644
index 0000000000..cd5096a8cf
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionJobTemplate.java
@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import org.apache.druid.indexing.input.DruidInputSource;
+import org.apache.druid.indexing.template.BatchIndexingJobTemplate;
+import org.apache.druid.java.util.common.granularity.Granularity;
+
+import javax.annotation.Nullable;
+import java.util.List;
+
+/**
+ * Base indexing template for creating {@link CompactionJob}.
+ */
+public interface CompactionJobTemplate extends BatchIndexingJobTemplate
+{
+  /**
+   * Creates compaction jobs with this template for the given datasource.
+   */
+  List<CompactionJob> createCompactionJobs(
+      DruidInputSource source,
+      CompactionJobParams jobParams
+  );
+
+  /**
+   * Granularity of segments created upon successful compaction.
+   *
+   * @return null only if this template does not change segment granularity upon
+   * successful compaction.
+   */
+  @Nullable
+  Granularity getSegmentGranularity();
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionRule.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionRule.java
new file mode 100644
index 0000000000..10099566ea
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionRule.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.apache.druid.java.util.common.granularity.Granularity;
+import org.joda.time.DateTime;
+import org.joda.time.Period;
+
+/**
+ * A single rule used inside {@link CascadingCompactionTemplate}.
+ */
+public class CompactionRule
+{
+  private final Period period;
+  private final CompactionJobTemplate template;
+
+  @JsonCreator
+  public CompactionRule(
+      @JsonProperty("period") Period period,
+      @JsonProperty("template") CompactionJobTemplate template
+  )
+  {
+    this.period = period;
+    this.template = template;
+  }
+
+  @JsonProperty
+  public CompactionJobTemplate getTemplate()
+  {
+    return template;
+  }
+
+  @JsonProperty
+  public Period getPeriod()
+  {
+    return period;
+  }
+
+  /**
+   * Computes the start time of this rule by subtracting its period from the
+   * reference timestamp.
+   * <p>
+   * If both this rule and the {@code beforeRule} explicitly specify a target
+   * segment granularity, the start time may be adjusted to ensure that there
+   * are no uncompacted gaps left in the timeline.
+   *
+   * @param referenceTime Current time when the rules are being evaluated
+   * @param beforeRule    The rule before this one in chronological order
+   */
+  public DateTime computeStartTime(DateTime referenceTime, CompactionRule beforeRule)
+  {
+    final Granularity granularity = template.getSegmentGranularity();
+    final Granularity beforeGranularity = beforeRule.template.getSegmentGranularity();
+
+    final DateTime calculatedStartTime = referenceTime.minus(period);
+
+    if (granularity == null || beforeGranularity == null) {
+      return calculatedStartTime;
+    } else {
+      // The gap can be filled only if it is bigger than the granularity of this rule.
+      // If beforeGranularity > granularity, gap would always be smaller than both
+      final DateTime beforeRuleEffectiveEnd = beforeGranularity.bucketStart(calculatedStartTime);
+      final DateTime possibleStartTime = granularity.bucketStart(beforeRuleEffectiveEnd);
+      return possibleStartTime.isBefore(beforeRuleEffectiveEnd)
+             ? granularity.increment(possibleStartTime)
+             : possibleStartTime;
+    }
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionScheduler.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionScheduler.java
index 5f0aa6e3ea..6f5ed1a7a6 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionScheduler.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionScheduler.java
@@ -65,7 +65,7 @@ public interface CompactionScheduler
   /**
    * Starts compaction for a datasource if not already running.
    */
-  void startCompaction(String dataSourceName, DataSourceCompactionConfig compactionConfig);
+  void startCompaction(String dataSourceName, CompactionSupervisor supervisor);
 
   /**
    * Stops compaction for a datasource if currently running.
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionStateMatcher.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionStateMatcher.java
new file mode 100644
index 0000000000..84a95e2f6e
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionStateMatcher.java
@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.apache.druid.data.input.impl.AggregateProjectionSpec;
+import org.apache.druid.data.input.impl.DimensionsSpec;
+import org.apache.druid.indexer.partitions.PartitionsSpec;
+import org.apache.druid.java.util.common.granularity.Granularity;
+import org.apache.druid.query.aggregation.AggregatorFactory;
+import org.apache.druid.segment.IndexSpec;
+import org.apache.druid.segment.transform.CompactionTransformSpec;
+import org.apache.druid.server.coordinator.UserCompactionTaskGranularityConfig;
+
+import javax.annotation.Nullable;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Objects;
+
+/**
+ * Target compacted state of segments used to determine if compaction is needed
+ * for an interval. An explicitly defined target state helps avoid superfluous
+ * compaction when only the job definition has changed.
+ * <p>
+ * This class is mostly a duplicate of {@code CompactionState} but is kept
+ * separate to allow:
+ * <ul>
+ * <li>fields to be nullable so that only non-null fields are used for matching</li>
+ * <li>legacy "compaction-incompatible" fields such as {@link #transformSpec} to
+ * be removed in the future. These fields do not just change the layout/partitioning
+ * of the data but may also alter its meaning which does not fall in the purview
+ * of compaction.</li>
+ * </ul>
+ */
+@JsonInclude(JsonInclude.Include.NON_NULL)
+public class CompactionStateMatcher
+{
+  private final PartitionsSpec partitionsSpec;
+  private final DimensionsSpec dimensionsSpec;
+  private final CompactionTransformSpec transformSpec;
+  private final IndexSpec indexSpec;
+  private final UserCompactionTaskGranularityConfig granularitySpec;
+  private final AggregatorFactory[] metricsSpec;
+  private final List<AggregateProjectionSpec> projections;
+
+  @JsonCreator
+  public CompactionStateMatcher(
+      @JsonProperty("partitionsSpec") @Nullable PartitionsSpec partitionsSpec,
+      @JsonProperty("dimensionsSpec") @Nullable DimensionsSpec dimensionsSpec,
+      @JsonProperty("metricsSpec") @Nullable AggregatorFactory[] metricsSpec,
+      @JsonProperty("transformSpec") @Nullable CompactionTransformSpec transformSpec,
+      @JsonProperty("indexSpec") @Nullable IndexSpec indexSpec,
+      @JsonProperty("granularitySpec") @Nullable UserCompactionTaskGranularityConfig granularitySpec,
+      @JsonProperty("projections") @Nullable List<AggregateProjectionSpec> projections
+  )
+  {
+    this.partitionsSpec = partitionsSpec;
+    this.dimensionsSpec = dimensionsSpec;
+    this.metricsSpec = metricsSpec;
+    this.transformSpec = transformSpec;
+    this.indexSpec = indexSpec;
+    this.granularitySpec = granularitySpec;
+    this.projections = projections;
+  }
+
+  @Nullable
+  @JsonProperty
+  public PartitionsSpec getPartitionsSpec()
+  {
+    return partitionsSpec;
+  }
+
+  @Nullable
+  @JsonProperty
+  public DimensionsSpec getDimensionsSpec()
+  {
+    return dimensionsSpec;
+  }
+
+  @Nullable
+  @JsonProperty
+  public AggregatorFactory[] getMetricsSpec()
+  {
+    return metricsSpec;
+  }
+
+  @Nullable
+  @JsonProperty
+  public CompactionTransformSpec getTransformSpec()
+  {
+    return transformSpec;
+  }
+
+  @Nullable
+  @JsonProperty
+  public IndexSpec getIndexSpec()
+  {
+    return indexSpec;
+  }
+
+  @Nullable
+  @JsonProperty
+  public UserCompactionTaskGranularityConfig getGranularitySpec()
+  {
+    return granularitySpec;
+  }
+
+  @JsonProperty
+  @Nullable
+  public List<AggregateProjectionSpec> getProjections()
+  {
+    return projections;
+  }
+
+  @Nullable
+  public Granularity getSegmentGranularity()
+  {
+    return granularitySpec == null ? null : granularitySpec.getSegmentGranularity();
+  }
+
+  @Override
+  public boolean equals(Object o)
+  {
+    if (this == o) {
+      return true;
+    }
+    if (o == null || getClass() != o.getClass()) {
+      return false;
+    }
+    CompactionStateMatcher that = (CompactionStateMatcher) o;
+    return Objects.equals(partitionsSpec, that.partitionsSpec) &&
+           Objects.equals(dimensionsSpec, that.dimensionsSpec) &&
+           Objects.equals(transformSpec, that.transformSpec) &&
+           Objects.equals(indexSpec, that.indexSpec) &&
+           Objects.equals(granularitySpec, that.granularitySpec) &&
+           Arrays.equals(metricsSpec, that.metricsSpec) &&
+           Objects.equals(projections, that.projections);
+  }
+
+  @Override
+  public int hashCode()
+  {
+    return Objects.hash(
+        partitionsSpec,
+        dimensionsSpec,
+        transformSpec,
+        indexSpec,
+        granularitySpec,
+        Arrays.hashCode(metricsSpec),
+        projections
+    );
+  }
+
+  @Override
+  public String toString()
+  {
+    return "CompactionState{" +
+           "partitionsSpec=" + partitionsSpec +
+           ", dimensionsSpec=" + dimensionsSpec +
+           ", transformSpec=" + transformSpec +
+           ", indexSpec=" + indexSpec +
+           ", granularitySpec=" + granularitySpec +
+           ", metricsSpec=" + Arrays.toString(metricsSpec) +
+           ", projections=" + projections +
+           '}';
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionSupervisor.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionSupervisor.java
index 851b3920b1..f9ba0eee6d 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionSupervisor.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionSupervisor.java
@@ -19,6 +19,7 @@
 
 package org.apache.druid.indexing.compact;
 
+import org.apache.druid.indexing.input.DruidInputSource;
 import org.apache.druid.indexing.overlord.DataSourceMetadata;
 import org.apache.druid.indexing.overlord.supervisor.Supervisor;
 import org.apache.druid.indexing.overlord.supervisor.SupervisorReport;
@@ -29,6 +30,7 @@ import org.apache.druid.java.util.common.logger.Logger;
 import org.apache.druid.server.coordinator.AutoCompactionSnapshot;
 
 import javax.annotation.Nullable;
+import java.util.List;
 
 /**
  * Supervisor for compaction of a single datasource.
@@ -51,6 +53,31 @@ public class CompactionSupervisor implements Supervisor
     this.dataSource = supervisorSpec.getSpec().getDataSource();
   }
 
+  public CompactionSupervisorSpec getSpec()
+  {
+    return supervisorSpec;
+  }
+
+  /**
+   * Checks if this supervisor is ready to create jobs in the current run of the
+   * scheduler.
+   */
+  public boolean shouldCreateJobs()
+  {
+    return !supervisorSpec.isSuspended();
+  }
+
+  /**
+   * Creates compaction jobs for this supervisor.
+   */
+  public List<CompactionJob> createJobs(
+      DruidInputSource inputSource,
+      CompactionJobParams jobParams
+  )
+  {
+    return supervisorSpec.getTemplate().createCompactionJobs(inputSource, jobParams);
+  }
+
   @Override
   public void start()
   {
@@ -66,7 +93,7 @@ public class CompactionSupervisor implements Supervisor
       );
     } else {
       log.info("Starting compaction for dataSource[%s].", dataSource);
-      scheduler.startCompaction(dataSource, supervisorSpec.getSpec());
+      scheduler.startCompaction(dataSource, this);
     }
   }
 
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionSupervisorSpec.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionSupervisorSpec.java
index ca6008cc9d..998d7e0c7e 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionSupervisorSpec.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/CompactionSupervisorSpec.java
@@ -93,6 +93,18 @@ public class CompactionSupervisorSpec implements SupervisorSpec
     return new CompactionSupervisor(this, scheduler);
   }
 
+  /**
+   * @return {@link CompactionJobTemplate} used to create jobs for the supervisor.
+   */
+  public CompactionJobTemplate getTemplate()
+  {
+    if (spec instanceof CascadingCompactionTemplate) {
+      return (CascadingCompactionTemplate) spec;
+    } else {
+      return new CompactionConfigBasedJobTemplate(spec);
+    }
+  }
+
   @Override
   public List<String> getDataSources()
   {
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/DruidInputSourceFactory.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/DruidInputSourceFactory.java
new file mode 100644
index 0000000000..4089146404
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/DruidInputSourceFactory.java
@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import com.google.inject.Inject;
+import org.apache.druid.client.coordinator.CoordinatorClient;
+import org.apache.druid.indexing.common.SegmentCacheManagerFactory;
+import org.apache.druid.indexing.common.config.TaskConfig;
+import org.apache.druid.indexing.input.DruidInputSource;
+import org.apache.druid.segment.IndexIO;
+import org.joda.time.Interval;
+
+/**
+ * Factory for creating {@link DruidInputSource} for a given datasource and
+ * interval used by {@link CompactionJobTemplate}.
+ */
+public class DruidInputSourceFactory
+{
+  private final IndexIO indexIO;
+  private final TaskConfig taskConfig;
+  private final CoordinatorClient coordinatorClient;
+  private final SegmentCacheManagerFactory segmentCacheManagerFactory;
+
+  @Inject
+  public DruidInputSourceFactory(
+      IndexIO indexIO,
+      TaskConfig taskConfig,
+      CoordinatorClient coordinatorClient,
+      SegmentCacheManagerFactory segmentCacheManagerFactory
+  )
+  {
+    this.indexIO = indexIO;
+    this.coordinatorClient = coordinatorClient;
+    this.segmentCacheManagerFactory = segmentCacheManagerFactory;
+    this.taskConfig = taskConfig;
+  }
+
+  /**
+   * Creates a new {@link DruidInputSource} for the given {@code dataSource} and
+   * {@code interval}.
+   */
+  public DruidInputSource create(String dataSource, Interval interval)
+  {
+    return new DruidInputSource(
+        dataSource,
+        interval,
+        null,
+        null,
+        null,
+        null,
+        indexIO,
+        coordinatorClient,
+        segmentCacheManagerFactory,
+        taskConfig
+    );
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/InlineCompactionJobTemplate.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/InlineCompactionJobTemplate.java
new file mode 100644
index 0000000000..43e4c6f8c2
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/InlineCompactionJobTemplate.java
@@ -0,0 +1,98 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.apache.druid.indexing.input.DruidInputSource;
+import org.apache.druid.java.util.common.granularity.Granularity;
+
+import javax.annotation.Nullable;
+import java.util.List;
+import java.util.Objects;
+
+/**
+ * Template to create compaction jobs using inline specifications. This template
+ * does not fetch any information from the Druid catalog while creating jobs.
+ */
+public class InlineCompactionJobTemplate implements CompactionJobTemplate
+{
+  public static final String TYPE = "compactInline";
+
+  private final CompactionStateMatcher targetState;
+
+  @JsonCreator
+  public InlineCompactionJobTemplate(
+      @JsonProperty("targetState") CompactionStateMatcher targetState
+  )
+  {
+    this.targetState = targetState;
+  }
+
+  @JsonProperty
+  public CompactionStateMatcher getTargetState()
+  {
+    return targetState;
+  }
+
+  @Nullable
+  @Override
+  public Granularity getSegmentGranularity()
+  {
+    return targetState.getSegmentGranularity();
+  }
+
+  @Override
+  public List<CompactionJob> createCompactionJobs(
+      DruidInputSource source,
+      CompactionJobParams jobParams
+  )
+  {
+    final String dataSource = source.getDataSource();
+    return CompactionConfigBasedJobTemplate
+        .create(dataSource, targetState)
+        .createCompactionJobs(source, jobParams);
+  }
+
+  @Override
+  public boolean equals(Object object)
+  {
+    if (this == object) {
+      return true;
+    }
+    if (object == null || getClass() != object.getClass()) {
+      return false;
+    }
+    InlineCompactionJobTemplate that = (InlineCompactionJobTemplate) object;
+    return Objects.equals(this.targetState, that.targetState);
+  }
+
+  @Override
+  public int hashCode()
+  {
+    return Objects.hash(targetState);
+  }
+
+  @Override
+  public String getType()
+  {
+    return TYPE;
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/LocalOverlordClient.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/LocalOverlordClient.java
index 3f1427c7c3..eb200577bb 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/compact/LocalOverlordClient.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/LocalOverlordClient.java
@@ -72,10 +72,12 @@ class LocalOverlordClient extends NoopOverlordClient
   @Override
   public ListenableFuture<Void> runTask(String taskId, Object clientTaskQuery)
   {
+    final CompactionTask task =
+        clientTaskQuery instanceof CompactionTask
+        ? (CompactionTask) clientTaskQuery
+        : convertTask(clientTaskQuery, ClientCompactionTaskQuery.class, CompactionTask.class);
     return futureOf(() -> {
-      getValidTaskQueue().add(
-          convertTask(clientTaskQuery, ClientCompactionTaskQuery.class, CompactionTask.class)
-      );
+      getValidTaskQueue().add(task);
       return null;
     });
   }
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/MSQCompactionJobTemplate.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/MSQCompactionJobTemplate.java
new file mode 100644
index 0000000000..ea7193e443
--- /dev/null
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/MSQCompactionJobTemplate.java
@@ -0,0 +1,191 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.compact;
+
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.apache.druid.indexing.input.DruidInputSource;
+import org.apache.druid.java.util.common.StringUtils;
+import org.apache.druid.java.util.common.granularity.Granularity;
+import org.apache.druid.query.http.ClientSqlQuery;
+import org.apache.druid.server.compaction.CompactionCandidate;
+import org.apache.druid.server.compaction.CompactionSlotManager;
+import org.apache.druid.server.compaction.DataSourceCompactibleSegmentIterator;
+import org.apache.druid.server.coordinator.duty.CompactSegments;
+import org.joda.time.Interval;
+import org.joda.time.format.DateTimeFormat;
+import org.joda.time.format.DateTimeFormatter;
+
+import javax.annotation.Nullable;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * Compaction template that creates MSQ SQL jobs using a templatized SQL with
+ * variables of the format {@code ${variableName}} for fields such as datasource
+ * name and start timestamp.
+ * <p>
+ * Compaction is triggered for an interval only if the current compaction state
+ * of the underlying segments DOES NOT match with the {@link #targetState}.
+ */
+public class MSQCompactionJobTemplate implements CompactionJobTemplate
+{
+  public static final String TYPE = "compactMsq";
+
+  public static final String VAR_DATASOURCE = "${dataSource}";
+  public static final String VAR_START_TIMESTAMP = "${startTimestamp}";
+  public static final String VAR_END_TIMESTAMP = "${endTimestamp}";
+
+  private static final DateTimeFormatter TIMESTAMP_FORMATTER =
+      DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss.SSS");
+
+  private final ClientSqlQuery sqlTemplate;
+  private final CompactionStateMatcher targetState;
+
+  @JsonCreator
+  public MSQCompactionJobTemplate(
+      @JsonProperty("sqlTemplate") ClientSqlQuery sqlTemplate,
+      @JsonProperty("targetState") CompactionStateMatcher targetState
+  )
+  {
+    this.sqlTemplate = sqlTemplate;
+    this.targetState = targetState;
+  }
+
+  @JsonProperty
+  public ClientSqlQuery getSqlTemplate()
+  {
+    return sqlTemplate;
+  }
+
+  @JsonProperty
+  public CompactionStateMatcher getTargetState()
+  {
+    return targetState;
+  }
+
+  @Nullable
+  @Override
+  public Granularity getSegmentGranularity()
+  {
+    return targetState.getSegmentGranularity();
+  }
+
+  @Override
+  public List<CompactionJob> createCompactionJobs(
+      DruidInputSource source,
+      CompactionJobParams jobParams
+  )
+  {
+    final String dataSource = source.getDataSource();
+
+    // Identify the compactible candidate segments
+    final CompactionConfigBasedJobTemplate delegate =
+        CompactionConfigBasedJobTemplate.create(dataSource, targetState);
+    final DataSourceCompactibleSegmentIterator candidateIterator =
+        delegate.getCompactibleCandidates(source, jobParams);
+
+    // Create MSQ jobs for each candidate by interpolating the template variables
+    final List<CompactionJob> jobs = new ArrayList<>();
+    while (candidateIterator.hasNext()) {
+      final CompactionCandidate candidate = candidateIterator.next();
+      jobs.add(
+          new CompactionJob(
+              createQueryForJob(dataSource, candidate.getCompactionInterval()),
+              candidate,
+              CompactionSlotManager.getMaxTaskSlotsForMSQCompactionTask(sqlTemplate.getContext())
+          )
+      );
+    }
+
+    return jobs;
+  }
+
+  private ClientSqlQuery createQueryForJob(String dataSource, Interval compactionInterval)
+  {
+    final String formattedSql = formatSql(
+        sqlTemplate.getQuery(),
+        Map.of(
+            VAR_DATASOURCE, dataSource,
+            VAR_START_TIMESTAMP, compactionInterval.getStart().toString(TIMESTAMP_FORMATTER),
+            VAR_END_TIMESTAMP, compactionInterval.getEnd().toString(TIMESTAMP_FORMATTER)
+        )
+    );
+
+    final Map<String, Object> context = new HashMap<>();
+    if (sqlTemplate.getContext() != null) {
+      context.putAll(sqlTemplate.getContext());
+    }
+    context.put(CompactSegments.STORE_COMPACTION_STATE_KEY, true);
+    context.put(CompactSegments.COMPACTION_INTERVAL_KEY, compactionInterval);
+
+    return new ClientSqlQuery(
+        formattedSql,
+        sqlTemplate.getResultFormat(),
+        sqlTemplate.isHeader(),
+        sqlTemplate.isTypesHeader(),
+        sqlTemplate.isSqlTypesHeader(),
+        context,
+        sqlTemplate.getParameters()
+    );
+  }
+
+  /**
+   * Formats the given SQL by replacing the template variables.
+   */
+  public static String formatSql(String sqlTemplate, Map<String, String> templateVariables)
+  {
+    String sql = sqlTemplate;
+    for (Map.Entry<String, String> variable : templateVariables.entrySet()) {
+      sql = StringUtils.replace(sql, variable.getKey(), variable.getValue());
+    }
+
+    return sql;
+  }
+
+  @Override
+  public boolean equals(Object object)
+  {
+    if (this == object) {
+      return true;
+    }
+    if (object == null || getClass() != object.getClass()) {
+      return false;
+    }
+    MSQCompactionJobTemplate that = (MSQCompactionJobTemplate) object;
+    return Objects.equals(sqlTemplate, that.sqlTemplate)
+           && Objects.equals(targetState, that.targetState);
+  }
+
+  @Override
+  public int hashCode()
+  {
+    return Objects.hash(sqlTemplate, targetState);
+  }
+
+  @Override
+  public String getType()
+  {
+    return TYPE;
+  }
+}
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/compact/OverlordCompactionScheduler.java b/indexing-service/src/main/java/org/apache/druid/indexing/compact/OverlordCompactionScheduler.java
index 95ea3d1169..c3bce6a09d 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/compact/OverlordCompactionScheduler.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/compact/OverlordCompactionScheduler.java
@@ -24,15 +24,18 @@ import com.google.common.base.Optional;
 import com.google.common.base.Supplier;
 import com.google.inject.Inject;
 import org.apache.druid.client.DataSourcesSnapshot;
+import org.apache.druid.client.broker.BrokerClient;
 import org.apache.druid.client.indexing.ClientCompactionRunnerInfo;
 import org.apache.druid.indexer.TaskLocation;
 import org.apache.druid.indexer.TaskStatus;
+import org.apache.druid.indexing.common.actions.TaskActionClientFactory;
+import org.apache.druid.indexing.overlord.GlobalTaskLockbox;
 import org.apache.druid.indexing.overlord.TaskMaster;
 import org.apache.druid.indexing.overlord.TaskQueryTool;
 import org.apache.druid.indexing.overlord.TaskRunner;
 import org.apache.druid.indexing.overlord.TaskRunnerListener;
+import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.java.util.common.Stopwatch;
-import org.apache.druid.java.util.common.concurrent.Execs;
 import org.apache.druid.java.util.common.concurrent.ScheduledExecutorFactory;
 import org.apache.druid.java.util.common.lifecycle.LifecycleStart;
 import org.apache.druid.java.util.common.lifecycle.LifecycleStop;
@@ -40,6 +43,7 @@ import org.apache.druid.java.util.common.logger.Logger;
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
 import org.apache.druid.java.util.emitter.service.ServiceMetricEvent;
 import org.apache.druid.metadata.SegmentsMetadataManager;
+import org.apache.druid.metadata.SegmentsMetadataManagerConfig;
 import org.apache.druid.server.compaction.CompactionRunSimulator;
 import org.apache.druid.server.compaction.CompactionSimulateResult;
 import org.apache.druid.server.compaction.CompactionStatusTracker;
@@ -50,19 +54,20 @@ import org.apache.druid.server.coordinator.CoordinatorOverlordServiceConfig;
 import org.apache.druid.server.coordinator.DataSourceCompactionConfig;
 import org.apache.druid.server.coordinator.DruidCompactionConfig;
 import org.apache.druid.server.coordinator.duty.CompactSegments;
-import org.apache.druid.server.coordinator.stats.CoordinatorRunStats;
 import org.apache.druid.server.coordinator.stats.CoordinatorStat;
-import org.apache.druid.server.coordinator.stats.Dimension;
+import org.apache.druid.server.coordinator.stats.RowKey;
 import org.apache.druid.server.coordinator.stats.Stats;
-import org.joda.time.Duration;
 
-import java.util.ArrayList;
 import java.util.Collections;
+import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.Consumer;
+import java.util.stream.Collectors;
 
 /**
  * Implementation of {@link CompactionScheduler}.
@@ -81,16 +86,31 @@ public class OverlordCompactionScheduler implements CompactionScheduler
 {
   private static final Logger log = new Logger(OverlordCompactionScheduler.class);
 
-  private static final long SCHEDULE_PERIOD_SECONDS = 5;
-  private static final Duration METRIC_EMISSION_PERIOD = Duration.standardMinutes(5);
+  /**
+   * Scheduler run period is 15 minutes. It has been kept high to avoid eager
+   * recomputation of the queue as it may be a very compute-intensive operation
+   * taking upto several minutes on clusters with a large number of used segments.
+   * Jobs for a single supervisor may still be recomputed when the supervisor is updated.
+   */
+  private static final long DEFAULT_SCHEDULE_PERIOD_MILLIS = 15 * 60_000;
 
   private final SegmentsMetadataManager segmentManager;
   private final LocalOverlordClient overlordClient;
+  private final BrokerClient brokerClient;
   private final ServiceEmitter emitter;
+  private final ObjectMapper objectMapper;
   private final TaskMaster taskMaster;
 
   private final Supplier<DruidCompactionConfig> compactionConfigSupplier;
-  private final ConcurrentHashMap<String, DataSourceCompactionConfig> activeDatasourceConfigs;
+  private final ConcurrentHashMap<String, CompactionSupervisor> activeSupervisors;
+
+  private final AtomicReference<Map<String, AutoCompactionSnapshot>> datasourceToCompactionSnapshot;
+  private final AtomicBoolean shouldRecomputeJobsForAnyDatasource = new AtomicBoolean(false);
+
+  /**
+   * Compaction job queue built in the last invocation of {@link #resetCompactionJobQueue()}.
+   */
+  private final AtomicReference<CompactionJobQueue> latestJobQueue;
 
   /**
    * Single-threaded executor to process the compaction queue.
@@ -98,6 +118,9 @@ public class OverlordCompactionScheduler implements CompactionScheduler
   private final ScheduledExecutorService executor;
 
   private final CompactionStatusTracker statusTracker;
+  private final TaskActionClientFactory taskActionClientFactory;
+  private final DruidInputSourceFactory druidInputSourceFactory;
+  private final GlobalTaskLockbox taskLockbox;
 
   /**
    * Listener to watch task completion events and update CompactionStatusTracker.
@@ -107,7 +130,6 @@ public class OverlordCompactionScheduler implements CompactionScheduler
 
   private final AtomicBoolean isLeader = new AtomicBoolean(false);
   private final AtomicBoolean started = new AtomicBoolean(false);
-  private final CompactSegments duty;
 
   /**
    * The scheduler should enable/disable polling of segments only if the Overlord
@@ -115,25 +137,35 @@ public class OverlordCompactionScheduler implements CompactionScheduler
    * class itself.
    */
   private final boolean shouldPollSegments;
-
-  private final Stopwatch sinceStatsEmitted = Stopwatch.createUnstarted();
+  private final long schedulePeriodMillis;
 
   @Inject
   public OverlordCompactionScheduler(
       TaskMaster taskMaster,
+      GlobalTaskLockbox taskLockbox,
       TaskQueryTool taskQueryTool,
       SegmentsMetadataManager segmentManager,
+      SegmentsMetadataManagerConfig segmentManagerConfig,
       Supplier<DruidCompactionConfig> compactionConfigSupplier,
       CompactionStatusTracker statusTracker,
       CoordinatorOverlordServiceConfig coordinatorOverlordServiceConfig,
+      TaskActionClientFactory taskActionClientFactory,
+      DruidInputSourceFactory druidInputSourceFactory,
       ScheduledExecutorFactory executorFactory,
+      BrokerClient brokerClient,
       ServiceEmitter emitter,
       ObjectMapper objectMapper
   )
   {
+    final long segmentPollPeriodMillis =
+        segmentManagerConfig.getPollDuration().toStandardDuration().getMillis();
+    this.schedulePeriodMillis = Math.min(DEFAULT_SCHEDULE_PERIOD_MILLIS, segmentPollPeriodMillis);
+
     this.segmentManager = segmentManager;
     this.emitter = emitter;
+    this.objectMapper = objectMapper;
     this.taskMaster = taskMaster;
+    this.taskLockbox = taskLockbox;
     this.compactionConfigSupplier = compactionConfigSupplier;
 
     this.executor = executorFactory.create(1, "CompactionScheduler-%s");
@@ -141,9 +173,13 @@ public class OverlordCompactionScheduler implements CompactionScheduler
     this.shouldPollSegments = segmentManager != null
                               && !coordinatorOverlordServiceConfig.isEnabled();
     this.overlordClient = new LocalOverlordClient(taskMaster, taskQueryTool, objectMapper);
-    this.duty = new CompactSegments(this.statusTracker, overlordClient);
-    this.activeDatasourceConfigs = new ConcurrentHashMap<>();
+    this.brokerClient = brokerClient;
+    this.activeSupervisors = new ConcurrentHashMap<>();
+    this.datasourceToCompactionSnapshot = new AtomicReference<>();
+    this.latestJobQueue = new AtomicReference<>();
 
+    this.taskActionClientFactory = taskActionClientFactory;
+    this.druidInputSourceFactory = druidInputSourceFactory;
     this.taskRunnerListener = new TaskRunnerListener()
     {
       @Override
@@ -162,7 +198,8 @@ public class OverlordCompactionScheduler implements CompactionScheduler
       public void statusChanged(String taskId, TaskStatus status)
       {
         if (status.isComplete()) {
-          statusTracker.onTaskFinished(taskId, status);
+          onTaskFinished(taskId, status);
+          launchPendingJobs();
         }
       }
     };
@@ -184,7 +221,8 @@ public class OverlordCompactionScheduler implements CompactionScheduler
   public void becomeLeader()
   {
     if (isLeader.compareAndSet(false, true)) {
-      scheduleOnExecutor(this::scheduledRun, SCHEDULE_PERIOD_SECONDS);
+      // Schedule first run after a small delay
+      scheduleOnExecutor(this::scheduledRun, 1_000L);
     }
   }
 
@@ -208,25 +246,31 @@ public class OverlordCompactionScheduler implements CompactionScheduler
     } else {
       return ClientCompactionRunnerInfo.validateCompactionConfig(
           compactionConfig,
-          getLatestConfig().getEngine()
+          getLatestClusterConfig().getEngine()
       );
     }
   }
 
   @Override
-  public void startCompaction(String dataSourceName, DataSourceCompactionConfig config)
+  public void startCompaction(String dataSourceName, CompactionSupervisor supervisor)
   {
-    // Track active datasources even if scheduler has not started yet because
+    // Track active supervisors even if scheduler has not started yet because
     // SupervisorManager is started before the scheduler
     if (isEnabled()) {
-      activeDatasourceConfigs.put(dataSourceName, config);
+      activeSupervisors.put(dataSourceName, supervisor);
+
+      if (started.get()) {
+        shouldRecomputeJobsForAnyDatasource.set(true);
+        scheduleOnExecutor(() -> recreateJobs(dataSourceName, supervisor), 0L);
+      }
     }
   }
 
   @Override
   public void stopCompaction(String dataSourceName)
   {
-    activeDatasourceConfigs.remove(dataSourceName);
+    activeSupervisors.remove(dataSourceName);
+    updateQueueIfComputed(queue -> queue.removeJobs(dataSourceName));
     statusTracker.removeDatasource(dataSourceName);
   }
 
@@ -239,10 +283,10 @@ public class OverlordCompactionScheduler implements CompactionScheduler
       return;
     }
 
-    log.info("Starting compaction scheduler.");
+    log.info("Starting compaction scheduler with period [%d] millis.", schedulePeriodMillis);
     final Optional<TaskRunner> taskRunnerOptional = taskMaster.getTaskRunner();
     if (taskRunnerOptional.isPresent()) {
-      taskRunnerOptional.get().registerListener(taskRunnerListener, Execs.directExecutor());
+      taskRunnerOptional.get().registerListener(taskRunnerListener, executor);
     }
     if (shouldPollSegments) {
       segmentManager.startPollingDatabasePeriodically();
@@ -264,7 +308,8 @@ public class OverlordCompactionScheduler implements CompactionScheduler
       taskRunnerOptional.get().unregisterListener(taskRunnerListener.getListenerId());
     }
     statusTracker.stop();
-    activeDatasourceConfigs.clear();
+    activeSupervisors.clear();
+    latestJobQueue.set(null);
 
     if (shouldPollSegments) {
       segmentManager.stopPollingDatabasePeriodically();
@@ -291,54 +336,119 @@ public class OverlordCompactionScheduler implements CompactionScheduler
     if (isEnabled()) {
       initState();
       try {
-        runCompactionDuty();
+        resetCompactionJobQueue();
       }
       catch (Exception e) {
         log.error(e, "Error processing compaction queue. Continuing schedule.");
       }
-      scheduleOnExecutor(this::scheduledRun, SCHEDULE_PERIOD_SECONDS);
+      scheduleOnExecutor(this::scheduledRun, schedulePeriodMillis);
     } else {
       cleanupState();
-      scheduleOnExecutor(this::scheduledRun, SCHEDULE_PERIOD_SECONDS * 4);
+      scheduleOnExecutor(this::scheduledRun, schedulePeriodMillis);
     }
   }
 
   /**
-   * Runs the compaction duty and emits stats if {@link #METRIC_EMISSION_PERIOD}
-   * has elapsed.
+   * Creates and launches eligible compaction jobs.
    */
-  private synchronized void runCompactionDuty()
+  private synchronized void resetCompactionJobQueue()
   {
-    final CoordinatorRunStats stats = new CoordinatorRunStats();
-    duty.run(getLatestConfig(), getDatasourceSnapshot(), getLatestConfig().getEngine(), stats);
-
-    // Emit stats only if emission period has elapsed
-    if (!sinceStatsEmitted.isRunning() || sinceStatsEmitted.hasElapsed(METRIC_EMISSION_PERIOD)) {
-      stats.forEachStat(
-          (stat, dimensions, value) -> {
-            if (stat.shouldEmit()) {
-              emitStat(stat, dimensions.getValues(), value);
-            }
-          }
-      );
-      sinceStatsEmitted.restart();
-    } else {
-      // Always emit number of submitted tasks
-      long numSubmittedTasks = stats.get(Stats.Compaction.SUBMITTED_TASKS);
-      emitStat(Stats.Compaction.SUBMITTED_TASKS, Collections.emptyMap(), numSubmittedTasks);
+    // Remove the old queue so that no more jobs are added to it
+    latestJobQueue.set(null);
+
+    final Stopwatch runDuration = Stopwatch.createStarted();
+    final DataSourcesSnapshot dataSourcesSnapshot = getDatasourceSnapshot();
+    final CompactionJobQueue queue = new CompactionJobQueue(
+        dataSourcesSnapshot,
+        getLatestClusterConfig(),
+        statusTracker,
+        taskActionClientFactory,
+        taskLockbox,
+        overlordClient,
+        brokerClient,
+        objectMapper
+    );
+    latestJobQueue.set(queue);
+
+    statusTracker.resetActiveDatasources(activeSupervisors.keySet());
+    statusTracker.onSegmentTimelineUpdated(dataSourcesSnapshot.getSnapshotTime());
+
+    // Jobs for all active supervisors are being freshly created
+    // recomputation will not be needed
+    shouldRecomputeJobsForAnyDatasource.set(false);
+    activeSupervisors.forEach(this::createAndEnqueueJobs);
+
+    launchPendingJobs();
+    queue.getRunStats().forEachStat(this::emitStat);
+    emitStat(Stats.Compaction.SCHEDULER_RUN_TIME, RowKey.empty(), runDuration.millisElapsed());
+  }
+
+  /**
+   * Launches pending compaction jobs if compaction task slots become available.
+   * This method uses the jobs created by the last invocation of {@link #resetCompactionJobQueue()}.
+   */
+  private synchronized void launchPendingJobs()
+  {
+    updateQueueIfComputed(queue -> {
+      queue.runReadyJobs();
+      updateCompactionSnapshots(queue);
+    });
+  }
+
+  private synchronized void recreateJobs(String dataSource, CompactionSupervisor supervisor)
+  {
+    if (shouldRecomputeJobsForAnyDatasource.get()) {
+      createAndEnqueueJobs(dataSource, supervisor);
+    }
+  }
+
+  private synchronized void createAndEnqueueJobs(String dataSource, CompactionSupervisor supervisor)
+  {
+    updateQueueIfComputed(
+        queue -> queue.createAndEnqueueJobs(
+            supervisor,
+            druidInputSourceFactory.create(dataSource, Intervals.ETERNITY)
+        )
+    );
+  }
+
+  /**
+   * Performs an operation on the {@link #latestJobQueue} if it has been already
+   * computed.
+   */
+  private void updateQueueIfComputed(Consumer<CompactionJobQueue> operation)
+  {
+    final CompactionJobQueue queue = latestJobQueue.get();
+    if (queue != null) {
+      operation.accept(queue);
     }
   }
 
+  private void onTaskFinished(String taskId, TaskStatus taskStatus)
+  {
+    statusTracker.onTaskFinished(taskId, taskStatus);
+
+    updateQueueIfComputed(queue -> {
+      queue.onTaskFinished(taskId, taskStatus);
+      updateCompactionSnapshots(queue);
+    });
+  }
+
+  private void updateCompactionSnapshots(CompactionJobQueue queue)
+  {
+    datasourceToCompactionSnapshot.set(queue.getSnapshots());
+  }
+
   @Override
   public AutoCompactionSnapshot getCompactionSnapshot(String dataSource)
   {
-    if (!activeDatasourceConfigs.containsKey(dataSource)) {
+    if (!activeSupervisors.containsKey(dataSource)) {
       return AutoCompactionSnapshot.builder(dataSource)
                                    .withStatus(AutoCompactionSnapshot.ScheduleStatus.NOT_ENABLED)
                                    .build();
     }
 
-    final AutoCompactionSnapshot snapshot = duty.getAutoCompactionSnapshot(dataSource);
+    final AutoCompactionSnapshot snapshot = datasourceToCompactionSnapshot.get().get(dataSource);
     if (snapshot == null) {
       final AutoCompactionSnapshot.ScheduleStatus status =
           isEnabled()
@@ -353,7 +463,7 @@ public class OverlordCompactionScheduler implements CompactionScheduler
   @Override
   public Map<String, AutoCompactionSnapshot> getAllCompactionSnapshots()
   {
-    return duty.getAutoCompactionSnapshot();
+    return Map.copyOf(datasourceToCompactionSnapshot.get());
   }
 
   @Override
@@ -363,17 +473,21 @@ public class OverlordCompactionScheduler implements CompactionScheduler
       return new CompactionRunSimulator(statusTracker, overlordClient).simulateRunWithConfig(
           getLatestConfig().withClusterConfig(updateRequest),
           getDatasourceSnapshot(),
-          getLatestConfig().getEngine()
+          updateRequest.getEngine()
       );
     } else {
       return new CompactionSimulateResult(Collections.emptyMap());
     }
   }
 
-  private void emitStat(CoordinatorStat stat, Map<Dimension, String> dimensionValues, long value)
+  private void emitStat(CoordinatorStat stat, RowKey rowKey, long value)
   {
+    if (!stat.shouldEmit()) {
+      return;
+    }
+
     ServiceMetricEvent.Builder eventBuilder = new ServiceMetricEvent.Builder();
-    dimensionValues.forEach(
+    rowKey.getValues().forEach(
         (dim, dimValue) -> eventBuilder.setDimension(dim.reportedName(), dimValue)
     );
     emitter.emit(eventBuilder.setMetric(stat.getMetricName(), value));
@@ -381,10 +495,20 @@ public class OverlordCompactionScheduler implements CompactionScheduler
 
   private DruidCompactionConfig getLatestConfig()
   {
+    final List<DataSourceCompactionConfig> configs = activeSupervisors
+        .values()
+        .stream()
+        .map(s -> s.getSpec().getSpec())
+        .collect(Collectors.toList());
     return DruidCompactionConfig
         .empty()
-        .withClusterConfig(compactionConfigSupplier.get().clusterConfig())
-        .withDatasourceConfigs(new ArrayList<>(activeDatasourceConfigs.values()));
+        .withClusterConfig(getLatestClusterConfig())
+        .withDatasourceConfigs(configs);
+  }
+
+  private ClusterCompactionConfig getLatestClusterConfig()
+  {
+    return compactionConfigSupplier.get().clusterConfig();
   }
 
   private DataSourcesSnapshot getDatasourceSnapshot()
@@ -392,7 +516,7 @@ public class OverlordCompactionScheduler implements CompactionScheduler
     return segmentManager.getRecentDataSourcesSnapshot();
   }
 
-  private void scheduleOnExecutor(Runnable runnable, long delaySeconds)
+  private void scheduleOnExecutor(Runnable runnable, long delayMillis)
   {
     executor.schedule(
         () -> {
@@ -403,8 +527,8 @@ public class OverlordCompactionScheduler implements CompactionScheduler
             log.error(t, "Error while executing runnable");
           }
         },
-        delaySeconds,
-        TimeUnit.SECONDS
+        delayMillis,
+        TimeUnit.MILLISECONDS
     );
   }
 }
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java b/indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java
index 84c17f1a8f..fea6f30ef4 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java
@@ -291,6 +291,25 @@ public class DruidInputSource extends AbstractInputSource implements SplittableI
     );
   }
 
+  /**
+   * Creates a new {@link DruidInputSource} with the given interval.
+   */
+  public DruidInputSource withInterval(Interval interval)
+  {
+    return new DruidInputSource(
+        this.dataSource,
+        interval,
+        null,
+        this.dimFilter,
+        this.dimensions,
+        this.metrics,
+        this.indexIO,
+        this.coordinatorClient,
+        this.segmentCacheManagerFactory,
+        this.taskConfig
+    );
+  }
+
   @Override
   protected InputSourceReader fixedFormatReader(InputRowSchema inputRowSchema, @Nullable File temporaryDirectory)
   {
diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/overlord/TaskQueue.java b/indexing-service/src/main/java/org/apache/druid/indexing/overlord/TaskQueue.java
index 0cfe481a8f..c923e70fca 100644
--- a/indexing-service/src/main/java/org/apache/druid/indexing/overlord/TaskQueue.java
+++ b/indexing-service/src/main/java/org/apache/druid/indexing/overlord/TaskQueue.java
@@ -1023,6 +1023,19 @@ public class TaskQueue
   }
 
   /**
+   * List of all active tasks currently being managed by this TaskQueue.
+   */
+  public List<Task> getActiveTasks()
+  {
+    return activeTasks.values()
+                      .stream()
+                      .filter(entry -> !entry.isComplete)
+                      .map(TaskEntry::getTask)
+                      .collect(Collectors.toList());
+  }
+
+  /**
+   * Returns the list of currently active tasks for the given datasource.
    * List of all active and completed task infos currently being managed by this TaskQueue.
    */
   public List<TaskInfo> getTaskInfos()
diff --git a/indexing-service/src/test/java/org/apache/druid/indexing/compact/CompactionSupervisorSpecTest.java b/indexing-service/src/test/java/org/apache/druid/indexing/compact/CompactionSupervisorSpecTest.java
index b1732acabb..b008d74829 100644
--- a/indexing-service/src/test/java/org/apache/druid/indexing/compact/CompactionSupervisorSpecTest.java
+++ b/indexing-service/src/test/java/org/apache/druid/indexing/compact/CompactionSupervisorSpecTest.java
@@ -137,7 +137,7 @@ public class CompactionSupervisorSpecTest
     supervisor.start();
     supervisor.stop(false);
 
-    Mockito.verify(scheduler, Mockito.times(1)).startCompaction(TestDataSource.WIKI, spec);
+    Mockito.verify(scheduler, Mockito.times(1)).startCompaction(TestDataSource.WIKI, supervisor);
     Mockito.verify(scheduler, Mockito.times(1)).stopCompaction(TestDataSource.WIKI);
   }
 
@@ -157,7 +157,7 @@ public class CompactionSupervisorSpecTest
     supervisor.start();
     supervisor.stop(false);
 
-    Mockito.verify(scheduler, Mockito.times(1)).startCompaction(TestDataSource.WIKI, spec);
+    Mockito.verify(scheduler, Mockito.times(1)).startCompaction(TestDataSource.WIKI, supervisor);
     Mockito.verify(scheduler, Mockito.times(1)).stopCompaction(TestDataSource.WIKI);
   }
 
diff --git a/indexing-service/src/test/java/org/apache/druid/indexing/compact/OverlordCompactionSchedulerTest.java b/indexing-service/src/test/java/org/apache/druid/indexing/compact/OverlordCompactionSchedulerTest.java
index 9353953fc2..1484a719db 100644
--- a/indexing-service/src/test/java/org/apache/druid/indexing/compact/OverlordCompactionSchedulerTest.java
+++ b/indexing-service/src/test/java/org/apache/druid/indexing/compact/OverlordCompactionSchedulerTest.java
@@ -21,15 +21,31 @@ package org.apache.druid.indexing.compact;
 
 import com.fasterxml.jackson.databind.InjectableValues;
 import com.fasterxml.jackson.databind.ObjectMapper;
+import com.google.common.base.Optional;
+import com.google.common.util.concurrent.Futures;
+import org.apache.druid.client.broker.BrokerClient;
+import org.apache.druid.client.coordinator.NoopCoordinatorClient;
 import org.apache.druid.client.indexing.ClientMSQContext;
+import org.apache.druid.common.utils.IdUtils;
 import org.apache.druid.guice.IndexingServiceTuningConfigModule;
+import org.apache.druid.guice.SupervisorModule;
 import org.apache.druid.indexer.CompactionEngine;
+import org.apache.druid.indexer.TaskState;
+import org.apache.druid.indexer.TaskStatus;
 import org.apache.druid.indexing.common.SegmentCacheManagerFactory;
+import org.apache.druid.indexing.common.TimeChunkLock;
+import org.apache.druid.indexing.common.actions.RetrieveUsedSegmentsAction;
+import org.apache.druid.indexing.common.actions.TaskAction;
+import org.apache.druid.indexing.common.actions.TaskActionClient;
+import org.apache.druid.indexing.common.actions.TaskActionClientFactory;
+import org.apache.druid.indexing.common.actions.TimeChunkLockTryAcquireAction;
+import org.apache.druid.indexing.common.config.TaskConfig;
 import org.apache.druid.indexing.common.config.TaskStorageConfig;
 import org.apache.druid.indexing.common.task.CompactionTask;
 import org.apache.druid.indexing.common.task.Task;
 import org.apache.druid.indexing.overlord.GlobalTaskLockbox;
 import org.apache.druid.indexing.overlord.HeapMemoryTaskStorage;
+import org.apache.druid.indexing.overlord.Segments;
 import org.apache.druid.indexing.overlord.TaskMaster;
 import org.apache.druid.indexing.overlord.TaskQueryTool;
 import org.apache.druid.indexing.overlord.TaskQueue;
@@ -38,9 +54,16 @@ import org.apache.druid.indexing.overlord.setup.DefaultWorkerBehaviorConfig;
 import org.apache.druid.indexing.overlord.setup.WorkerBehaviorConfig;
 import org.apache.druid.indexing.test.TestIndexerMetadataStorageCoordinator;
 import org.apache.druid.jackson.DefaultObjectMapper;
+import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.java.util.common.Intervals;
+import org.apache.druid.java.util.common.StringUtils;
 import org.apache.druid.java.util.common.granularity.Granularities;
+import org.apache.druid.java.util.common.granularity.Granularity;
 import org.apache.druid.java.util.metrics.StubServiceEmitter;
+import org.apache.druid.metadata.SegmentsMetadataManager;
+import org.apache.druid.metadata.SegmentsMetadataManagerConfig;
+import org.apache.druid.query.http.ClientSqlQuery;
+import org.apache.druid.query.http.SqlTaskStatus;
 import org.apache.druid.segment.TestDataSource;
 import org.apache.druid.segment.TestIndex;
 import org.apache.druid.server.compaction.CompactionSimulateResult;
@@ -56,11 +79,15 @@ import org.apache.druid.server.coordinator.CreateDataSegments;
 import org.apache.druid.server.coordinator.DataSourceCompactionConfig;
 import org.apache.druid.server.coordinator.DruidCompactionConfig;
 import org.apache.druid.server.coordinator.InlineSchemaDataSourceCompactionConfig;
+import org.apache.druid.server.coordinator.UserCompactionTaskGranularityConfig;
+import org.apache.druid.server.coordinator.duty.CompactSegments;
 import org.apache.druid.server.coordinator.simulate.BlockingExecutorService;
-import org.apache.druid.server.coordinator.simulate.TestSegmentsMetadataManager;
 import org.apache.druid.server.coordinator.simulate.WrappingScheduledExecutorService;
 import org.apache.druid.server.coordinator.stats.Stats;
 import org.apache.druid.timeline.DataSegment;
+import org.joda.time.DateTime;
+import org.joda.time.Duration;
+import org.joda.time.Interval;
 import org.joda.time.Period;
 import org.junit.Assert;
 import org.junit.Before;
@@ -71,7 +98,10 @@ import org.mockito.Mockito;
 
 import java.util.Arrays;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.atomic.AtomicReference;
 
 public class OverlordCompactionSchedulerTest
@@ -81,6 +111,7 @@ public class OverlordCompactionSchedulerTest
   static {
     OBJECT_MAPPER = new DefaultObjectMapper();
     OBJECT_MAPPER.registerModules(new IndexingServiceTuningConfigModule().getJacksonModules());
+    OBJECT_MAPPER.registerModules(new SupervisorModule().getJacksonModules());
     OBJECT_MAPPER.setInjectableValues(
         new InjectableValues
             .Std()
@@ -91,25 +122,53 @@ public class OverlordCompactionSchedulerTest
     );
   }
 
+  private static final DateTime JAN_20 = DateTimes.of("2025-01-20");
+  private static final DateTime MAR_11 = DateTimes.of("2025-03-11");
+
   private AtomicReference<ClusterCompactionConfig> compactionConfig;
   private CoordinatorOverlordServiceConfig coordinatorOverlordServiceConfig;
 
   private TaskMaster taskMaster;
   private TaskQueue taskQueue;
+  private BrokerClient brokerClient;
+  private TaskActionClientFactory taskActionClientFactory;
   private BlockingExecutorService executor;
 
   private HeapMemoryTaskStorage taskStorage;
-  private TestSegmentsMetadataManager segmentsMetadataManager;
+  private TestIndexerMetadataStorageCoordinator segmentStorage;
+  private SegmentsMetadataManager segmentsMetadataManager;
   private StubServiceEmitter serviceEmitter;
 
+  private String dataSource;
   private OverlordCompactionScheduler scheduler;
 
+  private Map<Interval, String> submittedMsqTaskIds;
+
   @Before
   public void setUp()
   {
+    dataSource = "wiki_" + IdUtils.getRandomId();
+
     final TaskRunner taskRunner = Mockito.mock(TaskRunner.class);
+    Mockito.when(taskRunner.getTotalCapacity()).thenReturn(100);
+    Mockito.when(taskRunner.getMaximumCapacityWithAutoscale()).thenReturn(100);
+
     taskQueue = Mockito.mock(TaskQueue.class);
 
+    submittedMsqTaskIds = new HashMap<>();
+    brokerClient = Mockito.mock(BrokerClient.class);
+    Mockito.when(brokerClient.submitSqlTask(ArgumentMatchers.any(ClientSqlQuery.class))).thenAnswer(
+        arg -> {
+          final ClientSqlQuery query = arg.getArgument(0);
+          final Interval compactionInterval =
+              (Interval) query.getContext().get(CompactSegments.COMPACTION_INTERVAL_KEY);
+
+          final String taskId = IdUtils.getRandomId();
+          submittedMsqTaskIds.put(compactionInterval, taskId);
+          return Futures.immediateFuture(new SqlTaskStatus(taskId, TaskState.RUNNING, null));
+        }
+    );
+
     taskMaster = new TaskMaster(null, null);
     Assert.assertFalse(taskMaster.isHalfOrFullLeader());
     Assert.assertFalse(taskMaster.isFullLeader());
@@ -126,11 +185,39 @@ public class OverlordCompactionSchedulerTest
 
     executor = new BlockingExecutorService("test");
     serviceEmitter = new StubServiceEmitter();
-    segmentsMetadataManager = new TestSegmentsMetadataManager();
+    segmentStorage = new TestIndexerMetadataStorageCoordinator();
+    segmentsMetadataManager = segmentStorage.getManager();
 
-    compactionConfig = new AtomicReference<>(new ClusterCompactionConfig(null, null, null, true, null));
+    compactionConfig = new AtomicReference<>(new ClusterCompactionConfig(1.0, 100, null, true, null));
     coordinatorOverlordServiceConfig = new CoordinatorOverlordServiceConfig(false, null);
 
+    taskActionClientFactory = task -> new TaskActionClient()
+    {
+      @Override
+      @SuppressWarnings("unchecked")
+      public <RetType> RetType submit(TaskAction<RetType> taskAction)
+      {
+        if (taskAction instanceof RetrieveUsedSegmentsAction) {
+          return (RetType) segmentStorage.retrieveAllUsedSegments(
+              ((RetrieveUsedSegmentsAction) taskAction).getDataSource(),
+              Segments.ONLY_VISIBLE
+          );
+        } else if (taskAction instanceof TimeChunkLockTryAcquireAction) {
+          final TimeChunkLockTryAcquireAction lockAcquireAction = (TimeChunkLockTryAcquireAction) taskAction;
+          return (RetType) new TimeChunkLock(
+              null,
+              task.getGroupId(),
+              task.getDataSource(),
+              lockAcquireAction.getInterval(),
+              DateTimes.nowUtc().toString(),
+              1
+          );
+        } else {
+          return null;
+        }
+      }
+    };
+
     initScheduler();
   }
 
@@ -142,19 +229,29 @@ public class OverlordCompactionSchedulerTest
         = new DefaultWorkerBehaviorConfig(WorkerBehaviorConfig.DEFAULT_STRATEGY, null);
     scheduler = new OverlordCompactionScheduler(
         taskMaster,
+        taskLockbox,
         new TaskQueryTool(taskStorage, taskLockbox, taskMaster, null, () -> defaultWorkerConfig),
         segmentsMetadataManager,
+        new SegmentsMetadataManagerConfig(null, null, null),
         () -> DruidCompactionConfig.empty().withClusterConfig(compactionConfig.get()),
-        new CompactionStatusTracker(OBJECT_MAPPER),
+        new CompactionStatusTracker(),
         coordinatorOverlordServiceConfig,
+        taskActionClientFactory,
+        new DruidInputSourceFactory(
+            TestIndex.INDEX_IO,
+            Mockito.mock(TaskConfig.class),
+            new NoopCoordinatorClient(),
+            new SegmentCacheManagerFactory(TestIndex.INDEX_IO, OBJECT_MAPPER)
+        ),
         (nameFormat, numThreads) -> new WrappingScheduledExecutorService("test", executor, false),
+        brokerClient,
         serviceEmitter,
         OBJECT_MAPPER
     );
   }
 
   @Test
-  public void testBecomeLeader_triggersStart_ifEnabled()
+  public void test_becomeLeader_triggersStart_ifEnabled()
   {
     Assert.assertTrue(scheduler.isEnabled());
 
@@ -168,7 +265,7 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testBecomeLeader_doesNotTriggerStart_ifDisabled()
+  public void test_becomeLeader_doesNotTriggerStart_ifDisabled()
   {
     disableScheduler();
     Assert.assertFalse(scheduler.isEnabled());
@@ -182,7 +279,7 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testStopBeingLeader_triggersStop()
+  public void test_stopBeingLeader_triggersStop()
   {
     Assert.assertFalse(scheduler.isRunning());
 
@@ -198,7 +295,7 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testDisablingScheduler_triggersStop()
+  public void test_disableSupervisors_triggersStop()
   {
     // Start scheduler
     scheduler.becomeLeader();
@@ -216,7 +313,7 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testEnablingScheduler_triggersStart()
+  public void test_enableSupervisors_triggersStart()
   {
     disableScheduler();
 
@@ -235,7 +332,7 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testSegmentsAreNotPolled_ifSupervisorsAreDisabled()
+  public void test_disableSupervisors_disablesSegmentPolling()
   {
     disableScheduler();
 
@@ -243,7 +340,7 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testSegmentsArePolled_whenRunningInStandaloneMode()
+  public void test_enableSupervisors_inStandaloneMode_enablesSegmentPolling()
   {
     coordinatorOverlordServiceConfig = new CoordinatorOverlordServiceConfig(false, null);
     initScheduler();
@@ -252,7 +349,7 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testSegmentsAreNotPolled_whenRunningInCoordinatorMode()
+  public void test_enableSupervisors_inCoordinatorMode_disablesSegmentPolling()
   {
     coordinatorOverlordServiceConfig = new CoordinatorOverlordServiceConfig(true, "overlord");
     initScheduler();
@@ -272,7 +369,7 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testNullCompactionConfigIsInvalid()
+  public void test_validateCompactionConfig_returnsInvalid_forNullConfig()
   {
     final CompactionConfigValidationResult result = scheduler.validateCompactionConfig(null);
     Assert.assertFalse(result.isValid());
@@ -280,11 +377,11 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testMsqCompactionConfigWithOneMaxTasksIsInvalid()
+  public void test_validateCompactionConfig_returnsInvalid_forMSQConfigWithOneMaxTasks()
   {
     final DataSourceCompactionConfig datasourceConfig = InlineSchemaDataSourceCompactionConfig
         .builder()
-        .forDataSource(TestDataSource.WIKI)
+        .forDataSource(dataSource)
         .withEngine(CompactionEngine.MSQ)
         .withTaskContext(Collections.singletonMap(ClientMSQContext.CTX_MAX_NUM_TASKS, 1))
         .build();
@@ -298,75 +395,50 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testStartCompaction()
+  public void test_startCompaction_enablesTaskSubmission_forDatasource()
   {
-    final List<DataSegment> wikiSegments = CreateDataSegments.ofDatasource(TestDataSource.WIKI).eachOfSizeInMb(100);
-    wikiSegments.forEach(segmentsMetadataManager::addSegment);
+    createSegments(1, Granularities.DAY, JAN_20);
 
     scheduler.becomeLeader();
-    scheduler.startCompaction(
-        TestDataSource.WIKI,
-        InlineSchemaDataSourceCompactionConfig.builder()
-                                              .forDataSource(TestDataSource.WIKI)
-                                              .withSkipOffsetFromLatest(Period.seconds(0))
-                                              .build()
-    );
-
-    executor.finishNextPendingTask();
+    scheduler.startCompaction(dataSource, createSupervisorWithInlineSpec());
 
-    ArgumentCaptor<Task> taskArgumentCaptor = ArgumentCaptor.forClass(Task.class);
-    Mockito.verify(taskQueue, Mockito.times(1)).add(taskArgumentCaptor.capture());
-
-    Task submittedTask = taskArgumentCaptor.getValue();
-    Assert.assertNotNull(submittedTask);
-    Assert.assertTrue(submittedTask instanceof CompactionTask);
-
-    final CompactionTask compactionTask = (CompactionTask) submittedTask;
-    Assert.assertEquals(TestDataSource.WIKI, compactionTask.getDataSource());
+    runCompactionTasks(1);
 
-    final AutoCompactionSnapshot.Builder expectedSnapshot = AutoCompactionSnapshot.builder(TestDataSource.WIKI);
-    expectedSnapshot.incrementCompactedStats(CompactionStatistics.create(100_000_000, 1, 1));
+    final AutoCompactionSnapshot.Builder expectedSnapshot = AutoCompactionSnapshot.builder(dataSource);
+    expectedSnapshot.incrementWaitingStats(CompactionStatistics.create(100_000_000, 1, 1));
 
     Assert.assertEquals(
         expectedSnapshot.build(),
-        scheduler.getCompactionSnapshot(TestDataSource.WIKI)
+        scheduler.getCompactionSnapshot(dataSource)
     );
     Assert.assertEquals(
-        Collections.singletonMap(TestDataSource.WIKI, expectedSnapshot.build()),
+        Map.of(dataSource, expectedSnapshot.build()),
         scheduler.getAllCompactionSnapshots()
     );
 
     serviceEmitter.verifyValue(Stats.Compaction.SUBMITTED_TASKS.getMetricName(), 1L);
-    serviceEmitter.verifyValue(Stats.Compaction.COMPACTED_BYTES.getMetricName(), 100_000_000L);
+    serviceEmitter.verifyValue(Stats.Compaction.PENDING_BYTES.getMetricName(), 100_000_000L);
 
     scheduler.stopBeingLeader();
   }
 
   @Test
-  public void testStopCompaction()
+  public void test_stopCompaction_disablesTaskSubmission_forDatasource()
   {
-    final List<DataSegment> wikiSegments = CreateDataSegments.ofDatasource(TestDataSource.WIKI).eachOfSizeInMb(100);
-    wikiSegments.forEach(segmentsMetadataManager::addSegment);
+    createSegments(1, Granularities.DAY, JAN_20);
 
     scheduler.becomeLeader();
-    scheduler.startCompaction(
-        TestDataSource.WIKI,
-        InlineSchemaDataSourceCompactionConfig.builder()
-                                              .forDataSource(TestDataSource.WIKI)
-                                              .withSkipOffsetFromLatest(Period.seconds(0))
-                                              .build()
-    );
-    scheduler.stopCompaction(TestDataSource.WIKI);
-
-    executor.finishNextPendingTask();
+    scheduler.startCompaction(dataSource, createSupervisorWithInlineSpec());
+    scheduler.stopCompaction(dataSource);
 
+    runScheduledJob();
     Mockito.verify(taskQueue, Mockito.never()).add(ArgumentMatchers.any());
 
     Assert.assertEquals(
-        AutoCompactionSnapshot.builder(TestDataSource.WIKI)
+        AutoCompactionSnapshot.builder(dataSource)
                               .withStatus(AutoCompactionSnapshot.ScheduleStatus.NOT_ENABLED)
                               .build(),
-        scheduler.getCompactionSnapshot(TestDataSource.WIKI)
+        scheduler.getCompactionSnapshot(dataSource)
     );
     Assert.assertTrue(scheduler.getAllCompactionSnapshots().isEmpty());
 
@@ -377,26 +449,14 @@ public class OverlordCompactionSchedulerTest
   }
 
   @Test
-  public void testSimulateRun()
+  public void test_simulateRunWithConfigUpdate()
   {
-    final List<DataSegment> wikiSegments = CreateDataSegments
-        .ofDatasource(TestDataSource.WIKI)
-        .forIntervals(1, Granularities.DAY)
-        .startingAt("2013-01-01")
-        .withNumPartitions(10)
-        .eachOfSizeInMb(100);
-    wikiSegments.forEach(segmentsMetadataManager::addSegment);
+    createSegments(1, Granularities.DAY, DateTimes.of("2013-01-01"));
 
     scheduler.becomeLeader();
     runScheduledJob();
 
-    scheduler.startCompaction(
-        TestDataSource.WIKI,
-        InlineSchemaDataSourceCompactionConfig.builder()
-                                              .forDataSource(TestDataSource.WIKI)
-                                              .withSkipOffsetFromLatest(Period.seconds(0))
-                                              .build()
-    );
+    scheduler.startCompaction(dataSource, createSupervisorWithInlineSpec());
 
     final CompactionSimulateResult simulateResult = scheduler.simulateRunWithConfigUpdate(
         new ClusterCompactionConfig(null, null, null, null, null)
@@ -410,10 +470,10 @@ public class OverlordCompactionSchedulerTest
     Assert.assertEquals(
         Collections.singletonList(
             Arrays.asList(
-                TestDataSource.WIKI,
+                dataSource,
                 Intervals.of("2013-01-01/P1D"),
-                10,
-                1_000_000_000L,
+                1,
+                100_000_000L,
                 1,
                 "not compacted yet"
             )
@@ -421,7 +481,7 @@ public class OverlordCompactionSchedulerTest
         pendingCompactionTable.getRows()
     );
 
-    scheduler.stopCompaction(TestDataSource.WIKI);
+    scheduler.stopCompaction(dataSource);
 
     final CompactionSimulateResult simulateResultWhenDisabled = scheduler.simulateRunWithConfigUpdate(
         new ClusterCompactionConfig(null, null, null, null, null)
@@ -431,6 +491,255 @@ public class OverlordCompactionSchedulerTest
     scheduler.stopBeingLeader();
   }
 
+  @Test
+  public void test_ingestHourGranularity_andCompactToDayAndMonth_withInlineTemplates()
+  {
+    final int numDays = (int) new Duration(MAR_11.getMillis() - JAN_20.getMillis()).getStandardDays();
+    createSegments(24 * numDays, Granularities.HOUR, JAN_20);
+    verifyNumSegmentsWith(Granularities.HOUR, 24 * numDays);
+
+    // Compact everything going back to Mar 10 to DAY granularity, rest to MONTH
+    final DateTime now = DateTimes.nowUtc();
+    final Period dayRulePeriod = new Period(now.getMillis() - MAR_11.minusDays(1).minusMinutes(1).getMillis());
+    CascadingCompactionTemplate cascadingTemplate = new CascadingCompactionTemplate(
+        dataSource,
+        List.of(
+            new CompactionRule(dayRulePeriod, new InlineCompactionJobTemplate(createMatcher(Granularities.DAY))),
+            new CompactionRule(Period.ZERO, new InlineCompactionJobTemplate(createMatcher(Granularities.MONTH)))
+        )
+    );
+
+    startCompactionWithSpec(cascadingTemplate);
+    runCompactionTasks(12);
+
+    verifyFullyCompacted();
+    verifyNumSegmentsWith(Granularities.HOUR, 0);
+    verifyNumSegmentsWith(Granularities.DAY, 10);
+    verifyNumSegmentsWith(Granularities.MONTH, 2);
+  }
+
+  @Test
+  public void test_ingestHourGranularity_andCompactToDayAndMonth_withCatalogTemplates()
+  {
+    final int numDays = (int) new Duration(MAR_11.getMillis() - JAN_20.getMillis()).getStandardDays();
+    createSegments(24 * numDays, Granularities.HOUR, JAN_20);
+    verifyNumSegmentsWith(Granularities.HOUR, 24 * numDays);
+
+    // Add compaction templates to catalog
+    final CompactionJobTemplate dayGranularityTemplate =
+        new InlineCompactionJobTemplate(createMatcher(Granularities.DAY));
+    final CompactionJobTemplate monthGranularityTemplate =
+        new InlineCompactionJobTemplate(createMatcher(Granularities.MONTH));
+
+    // Compact everything going back to Mar 10 to DAY granularity, rest to MONTH
+    final DateTime now = DateTimes.nowUtc();
+    final Period dayRulePeriod = new Period(now.getMillis() - MAR_11.minusDays(1).minusMinutes(1).getMillis());
+    CascadingCompactionTemplate cascadingTemplate = new CascadingCompactionTemplate(
+        dataSource,
+        List.of(
+            new CompactionRule(dayRulePeriod, dayGranularityTemplate),
+            new CompactionRule(Period.ZERO, monthGranularityTemplate)
+        )
+    );
+
+    startCompactionWithSpec(cascadingTemplate);
+    runCompactionTasks(12);
+
+    verifyFullyCompacted();
+    verifyNumSegmentsWith(Granularities.HOUR, 0);
+    verifyNumSegmentsWith(Granularities.DAY, 10);
+    verifyNumSegmentsWith(Granularities.MONTH, 2);
+  }
+
+  @Test
+  public void test_ingestHourGranularity_andCompactToDayAndMonth_withCatalogMSQTemplates()
+  {
+    dataSource = TestDataSource.WIKI;
+
+    final int numDays = (int) new Duration(MAR_11.getMillis() - JAN_20.getMillis()).getStandardDays();
+    createSegments(24 * numDays, Granularities.HOUR, JAN_20);
+    verifyNumSegmentsWith(Granularities.HOUR, 24 * numDays);
+
+    // Add compaction templates to catalog
+    final String sqlDayGranularity =
+        "REPLACE INTO ${dataSource}"
+        + " OVERWRITE WHERE __time >= TIMESTAMP '${startTimestamp}' AND __time < TIMESTAMP '${endTimestamp}'"
+        + " SELECT * FROM ${dataSource}"
+        + " WHERE __time BETWEEN '${startTimestamp}' AND '${endTimestamp}'"
+        + " PARTITIONED BY DAY";
+    final CompactionJobTemplate dayGranularityTemplate = new MSQCompactionJobTemplate(
+        new ClientSqlQuery(sqlDayGranularity, null, false, false, false, null, null),
+        createMatcher(Granularities.DAY)
+    );
+    final String sqlMonthGranularity =
+        "REPLACE INTO ${dataSource}"
+        + " OVERWRITE WHERE __time >= TIMESTAMP '${startTimestamp}' AND __time < TIMESTAMP '${endTimestamp}'"
+        + " SELECT * FROM ${dataSource}"
+        + " WHERE __time BETWEEN '${startTimestamp}' AND '${endTimestamp}'"
+        + " PARTITIONED BY MONTH";
+    final CompactionJobTemplate monthGranularityTemplate = new MSQCompactionJobTemplate(
+        new ClientSqlQuery(sqlMonthGranularity, null, false, false, false, null, null),
+        createMatcher(Granularities.MONTH)
+    );
+
+    // Compact everything going back to Mar 10 to DAY granularity, rest to MONTH
+    final DateTime now = DateTimes.nowUtc();
+    final Period dayRulePeriod = new Period(now.getMillis() - MAR_11.minusDays(1).minusMinutes(1).getMillis());
+    CascadingCompactionTemplate cascadingTemplate = new CascadingCompactionTemplate(
+        dataSource,
+        List.of(
+            new CompactionRule(dayRulePeriod, dayGranularityTemplate),
+            new CompactionRule(Period.ZERO, monthGranularityTemplate)
+        )
+    );
+
+    startCompactionWithSpec(cascadingTemplate);
+    runMSQCompactionJobs(12);
+
+    verifyFullyCompacted();
+    verifyNumSegmentsWith(Granularities.HOUR, 0);
+    verifyNumSegmentsWith(Granularities.DAY, 10);
+    verifyNumSegmentsWith(Granularities.MONTH, 2);
+  }
+
+  private void verifyNumSegmentsWith(Granularity granularity, int numExpectedSegments)
+  {
+    long numMatchingSegments = segmentStorage
+        .retrieveAllUsedSegments(dataSource, Segments.ONLY_VISIBLE)
+        .stream()
+        .filter(segment -> granularity.isAligned(segment.getInterval()))
+        .count();
+
+    Assert.assertEquals(
+        StringUtils.format("Segment with granularity[%s]", granularity),
+        numExpectedSegments,
+        (int) numMatchingSegments
+    );
+  }
+
+  private void verifyFullyCompacted()
+  {
+    runScheduledJob();
+    int numSegments = segmentStorage.retrieveAllUsedSegments(dataSource, Segments.ONLY_VISIBLE).size();
+
+    final AutoCompactionSnapshot snapshot = scheduler.getCompactionSnapshot(dataSource);
+    Assert.assertEquals(0, snapshot.getSegmentCountAwaitingCompaction());
+    Assert.assertEquals(0, snapshot.getSegmentCountSkipped());
+    Assert.assertEquals(numSegments, snapshot.getSegmentCountCompacted());
+  }
+
+  private void createSegments(int numSegments, Granularity granularity, DateTime firstSegmentStart)
+  {
+    final List<DataSegment> segments = CreateDataSegments
+        .ofDatasource(dataSource)
+        .forIntervals(numSegments, granularity)
+        .startingAt(firstSegmentStart)
+        .eachOfSizeInMb(100);
+    segmentStorage.commitSegments(Set.copyOf(segments), null);
+  }
+
+  private void startCompactionWithSpec(DataSourceCompactionConfig config)
+  {
+    scheduler.becomeLeader();
+    final CompactionSupervisorSpec compactionSupervisor
+        = new CompactionSupervisorSpec(config, false, scheduler);
+    scheduler.startCompaction(config.getDataSource(), compactionSupervisor.createSupervisor());
+  }
+
+  private void runCompactionTasks(int expectedCount)
+  {
+    runScheduledJob();
+    serviceEmitter.verifySum("compact/task/count", expectedCount);
+
+    ArgumentCaptor<Task> taskArgumentCaptor = ArgumentCaptor.forClass(Task.class);
+    Mockito.verify(taskQueue, Mockito.times(expectedCount)).add(taskArgumentCaptor.capture());
+
+    for (Task task : taskArgumentCaptor.getAllValues()) {
+      Assert.assertTrue(task instanceof CompactionTask);
+      Assert.assertEquals(dataSource, task.getDataSource());
+
+      final CompactionTask compactionTask = (CompactionTask) task;
+      runCompactionTask(
+          compactionTask.getId(),
+          compactionTask.getIoConfig().getInputSpec().findInterval(dataSource),
+          compactionTask.getSegmentGranularity()
+      );
+    }
+
+    segmentStorage.getManager().forceUpdateDataSourcesSnapshot();
+  }
+
+  private void runCompactionTask(String taskId, Interval compactionInterval, Granularity segmentGranularity)
+  {
+    // Update status of task in TaskQueue
+    Mockito.when(taskQueue.getTaskStatus(taskId))
+           .thenReturn(Optional.of(TaskStatus.success(taskId)));
+
+    // Determine interval and granularity and apply it to the timeline
+    if (segmentGranularity == null) {
+      // Nothing to do
+      return;
+    }
+
+    for (Interval replaceInterval : segmentGranularity.getIterable(compactionInterval)) {
+      // Create a single segment in this interval
+      DataSegment replaceSegment = CreateDataSegments
+          .ofDatasource(dataSource)
+          .forIntervals(1, segmentGranularity)
+          .startingAt(replaceInterval.getStart())
+          .withVersion("2")
+          .eachOfSizeInMb(100)
+          .get(0);
+      segmentStorage.commitSegments(Set.of(replaceSegment), null);
+    }
+  }
+
+  private void runMSQCompactionJobs(int numExpectedJobs)
+  {
+    runScheduledJob();
+    serviceEmitter.verifySum("compact/task/count", numExpectedJobs);
+
+    ArgumentCaptor<ClientSqlQuery> queryArgumentCaptor = ArgumentCaptor.forClass(ClientSqlQuery.class);
+    Mockito.verify(brokerClient, Mockito.times(numExpectedJobs))
+           .submitSqlTask(queryArgumentCaptor.capture());
+
+    for (ClientSqlQuery job : queryArgumentCaptor.getAllValues()) {
+      final String query = job.getQuery();
+
+      final Granularity segmentGranularity;
+      if (query.contains("PARTITIONED BY DAY")) {
+        segmentGranularity = Granularities.DAY;
+      } else if (query.contains("PARTITIONED BY MONTH")) {
+        segmentGranularity = Granularities.MONTH;
+      } else {
+        segmentGranularity = Granularities.HOUR;
+      }
+
+      final Interval compactionInterval =
+          (Interval) job.getContext().get(CompactSegments.COMPACTION_INTERVAL_KEY);
+      runCompactionTask(
+          submittedMsqTaskIds.get(compactionInterval),
+          compactionInterval,
+          segmentGranularity
+      );
+    }
+
+    segmentStorage.getManager().forceUpdateDataSourcesSnapshot();
+  }
+
+  private static CompactionStateMatcher createMatcher(Granularity segmentGranularity)
+  {
+    return new CompactionStateMatcher(
+        null,
+        null,
+        null,
+        null,
+        null,
+        new UserCompactionTaskGranularityConfig(segmentGranularity, null, null),
+        null
+    );
+  }
+
   private void disableScheduler()
   {
     compactionConfig.set(new ClusterCompactionConfig(null, null, null, false, null));
@@ -446,4 +755,16 @@ public class OverlordCompactionSchedulerTest
     executor.finishNextPendingTask();
   }
 
+  private CompactionSupervisor createSupervisorWithInlineSpec()
+  {
+    return new CompactionSupervisorSpec(
+        InlineSchemaDataSourceCompactionConfig
+            .builder()
+            .forDataSource(dataSource)
+            .withSkipOffsetFromLatest(Period.seconds(0))
+            .build(),
+        false,
+        scheduler
+    ).createSupervisor();
+  }
 }
diff --git a/indexing-service/src/test/java/org/apache/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java b/indexing-service/src/test/java/org/apache/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java
index e7ecc49df8..ee9e85204e 100644
--- a/indexing-service/src/test/java/org/apache/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java
+++ b/indexing-service/src/test/java/org/apache/druid/indexing/test/TestIndexerMetadataStorageCoordinator.java
@@ -27,9 +27,11 @@ import org.apache.druid.indexing.overlord.SegmentCreateRequest;
 import org.apache.druid.indexing.overlord.SegmentPublishResult;
 import org.apache.druid.indexing.overlord.Segments;
 import org.apache.druid.jackson.DefaultObjectMapper;
+import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.java.util.common.Pair;
 import org.apache.druid.metadata.PendingSegmentRecord;
 import org.apache.druid.metadata.ReplaceTaskLock;
+import org.apache.druid.metadata.SegmentsMetadataManager;
 import org.apache.druid.metadata.SortOrder;
 import org.apache.druid.segment.SegmentSchemaMapping;
 import org.apache.druid.segment.realtime.appenderator.SegmentIdWithShardSpec;
@@ -56,6 +58,11 @@ public class TestIndexerMetadataStorageCoordinator implements IndexerMetadataSto
 
   private int deleteSegmentsCount = 0;
 
+  public SegmentsMetadataManager getManager()
+  {
+    return segmentsMetadataManager;
+  }
+
   @Override
   public Set<String> retrieveAllDatasourceNames()
   {
@@ -106,9 +113,15 @@ public class TestIndexerMetadataStorageCoordinator implements IndexerMetadataSto
   @Override
   public Set<DataSegment> retrieveAllUsedSegments(String dataSource, Segments visibility)
   {
-    return Set.copyOf(
-        segmentsMetadataManager.getRecentDataSourcesSnapshot().getDataSource(dataSource).getSegments()
-    );
+    if (visibility == Segments.ONLY_VISIBLE) {
+      return segmentsMetadataManager
+          .getRecentDataSourcesSnapshot()
+          .getAllUsedNonOvershadowedSegments(dataSource, Intervals.ETERNITY);
+    } else {
+      return Set.copyOf(
+          segmentsMetadataManager.getRecentDataSourcesSnapshot().getDataSource(dataSource).getSegments()
+      );
+    }
   }
 
   @Override
diff --git a/processing/src/main/java/org/apache/druid/java/util/common/Intervals.java b/processing/src/main/java/org/apache/druid/java/util/common/Intervals.java
index 0960f63b49..ce34fc4798 100644
--- a/processing/src/main/java/org/apache/druid/java/util/common/Intervals.java
+++ b/processing/src/main/java/org/apache/druid/java/util/common/Intervals.java
@@ -29,6 +29,7 @@ import org.joda.time.format.DateTimeFormatter;
 import org.joda.time.format.ISODateTimeFormat;
 
 import javax.annotation.Nullable;
+import java.util.List;
 
 public final class Intervals
 {
@@ -167,6 +168,26 @@ public final class Intervals
     return null;
   }
 
+  /**
+   * @return List of intervals which when added to the given interval create
+   * {@link Intervals#ETERNITY}.
+   */
+  public static List<Interval> complementOf(Interval interval)
+  {
+    if (isEternity(interval)) {
+      return List.of();
+    } else if (DateTimes.MIN.equals(interval.getStart())) {
+      return List.of(new Interval(interval.getEnd(), DateTimes.MAX));
+    } else if (DateTimes.MAX.equals(interval.getEnd())) {
+      return List.of(new Interval(DateTimes.MIN, interval.getStart()));
+    } else {
+      return List.of(
+          new Interval(DateTimes.MIN, interval.getStart()),
+          new Interval(interval.getEnd(), DateTimes.MAX)
+      );
+    }
+  }
+
   private Intervals()
   {
   }
diff --git a/processing/src/main/java/org/apache/druid/query/http/ClientSqlQuery.java b/processing/src/main/java/org/apache/druid/query/http/ClientSqlQuery.java
index 3b3a2a2398..663714b4d9 100644
--- a/processing/src/main/java/org/apache/druid/query/http/ClientSqlQuery.java
+++ b/processing/src/main/java/org/apache/druid/query/http/ClientSqlQuery.java
@@ -109,7 +109,6 @@ public class ClientSqlQuery
     return parameters;
   }
 
-
   @Override
   public boolean equals(final Object o)
   {
diff --git a/processing/src/test/java/org/apache/druid/java/util/common/IntervalsTest.java b/processing/src/test/java/org/apache/druid/java/util/common/IntervalsTest.java
index 3d591a319f..de0a27bcb5 100644
--- a/processing/src/test/java/org/apache/druid/java/util/common/IntervalsTest.java
+++ b/processing/src/test/java/org/apache/druid/java/util/common/IntervalsTest.java
@@ -26,6 +26,7 @@ import org.junit.Assert;
 import org.junit.Test;
 
 import java.util.Arrays;
+import java.util.List;
 
 public class IntervalsTest
 {
@@ -124,4 +125,36 @@ public class IntervalsTest
         () -> Intervals.of("invalid string")
     );
   }
+
+  @Test
+  public void testComplementOf()
+  {
+    Assert.assertEquals(
+        List.of(),
+        Intervals.complementOf(Intervals.ETERNITY)
+    );
+
+    testComplementOf("2020/P1Y");
+    testComplementOf("2001/2001-01");
+    testComplementOf("2001-01-02/2001-02");
+  }
+
+  private void testComplementOf(String interval)
+  {
+    final Interval testInterval = Intervals.of(interval);
+    final List<Interval> complement = List.of(
+        new Interval(DateTimes.MIN, testInterval.getStart()),
+        new Interval(testInterval.getEnd(), DateTimes.MAX)
+    );
+    Assert.assertEquals(
+        complement,
+        Intervals.complementOf(testInterval)
+    );
+    Assert.assertEquals(
+        Intervals.ONLY_ETERNITY,
+        JodaUtils.condenseIntervals(
+            List.of(complement.get(0), complement.get(1), testInterval)
+        )
+    );
+  }
 }
diff --git a/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpec.java b/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpec.java
index 7ff9ff424f..7a7f655723 100644
--- a/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpec.java
+++ b/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpec.java
@@ -22,67 +22,24 @@ package org.apache.druid.client.indexing;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import org.apache.druid.java.util.common.IAE;
-import org.apache.druid.java.util.common.JodaUtils;
-import org.apache.druid.java.util.common.granularity.Granularity;
-import org.apache.druid.java.util.common.logger.Logger;
-import org.apache.druid.timeline.DataSegment;
 import org.joda.time.Interval;
 
 import javax.annotation.Nullable;
-import java.util.List;
 import java.util.Objects;
-import java.util.stream.Collectors;
 
 /**
  * InputSpec for {@link ClientCompactionIOConfig}.
- *
+ * <p>
  * Should be synchronized with org.apache.druid.indexing.common.task.CompactionIntervalSpec.
  */
 public class ClientCompactionIntervalSpec
 {
-  private static final Logger LOGGER = new Logger(ClientCompactionIntervalSpec.class);
-
   private static final String TYPE = "interval";
 
   private final Interval interval;
   @Nullable
   private final String sha256OfSortedSegmentIds;
 
-  public static ClientCompactionIntervalSpec fromSegments(List<DataSegment> segments, @Nullable Granularity segmentGranularity)
-  {
-    Interval interval = JodaUtils.umbrellaInterval(segments.stream().map(DataSegment::getInterval).collect(Collectors.toList()));
-    LOGGER.info("Original umbrella interval %s in compaction task for datasource %s", interval, segments.get(0).getDataSource());
-    if (segmentGranularity != null) {
-      // If segmentGranularity is set, then the segmentGranularity of the segments may not align with the configured segmentGranularity
-      // We must adjust the interval of the compaction task to fully cover and align with the segmentGranularity
-      // For example,
-      // - The umbrella interval of the segments is 2015-04-11/2015-04-12 but configured segmentGranularity is YEAR,
-      // if the compaction task's interval is 2015-04-11/2015-04-12 then we can run into race condition where after submitting
-      // the compaction task, a new segment outside of the interval (i.e. 2015-02-11/2015-02-12) got created will be lost as it is
-      // overshadowed by the compacted segment (compacted segment has interval 2015-01-01/2016-01-01.
-      // Hence, in this case, we must adjust the compaction task interval to 2015-01-01/2016-01-01.
-      // - The segment to be compacted has MONTH segmentGranularity with the interval 2015-02-01/2015-03-01 but configured
-      // segmentGranularity is WEEK. If the compaction task's interval is 2015-02-01/2015-03-01 then compacted segments created will be
-      // 2015-01-26/2015-02-02, 2015-02-02/2015-02-09, 2015-02-09/2015-02-16, 2015-02-16/2015-02-23, 2015-02-23/2015-03-02.
-      // This is because Druid's WEEK segments alway start and end on Monday. In the above example, 2015-01-26 and 2015-03-02
-      // are Mondays but 2015-02-01 and 2015-03-01 are not. Hence, the WEEK segments have to start and end on 2015-01-26 and 2015-03-02.
-      // If the compaction task's interval is 2015-02-01/2015-03-01, then the compacted segment would cause existing data
-      // from 2015-01-26 to 2015-02-01 and 2015-03-01 to 2015-03-02 to be lost. Hence, in this case,
-      // we must adjust the compaction task interval to 2015-01-26/2015-03-02
-      interval = JodaUtils.umbrellaInterval(segmentGranularity.getIterable(interval));
-      LOGGER.info(
-          "Interval adjusted to %s in compaction task for datasource %s with configured segmentGranularity %s",
-          interval,
-          segments.get(0).getDataSource(),
-          segmentGranularity
-      );
-    }
-    return new ClientCompactionIntervalSpec(
-        interval,
-        null
-    );
-  }
-
   @JsonCreator
   public ClientCompactionIntervalSpec(
       @JsonProperty("interval") Interval interval,
diff --git a/server/src/main/java/org/apache/druid/indexing/template/BatchIndexingJob.java b/server/src/main/java/org/apache/druid/indexing/template/BatchIndexingJob.java
new file mode 100644
index 0000000000..509226da10
--- /dev/null
+++ b/server/src/main/java/org/apache/druid/indexing/template/BatchIndexingJob.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.template;
+
+import org.apache.druid.client.indexing.ClientTaskQuery;
+import org.apache.druid.error.InvalidInput;
+import org.apache.druid.query.http.ClientSqlQuery;
+
+import javax.annotation.Nullable;
+import java.util.Objects;
+
+/**
+ * A batch indexing job that can be launched by the Overlord as a task.
+ * A job may contain the {@link ClientTaskQuery} itself or an MSQ query that gets converted
+ * by the Broker to a {@code ControllerTask} and is then submitted to the Overlord.
+ */
+public class BatchIndexingJob
+{
+  private final boolean isMsq;
+  private final ClientSqlQuery msqQuery;
+  private final ClientTaskQuery task;
+
+  protected BatchIndexingJob(
+      @Nullable ClientTaskQuery task,
+      @Nullable ClientSqlQuery msqQuery
+  )
+  {
+    this.isMsq = task == null;
+    this.msqQuery = msqQuery;
+    this.task = task;
+
+    InvalidInput.conditionalException(
+        (task == null || msqQuery == null) && (task != null || msqQuery != null),
+        "Exactly one of 'task' or 'msqQuery' must be non-null"
+    );
+  }
+
+  /**
+   * @return MSQ query to be run in this job, if any.
+   * @throws NullPointerException if this not an MSQ job.
+   */
+  public ClientSqlQuery getNonNullMsqQuery()
+  {
+    return Objects.requireNonNull(msqQuery);
+  }
+
+  /**
+   * @return Task to be run in this job, if any.
+   * @throws NullPointerException if this is an MSQ job.
+   */
+  public ClientTaskQuery getNonNullTask()
+  {
+    return Objects.requireNonNull(task);
+  }
+
+  /**
+   * @return true if this is an MSQ job.
+   */
+  public boolean isMsq()
+  {
+    return isMsq;
+  }
+
+  @Override
+  public String toString()
+  {
+    return "BatchIndexingJob{" +
+           "isMsq=" + isMsq +
+           ", msqQuery=" + msqQuery +
+           ", task=" + task +
+           '}';
+  }
+}
diff --git a/server/src/main/java/org/apache/druid/indexing/template/BatchIndexingJobTemplate.java b/server/src/main/java/org/apache/druid/indexing/template/BatchIndexingJobTemplate.java
new file mode 100644
index 0000000000..e8a09a6bb1
--- /dev/null
+++ b/server/src/main/java/org/apache/druid/indexing/template/BatchIndexingJobTemplate.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.indexing.template;
+
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.annotation.JsonTypeInfo;
+import org.apache.druid.data.input.InputSource;
+
+/**
+ * ETL template to create a {@link BatchIndexingJob} that indexes data from an
+ * {@link InputSource} into an output destination.
+ * <p>
+ * This interface currently defines no methods and is used only as the root type
+ * for serialization of compaction template objects.
+ */
+@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
+public interface BatchIndexingJobTemplate
+{
+  /**
+   * Unique type name of this template used for JSON serialization.
+   */
+  @JsonProperty
+  String getType();
+}
diff --git a/server/src/main/java/org/apache/druid/server/compaction/CompactionCandidate.java b/server/src/main/java/org/apache/druid/server/compaction/CompactionCandidate.java
index 4cd9b22df8..f936f3d49a 100644
--- a/server/src/main/java/org/apache/druid/server/compaction/CompactionCandidate.java
+++ b/server/src/main/java/org/apache/druid/server/compaction/CompactionCandidate.java
@@ -21,12 +21,14 @@ package org.apache.druid.server.compaction;
 
 import org.apache.druid.error.InvalidInput;
 import org.apache.druid.java.util.common.JodaUtils;
+import org.apache.druid.java.util.common.granularity.Granularity;
 import org.apache.druid.segment.SegmentUtils;
 import org.apache.druid.timeline.DataSegment;
 import org.joda.time.Interval;
 
 import javax.annotation.Nullable;
 import java.util.List;
+import java.util.Set;
 import java.util.stream.Collectors;
 
 /**
@@ -37,29 +39,54 @@ public class CompactionCandidate
 {
   private final List<DataSegment> segments;
   private final Interval umbrellaInterval;
+  private final Interval compactionInterval;
   private final String dataSource;
   private final long totalBytes;
   private final int numIntervals;
 
   private final CompactionStatus currentStatus;
 
-  public static CompactionCandidate from(List<DataSegment> segments)
+  public static CompactionCandidate from(
+      List<DataSegment> segments,
+      @Nullable Granularity targetSegmentGranularity
+  )
   {
     if (segments == null || segments.isEmpty()) {
       throw InvalidInput.exception("Segments to compact must be non-empty");
-    } else {
-      return new CompactionCandidate(segments, null);
     }
+
+    final Set<Interval> segmentIntervals =
+        segments.stream().map(DataSegment::getInterval).collect(Collectors.toSet());
+    final Interval umbrellaInterval = JodaUtils.umbrellaInterval(segmentIntervals);
+    final Interval compactionInterval =
+        targetSegmentGranularity == null
+        ? umbrellaInterval
+        : JodaUtils.umbrellaInterval(targetSegmentGranularity.getIterable(umbrellaInterval));
+
+    return new CompactionCandidate(
+        segments,
+        umbrellaInterval,
+        compactionInterval,
+        segmentIntervals.size(),
+        null
+    );
   }
 
-  private CompactionCandidate(List<DataSegment> segments, @Nullable CompactionStatus currentStatus)
+  private CompactionCandidate(
+      List<DataSegment> segments,
+      Interval umbrellaInterval,
+      Interval compactionInterval,
+      int numDistinctSegmentIntervals,
+      @Nullable CompactionStatus currentStatus
+  )
   {
     this.segments = segments;
     this.totalBytes = segments.stream().mapToLong(DataSegment::getSize).sum();
-    this.umbrellaInterval = JodaUtils.umbrellaInterval(
-        segments.stream().map(DataSegment::getInterval).collect(Collectors.toList())
-    );
-    this.numIntervals = (int) segments.stream().map(DataSegment::getInterval).distinct().count();
+
+    this.umbrellaInterval = umbrellaInterval;
+    this.compactionInterval = compactionInterval;
+
+    this.numIntervals = numDistinctSegmentIntervals;
     this.dataSource = segments.get(0).getDataSource();
     this.currentStatus = currentStatus;
   }
@@ -91,6 +118,15 @@ public class CompactionCandidate
     return umbrellaInterval;
   }
 
+  /**
+   * Interval aligned to the target segment granularity used for the compaction
+   * task. This interval completely contains the {@link #umbrellaInterval}.
+   */
+  public Interval getCompactionInterval()
+  {
+    return compactionInterval;
+  }
+
   public String getDataSource()
   {
     return dataSource;
@@ -115,7 +151,7 @@ public class CompactionCandidate
    */
   public CompactionCandidate withCurrentStatus(CompactionStatus status)
   {
-    return new CompactionCandidate(this.segments, status);
+    return new CompactionCandidate(segments, umbrellaInterval, compactionInterval, numIntervals, status);
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/server/compaction/CompactionRunSimulator.java b/server/src/main/java/org/apache/druid/server/compaction/CompactionRunSimulator.java
index 646319eec3..781019e12f 100644
--- a/server/src/main/java/org/apache/druid/server/compaction/CompactionRunSimulator.java
+++ b/server/src/main/java/org/apache/druid/server/compaction/CompactionRunSimulator.java
@@ -22,7 +22,6 @@ package org.apache.druid.server.compaction;
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
 import org.apache.druid.client.DataSourcesSnapshot;
-import org.apache.druid.client.indexing.ClientCompactionTaskQuery;
 import org.apache.druid.client.indexing.ClientCompactionTaskQueryTuningConfig;
 import org.apache.druid.client.indexing.IndexingTotalWorkerCapacityInfo;
 import org.apache.druid.client.indexing.TaskPayloadResponse;
@@ -86,16 +85,15 @@ public class CompactionRunSimulator
 
     // Add a read-only wrapper over the actual status tracker so that we can
     // account for the active tasks
-    final CompactionStatusTracker simulationStatusTracker = new CompactionStatusTracker(null)
+    final CompactionStatusTracker simulationStatusTracker = new CompactionStatusTracker()
     {
       @Override
       public CompactionStatus computeCompactionStatus(
           CompactionCandidate candidate,
-          DataSourceCompactionConfig config,
           CompactionCandidateSearchPolicy searchPolicy
       )
       {
-        return statusTracker.computeCompactionStatus(candidate, config, searchPolicy);
+        return statusTracker.computeCompactionStatus(candidate, searchPolicy);
       }
 
       @Override
@@ -123,12 +121,12 @@ public class CompactionRunSimulator
       }
 
       @Override
-      public void onTaskSubmitted(ClientCompactionTaskQuery taskPayload, CompactionCandidate candidateSegments)
+      public void onTaskSubmitted(String taskId, CompactionCandidate candidateSegments)
       {
         // Add a row for each task in order of submission
         final CompactionStatus status = candidateSegments.getCurrentStatus();
         queuedIntervals.addRow(
-            createRow(candidateSegments, taskPayload.getTuningConfig(), status == null ? "" : status.getReason())
+            createRow(candidateSegments, null, status == null ? "" : status.getReason())
         );
       }
     };
@@ -171,12 +169,10 @@ public class CompactionRunSimulator
   {
     final List<Object> row = new ArrayList<>();
     row.add(candidate.getDataSource());
-    row.add(candidate.getUmbrellaInterval());
+    row.add(candidate.getCompactionInterval());
     row.add(candidate.numSegments());
     row.add(candidate.getTotalBytes());
-    if (tuningConfig != null) {
-      row.add(CompactSegments.findMaxNumTaskSlotsUsedByOneNativeCompactionTask(tuningConfig));
-    }
+    row.add(CompactionSlotManager.getMaxTaskSlotsForNativeCompactionTask(tuningConfig));
     if (reason != null) {
       row.add(reason);
     }
diff --git a/server/src/main/java/org/apache/druid/server/compaction/CompactionSlotManager.java b/server/src/main/java/org/apache/druid/server/compaction/CompactionSlotManager.java
new file mode 100644
index 0000000000..428367db8e
--- /dev/null
+++ b/server/src/main/java/org/apache/druid/server/compaction/CompactionSlotManager.java
@@ -0,0 +1,376 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.server.compaction;
+
+import com.google.common.annotations.VisibleForTesting;
+import org.apache.druid.client.indexing.ClientCompactionTaskQuery;
+import org.apache.druid.client.indexing.ClientCompactionTaskQueryTuningConfig;
+import org.apache.druid.client.indexing.ClientMSQContext;
+import org.apache.druid.client.indexing.TaskPayloadResponse;
+import org.apache.druid.common.guava.FutureUtils;
+import org.apache.druid.indexer.CompactionEngine;
+import org.apache.druid.indexer.TaskStatus;
+import org.apache.druid.indexer.TaskStatusPlus;
+import org.apache.druid.indexer.partitions.DimensionRangePartitionsSpec;
+import org.apache.druid.java.util.common.ISE;
+import org.apache.druid.java.util.common.granularity.Granularity;
+import org.apache.druid.java.util.common.logger.Logger;
+import org.apache.druid.metadata.LockFilterPolicy;
+import org.apache.druid.rpc.indexing.OverlordClient;
+import org.apache.druid.server.coordinator.ClusterCompactionConfig;
+import org.apache.druid.server.coordinator.DataSourceCompactionConfig;
+import org.apache.druid.server.coordinator.duty.CoordinatorDutyUtils;
+import org.joda.time.Interval;
+
+import javax.annotation.Nullable;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/**
+ * Fetches running compaction tasks from the Overlord and tracks their compaction
+ * intervals and task slots.
+ */
+public class CompactionSlotManager
+{
+  /**
+   * Task type for native compaction tasks.
+   */
+  public static final String COMPACTION_TASK_TYPE = "compact";
+
+  private static final Logger log = new Logger(CompactionSlotManager.class);
+
+  private final OverlordClient overlordClient;
+  private final CompactionStatusTracker statusTracker;
+
+  private final Map<String, List<Interval>> intervalsToSkipCompaction;
+
+  private int numAvailableTaskSlots;
+
+  public CompactionSlotManager(
+      OverlordClient overlordClient,
+      CompactionStatusTracker statusTracker,
+      ClusterCompactionConfig clusterCompactionConfig
+  )
+  {
+    this.overlordClient = overlordClient;
+    this.statusTracker = statusTracker;
+    this.numAvailableTaskSlots = getCompactionTaskCapacity(clusterCompactionConfig);
+    this.intervalsToSkipCompaction = new HashMap<>();
+  }
+
+  public int getNumAvailableTaskSlots()
+  {
+    return numAvailableTaskSlots;
+  }
+
+  public Map<String, List<Interval>> getDatasourceIntervalsToSkipCompaction()
+  {
+    return intervalsToSkipCompaction;
+  }
+
+  public void reserveTaskSlots(int numSlotsToReserve)
+  {
+    numAvailableTaskSlots -= numSlotsToReserve;
+  }
+
+  /**
+   * Reserves task slots for the given task from the overall compaction task capacity.
+   */
+  public void reserveTaskSlots(ClientCompactionTaskQuery compactionTaskQuery)
+  {
+    // Note: The default compactionRunnerType used here should match the default runner used in CompactionTask when
+    // no runner is provided there.
+    CompactionEngine compactionRunnerType = compactionTaskQuery.getCompactionRunner() == null
+                                            ? CompactionEngine.NATIVE
+                                            : compactionTaskQuery.getCompactionRunner().getType();
+    if (compactionRunnerType == CompactionEngine.NATIVE) {
+      numAvailableTaskSlots -=
+          getMaxTaskSlotsForNativeCompactionTask(compactionTaskQuery.getTuningConfig());
+    } else {
+      numAvailableTaskSlots -=
+          getMaxTaskSlotsForMSQCompactionTask(compactionTaskQuery.getContext());
+    }
+  }
+
+  /**
+   * Reserves task slots for all running compaction tasks.
+   */
+  public void reserveTaskSlotsForRunningCompactionTasks()
+  {
+    for (ClientCompactionTaskQuery task : fetchRunningCompactionTasks()) {
+      reserveTaskSlots(task);
+    }
+  }
+
+  /**
+   * Retrieves currently running tasks of type {@link #COMPACTION_TASK_TYPE} from
+   * the Overlord.
+   * <p>
+   * Also queries the Overlord for the status of all tasks that were submitted
+   * recently but are not active anymore. The statuses are then updated in the
+   * {@link CompactionStatusTracker}.
+   */
+  public List<ClientCompactionTaskQuery> fetchRunningCompactionTasks()
+  {
+    // Fetch currently running compaction tasks
+    final List<TaskStatusPlus> compactionTasks = CoordinatorDutyUtils.getStatusOfActiveTasks(
+        overlordClient,
+        status -> status != null && COMPACTION_TASK_TYPE.equals(status.getType())
+    );
+
+    final Set<String> activeTaskIds
+        = compactionTasks.stream().map(TaskStatusPlus::getId).collect(Collectors.toSet());
+    trackStatusOfCompletedTasks(activeTaskIds);
+
+    final List<ClientCompactionTaskQuery> runningCompactTasks = new ArrayList<>();
+    for (TaskStatusPlus status : compactionTasks) {
+      final TaskPayloadResponse response =
+          FutureUtils.getUnchecked(overlordClient.taskPayload(status.getId()), true);
+      if (response == null) {
+        throw new ISE("Could not find payload for active compaction task[%s]", status.getId());
+      } else if (!COMPACTION_TASK_TYPE.equals(response.getPayload().getType())) {
+        throw new ISE(
+            "Payload of active compaction task[%s] is of invalid type[%s]",
+            status.getId(), response.getPayload().getType()
+        );
+      }
+
+      runningCompactTasks.add((ClientCompactionTaskQuery) response.getPayload());
+    }
+
+    return runningCompactTasks;
+  }
+
+  /**
+   * Cancels a currently running compaction task only if the segment granularity
+   * has changed in the datasource compaction config. Otherwise, the task is
+   * retained and its intervals are skipped from the current round of compaction.
+   *
+   * @return true if the task was canceled, false otherwise.
+   */
+  public boolean cancelTaskOnlyIfGranularityChanged(
+      ClientCompactionTaskQuery compactionTaskQuery,
+      DataSourceCompactionConfig dataSourceCompactionConfig
+  )
+  {
+    if (dataSourceCompactionConfig == null
+        || dataSourceCompactionConfig.getGranularitySpec() == null
+        || compactionTaskQuery.getGranularitySpec() == null) {
+      skipTaskInterval(compactionTaskQuery);
+      reserveTaskSlots(compactionTaskQuery);
+      return false;
+    }
+
+    Granularity configuredSegmentGranularity = dataSourceCompactionConfig.getGranularitySpec()
+                                                                         .getSegmentGranularity();
+    Granularity taskSegmentGranularity = compactionTaskQuery.getGranularitySpec().getSegmentGranularity();
+    if (configuredSegmentGranularity == null || configuredSegmentGranularity.equals(taskSegmentGranularity)) {
+      skipTaskInterval(compactionTaskQuery);
+      reserveTaskSlots(compactionTaskQuery);
+      return false;
+    }
+
+    log.info(
+        "Cancelling task[%s] as task segmentGranularity[%s] differs from compaction config segmentGranularity[%s].",
+        compactionTaskQuery.getId(), taskSegmentGranularity, configuredSegmentGranularity
+    );
+    overlordClient.cancelTask(compactionTaskQuery.getId());
+    return true;
+  }
+
+  /**
+   * Retrieves the list of intervals locked by higher priority tasks for each datasource.
+   * Since compaction tasks submitted for these Intervals would have to wait anyway,
+   * we skip these Intervals until the next compaction run by adding them to
+   * {@link #intervalsToSkipCompaction}.
+   * <p>
+   * This method must be called after invalid compaction tasks have already been
+   * cancelled using {@link #cancelTaskOnlyIfGranularityChanged} so that their
+   * intervals are not considered locked.
+   */
+  public void skipLockedIntervals(List<DataSourceCompactionConfig> compactionConfigs)
+  {
+    final List<LockFilterPolicy> lockFilterPolicies = compactionConfigs
+        .stream()
+        .map(
+            config -> new LockFilterPolicy(
+                config.getDataSource(),
+                config.getTaskPriority(),
+                null,
+                config.getTaskContext()
+            )
+        )
+        .collect(Collectors.toList());
+    final Map<String, List<Interval>> datasourceToLockedIntervals = new HashMap<>(
+        FutureUtils.getUnchecked(overlordClient.findLockedIntervals(lockFilterPolicies), true)
+    );
+    log.debug(
+        "Skipping the following intervals for Compaction as they are currently locked: %s",
+        datasourceToLockedIntervals
+    );
+
+    // Skip all the intervals locked by higher priority tasks for each datasource
+    datasourceToLockedIntervals.forEach(
+        (dataSource, intervals) ->
+            intervalsToSkipCompaction
+                .computeIfAbsent(dataSource, ds -> new ArrayList<>())
+                .addAll(intervals)
+    );
+  }
+
+  /**
+   * Adds the compaction interval of this task to {@link #intervalsToSkipCompaction}
+   */
+  private void skipTaskInterval(ClientCompactionTaskQuery compactionTask)
+  {
+    final Interval interval = compactionTask.getIoConfig().getInputSpec().getInterval();
+    intervalsToSkipCompaction.computeIfAbsent(compactionTask.getDataSource(), k -> new ArrayList<>())
+                             .add(interval);
+  }
+
+  /**
+   * Computes overall compaction task capacity for the cluster.
+   *
+   * @return A value >= 1.
+   */
+  private int getCompactionTaskCapacity(ClusterCompactionConfig clusterConfig)
+  {
+    int totalWorkerCapacity = CoordinatorDutyUtils.getTotalWorkerCapacity(overlordClient);
+
+    int compactionTaskCapacity = Math.min(
+        (int) (totalWorkerCapacity * clusterConfig.getCompactionTaskSlotRatio()),
+        clusterConfig.getMaxCompactionTaskSlots()
+    );
+
+    // Always consider atleast one slot available for compaction
+    return Math.max(1, compactionTaskCapacity);
+  }
+
+  /**
+   * Queries the Overlord for the status of all tasks that were submitted
+   * recently but are not active anymore. The statuses are then updated in the
+   * {@link #statusTracker}.
+   */
+  private void trackStatusOfCompletedTasks(Set<String> activeTaskIds)
+  {
+    final Set<String> finishedTaskIds = new HashSet<>(statusTracker.getSubmittedTaskIds());
+    finishedTaskIds.removeAll(activeTaskIds);
+
+    if (finishedTaskIds.isEmpty()) {
+      return;
+    }
+
+    final Map<String, TaskStatus> taskStatusMap
+        = FutureUtils.getUnchecked(overlordClient.taskStatuses(finishedTaskIds), true);
+    for (String taskId : finishedTaskIds) {
+      // Assume unknown task to have finished successfully
+      final TaskStatus taskStatus = taskStatusMap.getOrDefault(taskId, TaskStatus.success(taskId));
+      if (taskStatus.isComplete()) {
+        statusTracker.onTaskFinished(taskId, taskStatus);
+      }
+    }
+  }
+
+  /**
+   * @return Maximum number of task slots used by a native compaction task at
+   * any time when the task is run with the given tuning config.
+   */
+  public static int getMaxTaskSlotsForNativeCompactionTask(
+      @Nullable ClientCompactionTaskQueryTuningConfig tuningConfig
+  )
+  {
+    if (isParallelMode(tuningConfig)) {
+      Integer maxNumConcurrentSubTasks = tuningConfig.getMaxNumConcurrentSubTasks();
+      // Max number of task slots used in parallel mode = maxNumConcurrentSubTasks + 1 (supervisor task)
+      return (maxNumConcurrentSubTasks == null ? 1 : maxNumConcurrentSubTasks) + 1;
+    } else {
+      return 1;
+    }
+  }
+
+  /**
+   * @return Maximum number of task slots used by an MSQ compaction task at any
+   * time when the task is run with the given context.
+   */
+  public static int getMaxTaskSlotsForMSQCompactionTask(@Nullable Map<String, Object> context)
+  {
+    return context == null
+           ? ClientMSQContext.DEFAULT_MAX_NUM_TASKS
+           : (int) context.getOrDefault(ClientMSQContext.CTX_MAX_NUM_TASKS, ClientMSQContext.DEFAULT_MAX_NUM_TASKS);
+  }
+
+
+  /**
+   * Returns true if the compaction task can run in the parallel mode with the given tuningConfig.
+   * This method should be synchronized with ParallelIndexSupervisorTask.isParallelMode(InputSource, ParallelIndexTuningConfig).
+   */
+  @VisibleForTesting
+  public static boolean isParallelMode(@Nullable ClientCompactionTaskQueryTuningConfig tuningConfig)
+  {
+    if (null == tuningConfig) {
+      return false;
+    }
+    boolean useRangePartitions = useRangePartitions(tuningConfig);
+    int minRequiredNumConcurrentSubTasks = useRangePartitions ? 1 : 2;
+    return tuningConfig.getMaxNumConcurrentSubTasks() != null
+           && tuningConfig.getMaxNumConcurrentSubTasks() >= minRequiredNumConcurrentSubTasks;
+  }
+
+  private static boolean useRangePartitions(ClientCompactionTaskQueryTuningConfig tuningConfig)
+  {
+    // dynamic partitionsSpec will be used if getPartitionsSpec() returns null
+    return tuningConfig.getPartitionsSpec() instanceof DimensionRangePartitionsSpec;
+  }
+
+  public int computeSlotsRequiredForTask(
+      ClientCompactionTaskQuery task,
+      DataSourceCompactionConfig config
+  )
+  {
+    if (task.getCompactionRunner().getType() == CompactionEngine.MSQ) {
+      final Map<String, Object> autoCompactionContext = task.getContext();
+      if (autoCompactionContext.containsKey(ClientMSQContext.CTX_MAX_NUM_TASKS)) {
+        return (int) autoCompactionContext.get(ClientMSQContext.CTX_MAX_NUM_TASKS);
+      } else {
+        // Since MSQ needs all task slots for the calculated #tasks to be available upfront, allot all available
+        // compaction slots (upto a max of MAX_TASK_SLOTS_FOR_MSQ_COMPACTION) to current compaction task to avoid
+        // stalling. Setting "taskAssignment" to "auto" has the problem of not being able to determine the actual
+        // count, which is required for subsequent tasks.
+        final int slotsRequiredForCurrentTask = Math.min(
+            // Update the slots to 2 (min required for MSQ) if only 1 slot is available.
+            numAvailableTaskSlots == 1 ? 2 : numAvailableTaskSlots,
+            ClientMSQContext.MAX_TASK_SLOTS_FOR_MSQ_COMPACTION_TASK
+        );
+        autoCompactionContext.put(ClientMSQContext.CTX_MAX_NUM_TASKS, slotsRequiredForCurrentTask);
+
+        return slotsRequiredForCurrentTask;
+      }
+    } else {
+      return CompactionSlotManager.getMaxTaskSlotsForNativeCompactionTask(
+          config.getTuningConfig()
+      );
+    }
+  }
+}
diff --git a/server/src/main/java/org/apache/druid/server/compaction/CompactionSnapshotBuilder.java b/server/src/main/java/org/apache/druid/server/compaction/CompactionSnapshotBuilder.java
new file mode 100644
index 0000000000..d186587cb8
--- /dev/null
+++ b/server/src/main/java/org/apache/druid/server/compaction/CompactionSnapshotBuilder.java
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.druid.server.compaction;
+
+import org.apache.druid.server.coordinator.AutoCompactionSnapshot;
+import org.apache.druid.server.coordinator.stats.CoordinatorRunStats;
+import org.apache.druid.server.coordinator.stats.Dimension;
+import org.apache.druid.server.coordinator.stats.RowKey;
+import org.apache.druid.server.coordinator.stats.Stats;
+
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * Builds {@link AutoCompactionSnapshot} for multiple datasources using the
+ * identified {@link CompactionCandidate} list.
+ */
+public class CompactionSnapshotBuilder
+{
+  private final CoordinatorRunStats stats;
+  private final Map<String, AutoCompactionSnapshot.Builder> datasourceToBuilder = new HashMap<>();
+
+  public CompactionSnapshotBuilder(CoordinatorRunStats runStats)
+  {
+    this.stats = runStats;
+  }
+
+  public void addToComplete(CompactionCandidate candidate)
+  {
+    getBuilderForDatasource(candidate.getDataSource())
+        .incrementCompactedStats(candidate.getStats());
+  }
+
+  public void addToPending(CompactionCandidate candidate)
+  {
+    getBuilderForDatasource(candidate.getDataSource())
+        .incrementWaitingStats(candidate.getStats());
+  }
+
+  public void addToSkipped(CompactionCandidate candidate)
+  {
+    getBuilderForDatasource(candidate.getDataSource())
+        .incrementSkippedStats(candidate.getStats());
+  }
+
+  public void moveFromPendingToSkipped(CompactionCandidate candidate)
+  {
+    getBuilderForDatasource(candidate.getDataSource())
+        .decrementWaitingStats(candidate.getStats());
+    addToSkipped(candidate);
+  }
+
+  public void moveFromPendingToCompleted(CompactionCandidate candidate)
+  {
+    getBuilderForDatasource(candidate.getDataSource())
+        .decrementWaitingStats(candidate.getStats());
+    addToComplete(candidate);
+  }
+
+  public Map<String, AutoCompactionSnapshot> build()
+  {
+    final Map<String, AutoCompactionSnapshot> datasourceToSnapshot = new HashMap<>();
+    datasourceToBuilder.forEach((dataSource, builder) -> {
+      final AutoCompactionSnapshot autoCompactionSnapshot = builder.build();
+      datasourceToSnapshot.put(dataSource, autoCompactionSnapshot);
+      collectSnapshotStats(autoCompactionSnapshot);
+    });
+
+    return datasourceToSnapshot;
+  }
+
+  private AutoCompactionSnapshot.Builder getBuilderForDatasource(String dataSource)
+  {
+    return datasourceToBuilder.computeIfAbsent(dataSource, AutoCompactionSnapshot::builder);
+  }
+
+  private void collectSnapshotStats(AutoCompactionSnapshot autoCompactionSnapshot)
+  {
+    final RowKey rowKey = RowKey.of(Dimension.DATASOURCE, autoCompactionSnapshot.getDataSource());
+
+    stats.add(Stats.Compaction.PENDING_BYTES, rowKey, autoCompactionSnapshot.getBytesAwaitingCompaction());
+    stats.add(Stats.Compaction.PENDING_SEGMENTS, rowKey, autoCompactionSnapshot.getSegmentCountAwaitingCompaction());
+    stats.add(Stats.Compaction.PENDING_INTERVALS, rowKey, autoCompactionSnapshot.getIntervalCountAwaitingCompaction());
+    stats.add(Stats.Compaction.COMPACTED_BYTES, rowKey, autoCompactionSnapshot.getBytesCompacted());
+    stats.add(Stats.Compaction.COMPACTED_SEGMENTS, rowKey, autoCompactionSnapshot.getSegmentCountCompacted());
+    stats.add(Stats.Compaction.COMPACTED_INTERVALS, rowKey, autoCompactionSnapshot.getIntervalCountCompacted());
+    stats.add(Stats.Compaction.SKIPPED_BYTES, rowKey, autoCompactionSnapshot.getBytesSkipped());
+    stats.add(Stats.Compaction.SKIPPED_SEGMENTS, rowKey, autoCompactionSnapshot.getSegmentCountSkipped());
+    stats.add(Stats.Compaction.SKIPPED_INTERVALS, rowKey, autoCompactionSnapshot.getIntervalCountSkipped());
+  }
+}
diff --git a/server/src/main/java/org/apache/druid/server/compaction/CompactionStatistics.java b/server/src/main/java/org/apache/druid/server/compaction/CompactionStatistics.java
index 23f1b7fe9e..d7e5165586 100644
--- a/server/src/main/java/org/apache/druid/server/compaction/CompactionStatistics.java
+++ b/server/src/main/java/org/apache/druid/server/compaction/CompactionStatistics.java
@@ -58,4 +58,11 @@ public class CompactionStatistics
     numIntervals += other.getNumIntervals();
     numSegments += other.getNumSegments();
   }
+
+  public void decrement(CompactionStatistics other)
+  {
+    totalBytes -= other.getTotalBytes();
+    numIntervals -= other.getNumIntervals();
+    numSegments -= other.getNumSegments();
+  }
 }
diff --git a/server/src/main/java/org/apache/druid/server/compaction/CompactionStatus.java b/server/src/main/java/org/apache/druid/server/compaction/CompactionStatus.java
index 3c1616b6e7..be4acd00e2 100644
--- a/server/src/main/java/org/apache/druid/server/compaction/CompactionStatus.java
+++ b/server/src/main/java/org/apache/druid/server/compaction/CompactionStatus.java
@@ -19,9 +19,7 @@
 
 package org.apache.druid.server.compaction;
 
-import com.fasterxml.jackson.databind.ObjectMapper;
 import org.apache.commons.lang3.ArrayUtils;
-import org.apache.druid.client.indexing.ClientCompactionTaskGranularitySpec;
 import org.apache.druid.client.indexing.ClientCompactionTaskQueryTuningConfig;
 import org.apache.druid.common.config.Configs;
 import org.apache.druid.data.input.impl.DimensionSchema;
@@ -64,6 +62,7 @@ public class CompactionStatus
    * The order of the checks must be honored while evaluating them.
    */
   private static final List<Function<Evaluator, CompactionStatus>> CHECKS = Arrays.asList(
+      Evaluator::inputBytesAreWithinLimit,
       Evaluator::segmentsHaveBeenCompactedAtLeastOnce,
       Evaluator::allCandidatesHaveSameCompactionState,
       Evaluator::partitionsSpecIsUpToDate,
@@ -115,12 +114,17 @@ public class CompactionStatus
            '}';
   }
 
-  private static CompactionStatus incomplete(String reasonFormat, Object... args)
+  public static CompactionStatus pending(String reasonFormat, Object... args)
   {
     return new CompactionStatus(State.PENDING, StringUtils.format(reasonFormat, args));
   }
 
-  private static <T> CompactionStatus completeIfEqual(
+  /**
+   * Computes compaction status for the given field. The status is assumed to be
+   * COMPLETE (i.e. no further compaction is required) if the configured value
+   * of the field is null or equal to the current value.
+   */
+  private static <T> CompactionStatus completeIfNullOrEqual(
       String field,
       T configured,
       T current,
@@ -141,7 +145,7 @@ public class CompactionStatus
       Function<T, String> stringFunction
   )
   {
-    return CompactionStatus.incomplete(
+    return CompactionStatus.pending(
         "'%s' mismatch: required[%s], current[%s]",
         field,
         target == null ? null : stringFunction.apply(target),
@@ -187,14 +191,19 @@ public class CompactionStatus
     }
   }
 
-  static CompactionStatus skipped(String reasonFormat, Object... args)
+  public static CompactionStatus skipped(String reasonFormat, Object... args)
   {
     return new CompactionStatus(State.SKIPPED, StringUtils.format(reasonFormat, args));
   }
 
-  static CompactionStatus running(String reasonForCompaction)
+  public static CompactionStatus running(String message)
+  {
+    return new CompactionStatus(State.RUNNING, message);
+  }
+
+  public static CompactionStatus complete(String message)
   {
-    return new CompactionStatus(State.RUNNING, reasonForCompaction);
+    return new CompactionStatus(State.COMPLETE, message);
   }
 
   /**
@@ -204,23 +213,30 @@ public class CompactionStatus
    */
   static CompactionStatus compute(
       CompactionCandidate candidateSegments,
-      DataSourceCompactionConfig config,
-      ObjectMapper objectMapper
+      DataSourceCompactionConfig config
   )
   {
-    final Evaluator evaluator = new Evaluator(candidateSegments, config, objectMapper);
+    final Evaluator evaluator = new Evaluator(candidateSegments, config);
     return CHECKS.stream()
                  .map(f -> f.apply(evaluator))
                  .filter(status -> !status.isComplete())
                  .findFirst().orElse(COMPLETE);
   }
 
+  @Nullable
   static PartitionsSpec findPartitionsSpecFromConfig(ClientCompactionTaskQueryTuningConfig tuningConfig)
   {
     final PartitionsSpec partitionsSpecFromTuningConfig = tuningConfig.getPartitionsSpec();
     if (partitionsSpecFromTuningConfig == null) {
-      final long maxTotalRows = Configs.valueOrDefault(tuningConfig.getMaxTotalRows(), Long.MAX_VALUE);
-      return new DynamicPartitionsSpec(tuningConfig.getMaxRowsPerSegment(), maxTotalRows);
+      final Long maxTotalRows = tuningConfig.getMaxTotalRows();
+      final Integer maxRowsPerSegment = tuningConfig.getMaxRowsPerSegment();
+
+      if (maxTotalRows == null && maxRowsPerSegment == null) {
+        // If not specified, return null so that partitionsSpec is not compared
+        return null;
+      } else {
+        return new DynamicPartitionsSpec(maxRowsPerSegment, maxTotalRows);
+      }
     } else if (partitionsSpecFromTuningConfig instanceof DynamicPartitionsSpec) {
       return new DynamicPartitionsSpec(
           partitionsSpecFromTuningConfig.getMaxRowsPerSegment(),
@@ -276,22 +292,19 @@ public class CompactionStatus
    */
   private static class Evaluator
   {
-    private final ObjectMapper objectMapper;
     private final DataSourceCompactionConfig compactionConfig;
     private final CompactionCandidate candidateSegments;
     private final CompactionState lastCompactionState;
     private final ClientCompactionTaskQueryTuningConfig tuningConfig;
-    private final ClientCompactionTaskGranularitySpec existingGranularitySpec;
+    private final UserCompactionTaskGranularityConfig existingGranularitySpec;
     private final UserCompactionTaskGranularityConfig configuredGranularitySpec;
 
     private Evaluator(
         CompactionCandidate candidateSegments,
-        DataSourceCompactionConfig compactionConfig,
-        ObjectMapper objectMapper
+        DataSourceCompactionConfig compactionConfig
     )
     {
       this.candidateSegments = candidateSegments;
-      this.objectMapper = objectMapper;
       this.lastCompactionState = candidateSegments.getSegments().get(0).getLastCompactionState();
       this.compactionConfig = compactionConfig;
       this.tuningConfig = ClientCompactionTaskQueryTuningConfig.from(compactionConfig);
@@ -299,9 +312,8 @@ public class CompactionStatus
       if (lastCompactionState == null) {
         this.existingGranularitySpec = null;
       } else {
-        this.existingGranularitySpec = convertIfNotNull(
-            lastCompactionState.getGranularitySpec(),
-            ClientCompactionTaskGranularitySpec.class
+        this.existingGranularitySpec = UserCompactionTaskGranularityConfig.from(
+            lastCompactionState.getGranularitySpec()
         );
       }
     }
@@ -309,7 +321,7 @@ public class CompactionStatus
     private CompactionStatus segmentsHaveBeenCompactedAtLeastOnce()
     {
       if (lastCompactionState == null) {
-        return CompactionStatus.incomplete("not compacted yet");
+        return CompactionStatus.pending("not compacted yet");
       } else {
         return COMPLETE;
       }
@@ -323,7 +335,7 @@ public class CompactionStatus
       if (allHaveSameCompactionState) {
         return COMPLETE;
       } else {
-        return CompactionStatus.incomplete("segments have different last compaction states");
+        return CompactionStatus.pending("segments have different last compaction states");
       }
     }
 
@@ -337,7 +349,7 @@ public class CompactionStatus
             existingPartionsSpec.getMaxRowsPerSegment(),
             ((DynamicPartitionsSpec) existingPartionsSpec).getMaxTotalRowsOr(Long.MAX_VALUE));
       }
-      return CompactionStatus.completeIfEqual(
+      return CompactionStatus.completeIfNullOrEqual(
           "partitionsSpec",
           findPartitionsSpecFromConfig(tuningConfig),
           existingPartionsSpec,
@@ -347,17 +359,17 @@ public class CompactionStatus
 
     private CompactionStatus indexSpecIsUpToDate()
     {
-      return CompactionStatus.completeIfEqual(
+      return CompactionStatus.completeIfNullOrEqual(
           "indexSpec",
           Configs.valueOrDefault(tuningConfig.getIndexSpec(), IndexSpec.getDefault()).getEffectiveSpec(),
-          objectMapper.convertValue(lastCompactionState.getIndexSpec(), IndexSpec.class).getEffectiveSpec(),
+          lastCompactionState.getIndexSpec().getEffectiveSpec(),
           String::valueOf
       );
     }
 
     private CompactionStatus projectionsAreUpToDate()
     {
-      return CompactionStatus.completeIfEqual(
+      return CompactionStatus.completeIfNullOrEqual(
           "projections",
           compactionConfig.getProjections(),
           lastCompactionState.getProjections(),
@@ -365,6 +377,19 @@ public class CompactionStatus
       );
     }
 
+    private CompactionStatus inputBytesAreWithinLimit()
+    {
+      final long inputSegmentSize = compactionConfig.getInputSegmentSizeBytes();
+      if (candidateSegments.getTotalBytes() > inputSegmentSize) {
+        return CompactionStatus.skipped(
+            "'inputSegmentSize' exceeded: Total segment size[%d] is larger than allowed inputSegmentSize[%d]",
+            candidateSegments.getTotalBytes(), inputSegmentSize
+        );
+      } else {
+        return COMPLETE;
+      }
+    }
+
     private CompactionStatus segmentGranularityIsUpToDate()
     {
       if (configuredGranularitySpec == null
@@ -385,7 +410,7 @@ public class CompactionStatus
             segment -> !configuredSegmentGranularity.isAligned(segment.getInterval())
         );
         if (needsCompaction) {
-          return CompactionStatus.incomplete(
+          return CompactionStatus.pending(
               "segmentGranularity: segments do not align with target[%s]",
               asString(configuredSegmentGranularity)
           );
@@ -407,7 +432,7 @@ public class CompactionStatus
       if (configuredGranularitySpec == null) {
         return COMPLETE;
       } else {
-        return CompactionStatus.completeIfEqual(
+        return CompactionStatus.completeIfNullOrEqual(
             "rollup",
             configuredGranularitySpec.isRollup(),
             existingGranularitySpec == null ? null : existingGranularitySpec.isRollup(),
@@ -421,7 +446,7 @@ public class CompactionStatus
       if (configuredGranularitySpec == null) {
         return COMPLETE;
       } else {
-        return CompactionStatus.completeIfEqual(
+        return CompactionStatus.completeIfNullOrEqual(
             "queryGranularity",
             configuredGranularitySpec.getQueryGranularity(),
             existingGranularitySpec == null ? null : existingGranularitySpec.getQueryGranularity(),
@@ -454,7 +479,7 @@ public class CompactionStatus
             ? IndexSpec.getDefault()
             : compactionConfig.getTuningConfig().getIndexSpec()
         );
-        return CompactionStatus.completeIfEqual(
+        return CompactionStatus.completeIfNullOrEqual(
             "dimensionsSpec",
             configuredDimensions,
             existingDimensions,
@@ -493,26 +518,13 @@ public class CompactionStatus
         return COMPLETE;
       }
 
-      CompactionTransformSpec existingTransformSpec = convertIfNotNull(
-          lastCompactionState.getTransformSpec(),
-          CompactionTransformSpec.class
-      );
-      return CompactionStatus.completeIfEqual(
+      CompactionTransformSpec existingTransformSpec = lastCompactionState.getTransformSpec();
+      return CompactionStatus.completeIfNullOrEqual(
           "transformSpec filter",
           compactionConfig.getTransformSpec().getFilter(),
           existingTransformSpec == null ? null : existingTransformSpec.getFilter(),
           String::valueOf
       );
     }
-
-    @Nullable
-    private <T> T convertIfNotNull(Object object, Class<T> clazz)
-    {
-      if (object == null) {
-        return null;
-      } else {
-        return objectMapper.convertValue(object, clazz);
-      }
-    }
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/compaction/CompactionStatusTracker.java b/server/src/main/java/org/apache/druid/server/compaction/CompactionStatusTracker.java
index cbf5f25f9d..401f413e7f 100644
--- a/server/src/main/java/org/apache/druid/server/compaction/CompactionStatusTracker.java
+++ b/server/src/main/java/org/apache/druid/server/compaction/CompactionStatusTracker.java
@@ -19,14 +19,10 @@
 
 package org.apache.druid.server.compaction;
 
-import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.inject.Inject;
-import org.apache.druid.client.indexing.ClientCompactionTaskQuery;
 import org.apache.druid.indexer.TaskState;
 import org.apache.druid.indexer.TaskStatus;
 import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.server.coordinator.DataSourceCompactionConfig;
-import org.apache.druid.server.coordinator.DruidCompactionConfig;
 import org.joda.time.DateTime;
 import org.joda.time.Duration;
 import org.joda.time.Interval;
@@ -45,7 +41,6 @@ public class CompactionStatusTracker
 {
   private static final Duration MAX_STATUS_RETAIN_DURATION = Duration.standardHours(12);
 
-  private final ObjectMapper objectMapper;
   private final ConcurrentHashMap<String, DatasourceStatus> datasourceStatuses
       = new ConcurrentHashMap<>();
   private final ConcurrentHashMap<String, CompactionCandidate> submittedTaskIdToSegments
@@ -53,12 +48,6 @@ public class CompactionStatusTracker
 
   private final AtomicReference<DateTime> segmentSnapshotTime = new AtomicReference<>();
 
-  @Inject
-  public CompactionStatusTracker(ObjectMapper objectMapper)
-  {
-    this.objectMapper = objectMapper;
-  }
-
   public void stop()
   {
     datasourceStatuses.clear();
@@ -74,7 +63,7 @@ public class CompactionStatusTracker
     return datasourceStatuses
         .getOrDefault(candidates.getDataSource(), DatasourceStatus.EMPTY)
         .intervalToTaskStatus
-        .get(candidates.getUmbrellaInterval());
+        .get(candidates.getCompactionInterval());
   }
 
   /**
@@ -86,30 +75,20 @@ public class CompactionStatusTracker
     return submittedTaskIdToSegments.keySet();
   }
 
+  /**
+   * Checks if compaction can be started for the given {@link CompactionCandidate}.
+   * This method assumes that the given candidate is eligible for compaction
+   * based on the current compaction config/supervisor of the datasource.
+   */
   public CompactionStatus computeCompactionStatus(
       CompactionCandidate candidate,
-      DataSourceCompactionConfig config,
       CompactionCandidateSearchPolicy searchPolicy
   )
   {
-    final CompactionStatus compactionStatus = CompactionStatus.compute(candidate, config, objectMapper);
-    if (compactionStatus.isComplete()) {
-      return compactionStatus;
-    }
-
-    // Skip intervals that violate max allowed input segment size
-    final long inputSegmentSize = config.getInputSegmentSizeBytes();
-    if (candidate.getTotalBytes() > inputSegmentSize) {
-      return CompactionStatus.skipped(
-          "'inputSegmentSize' exceeded: Total segment size[%d] is larger than allowed inputSegmentSize[%d]",
-          candidate.getTotalBytes(), inputSegmentSize
-      );
-    }
-
     // Skip intervals that already have a running task
     final CompactionTaskStatus lastTaskStatus = getLatestTaskStatus(candidate);
     if (lastTaskStatus != null && lastTaskStatus.getState() == TaskState.RUNNING) {
-      return CompactionStatus.skipped("Task for interval is already running");
+      return CompactionStatus.running("Task for interval is already running");
     }
 
     // Skip intervals that have been recently compacted if segment timeline is not updated yet
@@ -117,19 +96,23 @@ public class CompactionStatusTracker
     if (lastTaskStatus != null
         && lastTaskStatus.getState() == TaskState.SUCCESS
         && snapshotTime != null && snapshotTime.isBefore(lastTaskStatus.getUpdatedTime())) {
-      return CompactionStatus.skipped(
+      return CompactionStatus.complete(
           "Segment timeline not updated since last compaction task succeeded"
       );
     }
 
     // Skip intervals that have been filtered out by the policy
-    if (!searchPolicy.isEligibleForCompaction(candidate, compactionStatus, lastTaskStatus)) {
+    if (!searchPolicy.isEligibleForCompaction(candidate, CompactionStatus.pending(""), lastTaskStatus)) {
       return CompactionStatus.skipped("Rejected by search policy");
     }
 
-    return compactionStatus;
+    return CompactionStatus.pending("Not compacted yet");
   }
 
+  /**
+   * Tracks the latest compaction status of the given compaction candidates.
+   * Used only by the {@link CompactionRunSimulator}.
+   */
   public void onCompactionStatusComputed(
       CompactionCandidate candidateSegments,
       DataSourceCompactionConfig config
@@ -143,17 +126,15 @@ public class CompactionStatusTracker
     this.segmentSnapshotTime.set(snapshotTime);
   }
 
-  public void onCompactionConfigUpdated(DruidCompactionConfig compactionConfig)
+  /**
+   * Updates the set of datasources that have compaction enabled and cleans up
+   * stale task statuses.
+   */
+  public void resetActiveDatasources(Set<String> compactionEnabledDatasources)
   {
-    final Set<String> compactionEnabledDatasources = new HashSet<>();
-    if (compactionConfig.getCompactionConfigs() != null) {
-      compactionConfig.getCompactionConfigs().forEach(config -> {
-        getOrComputeDatasourceStatus(config.getDataSource())
-            .cleanupStaleTaskStatuses();
-
-        compactionEnabledDatasources.add(config.getDataSource());
-      });
-    }
+    compactionEnabledDatasources.forEach(
+        dataSource -> getOrComputeDatasourceStatus(dataSource).cleanupStaleTaskStatuses()
+    );
 
     // Clean up state for datasources where compaction has been disabled
     final Set<String> allDatasources = new HashSet<>(datasourceStatuses.keySet());
@@ -165,12 +146,12 @@ public class CompactionStatusTracker
   }
 
   public void onTaskSubmitted(
-      ClientCompactionTaskQuery taskPayload,
+      String taskId,
       CompactionCandidate candidateSegments
   )
   {
-    submittedTaskIdToSegments.put(taskPayload.getId(), candidateSegments);
-    getOrComputeDatasourceStatus(taskPayload.getDataSource())
+    submittedTaskIdToSegments.put(taskId, candidateSegments);
+    getOrComputeDatasourceStatus(candidateSegments.getDataSource())
         .handleSubmittedTask(candidateSegments);
   }
 
@@ -186,7 +167,7 @@ public class CompactionStatusTracker
       return;
     }
 
-    final Interval compactionInterval = candidateSegments.getUmbrellaInterval();
+    final Interval compactionInterval = candidateSegments.getCompactionInterval();
     getOrComputeDatasourceStatus(candidateSegments.getDataSource())
         .handleCompletedTask(compactionInterval, taskStatus);
   }
@@ -229,7 +210,7 @@ public class CompactionStatusTracker
 
     void handleSubmittedTask(CompactionCandidate candidateSegments)
     {
-      final Interval interval = candidateSegments.getUmbrellaInterval();
+      final Interval interval = candidateSegments.getCompactionInterval();
       final CompactionTaskStatus lastStatus = intervalToTaskStatus.get(interval);
 
       final DateTime now = DateTimes.nowUtc();
diff --git a/server/src/main/java/org/apache/druid/server/compaction/DataSourceCompactibleSegmentIterator.java b/server/src/main/java/org/apache/druid/server/compaction/DataSourceCompactibleSegmentIterator.java
index e15d310b33..40867eb5c0 100644
--- a/server/src/main/java/org/apache/druid/server/compaction/DataSourceCompactibleSegmentIterator.java
+++ b/server/src/main/java/org/apache/druid/server/compaction/DataSourceCompactibleSegmentIterator.java
@@ -65,8 +65,6 @@ public class DataSourceCompactibleSegmentIterator implements CompactionSegmentIt
 
   private final String dataSource;
   private final DataSourceCompactionConfig config;
-  private final CompactionStatusTracker statusTracker;
-  private final CompactionCandidateSearchPolicy searchPolicy;
 
   private final List<CompactionCandidate> compactedSegments = new ArrayList<>();
   private final List<CompactionCandidate> skippedSegments = new ArrayList<>();
@@ -83,14 +81,11 @@ public class DataSourceCompactibleSegmentIterator implements CompactionSegmentIt
       DataSourceCompactionConfig config,
       SegmentTimeline timeline,
       List<Interval> skipIntervals,
-      CompactionCandidateSearchPolicy searchPolicy,
-      CompactionStatusTracker statusTracker
+      CompactionCandidateSearchPolicy searchPolicy
   )
   {
-    this.statusTracker = statusTracker;
     this.config = config;
     this.dataSource = config.getDataSource();
-    this.searchPolicy = searchPolicy;
     this.queue = new PriorityQueue<>(searchPolicy::compareCandidates);
 
     populateQueue(timeline, skipIntervals);
@@ -117,11 +112,12 @@ public class DataSourceCompactibleSegmentIterator implements CompactionSegmentIt
             }
           }
           if (!partialEternitySegments.isEmpty()) {
-            CompactionCandidate candidatesWithStatus = CompactionCandidate.from(partialEternitySegments).withCurrentStatus(
-                CompactionStatus.skipped("Segments have partial-eternity intervals")
-            );
+            // Do not use the target segment granularity in the CompactionCandidate
+            // as Granularities.getIterable() will cause OOM due to the above issue
+            CompactionCandidate candidatesWithStatus = CompactionCandidate
+                .from(partialEternitySegments, null)
+                .withCurrentStatus(CompactionStatus.skipped("Segments have partial-eternity intervals"));
             skippedSegments.add(candidatesWithStatus);
-            statusTracker.onCompactionStatusComputed(candidatesWithStatus, config);
             return;
           }
 
@@ -315,11 +311,9 @@ public class DataSourceCompactibleSegmentIterator implements CompactionSegmentIt
         continue;
       }
 
-      final CompactionCandidate candidates = CompactionCandidate.from(segments);
-      final CompactionStatus compactionStatus
-          = statusTracker.computeCompactionStatus(candidates, config, searchPolicy);
+      final CompactionCandidate candidates = CompactionCandidate.from(segments, config.getSegmentGranularity());
+      final CompactionStatus compactionStatus = CompactionStatus.compute(candidates, config);
       final CompactionCandidate candidatesWithStatus = candidates.withCurrentStatus(compactionStatus);
-      statusTracker.onCompactionStatusComputed(candidatesWithStatus, config);
 
       if (compactionStatus.isComplete()) {
         compactedSegments.add(candidatesWithStatus);
@@ -360,10 +354,10 @@ public class DataSourceCompactibleSegmentIterator implements CompactionSegmentIt
           timeline.findNonOvershadowedObjectsInInterval(skipInterval, Partitions.ONLY_COMPLETE)
       );
       if (!CollectionUtils.isNullOrEmpty(segments)) {
-        final CompactionCandidate candidates = CompactionCandidate.from(segments);
+        final CompactionCandidate candidates = CompactionCandidate.from(segments, config.getSegmentGranularity());
 
         final CompactionStatus reason;
-        if (candidates.getUmbrellaInterval().overlaps(latestSkipInterval)) {
+        if (candidates.getCompactionInterval().overlaps(latestSkipInterval)) {
           reason = CompactionStatus.skipped("skip offset from latest[%s]", skipOffset);
         } else {
           reason = CompactionStatus.skipped("interval locked by another task");
@@ -371,7 +365,6 @@ public class DataSourceCompactibleSegmentIterator implements CompactionSegmentIt
 
         final CompactionCandidate candidatesWithStatus = candidates.withCurrentStatus(reason);
         skippedSegments.add(candidatesWithStatus);
-        statusTracker.onCompactionStatusComputed(candidatesWithStatus, config);
       }
     }
 
diff --git a/server/src/main/java/org/apache/druid/server/compaction/PriorityBasedCompactionSegmentIterator.java b/server/src/main/java/org/apache/druid/server/compaction/PriorityBasedCompactionSegmentIterator.java
index 1e91df7e38..49d936fda0 100644
--- a/server/src/main/java/org/apache/druid/server/compaction/PriorityBasedCompactionSegmentIterator.java
+++ b/server/src/main/java/org/apache/druid/server/compaction/PriorityBasedCompactionSegmentIterator.java
@@ -48,8 +48,7 @@ public class PriorityBasedCompactionSegmentIterator implements CompactionSegment
       CompactionCandidateSearchPolicy searchPolicy,
       Map<String, DataSourceCompactionConfig> compactionConfigs,
       Map<String, SegmentTimeline> datasourceToTimeline,
-      Map<String, List<Interval>> skipIntervals,
-      CompactionStatusTracker statusTracker
+      Map<String, List<Interval>> skipIntervals
   )
   {
     this.queue = new PriorityQueue<>(searchPolicy::compareCandidates);
@@ -70,8 +69,7 @@ public class PriorityBasedCompactionSegmentIterator implements CompactionSegment
               compactionConfigs.get(datasource),
               timeline,
               skipIntervals.getOrDefault(datasource, Collections.emptyList()),
-              searchPolicy,
-              statusTracker
+              searchPolicy
           )
       );
       addNextItemForDatasourceToQueue(datasource);
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java b/server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java
index e31a7919f2..b450dd5a50 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java
@@ -204,6 +204,25 @@ public class AutoCompactionSnapshot
     );
   }
 
+  @Override
+  public String toString()
+  {
+    return "AutoCompactionSnapshot{" +
+           "dataSource='" + dataSource + '\'' +
+           ", scheduleStatus=" + scheduleStatus +
+           ", message='" + message + '\'' +
+           ", bytesAwaitingCompaction=" + bytesAwaitingCompaction +
+           ", bytesCompacted=" + bytesCompacted +
+           ", bytesSkipped=" + bytesSkipped +
+           ", segmentCountAwaitingCompaction=" + segmentCountAwaitingCompaction +
+           ", segmentCountCompacted=" + segmentCountCompacted +
+           ", segmentCountSkipped=" + segmentCountSkipped +
+           ", intervalCountAwaitingCompaction=" + intervalCountAwaitingCompaction +
+           ", intervalCountCompacted=" + intervalCountCompacted +
+           ", intervalCountSkipped=" + intervalCountSkipped +
+           '}';
+  }
+
   public static class Builder
   {
     private final String dataSource;
@@ -241,6 +260,11 @@ public class AutoCompactionSnapshot
       waitingStats.increment(entry);
     }
 
+    public void decrementWaitingStats(CompactionStatistics entry)
+    {
+      waitingStats.decrement(entry);
+    }
+
     public void incrementCompactedStats(CompactionStatistics entry)
     {
       compactedStats.increment(entry);
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskGranularityConfig.java b/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskGranularityConfig.java
index be3c7e1af9..4ad57f3adf 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskGranularityConfig.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskGranularityConfig.java
@@ -20,10 +20,12 @@
 package org.apache.druid.server.coordinator;
 
 import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import org.apache.druid.indexer.granularity.GranularitySpec;
 import org.apache.druid.java.util.common.granularity.Granularity;
 
+import javax.annotation.Nullable;
 import java.util.Objects;
 
 /**
@@ -35,6 +37,7 @@ import java.util.Objects;
  * Note that this class is not the same as {@link GranularitySpec}. This class simply holds Granularity configs
  * and pass it to compaction task spec. This class does not do bucketing, group events or knows how to partition data.
  */
+@JsonInclude(JsonInclude.Include.NON_NULL)
 public class UserCompactionTaskGranularityConfig
 {
   private final Granularity segmentGranularity;
@@ -71,6 +74,20 @@ public class UserCompactionTaskGranularityConfig
     return rollup;
   }
 
+  @Nullable
+  public static UserCompactionTaskGranularityConfig from(GranularitySpec granularitySpec)
+  {
+    if (granularitySpec == null) {
+      return null;
+    } else {
+      return new UserCompactionTaskGranularityConfig(
+          granularitySpec.getSegmentGranularity(),
+          granularitySpec.getQueryGranularity(),
+          granularitySpec.isRollup()
+      );
+    }
+  }
+
   @Override
   public boolean equals(Object o)
   {
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java
index 135852c1de..1dca23090c 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java
@@ -23,7 +23,6 @@ import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Preconditions;
-import com.google.common.base.Predicate;
 import org.apache.druid.client.DataSourcesSnapshot;
 import org.apache.druid.client.indexing.ClientCompactionIOConfig;
 import org.apache.druid.client.indexing.ClientCompactionIntervalSpec;
@@ -32,26 +31,22 @@ import org.apache.druid.client.indexing.ClientCompactionTaskDimensionsSpec;
 import org.apache.druid.client.indexing.ClientCompactionTaskGranularitySpec;
 import org.apache.druid.client.indexing.ClientCompactionTaskQuery;
 import org.apache.druid.client.indexing.ClientCompactionTaskQueryTuningConfig;
-import org.apache.druid.client.indexing.ClientMSQContext;
-import org.apache.druid.client.indexing.TaskPayloadResponse;
 import org.apache.druid.common.guava.FutureUtils;
 import org.apache.druid.common.utils.IdUtils;
 import org.apache.druid.data.input.impl.AggregateProjectionSpec;
 import org.apache.druid.indexer.CompactionEngine;
-import org.apache.druid.indexer.TaskStatus;
-import org.apache.druid.indexer.TaskStatusPlus;
-import org.apache.druid.indexer.partitions.DimensionRangePartitionsSpec;
-import org.apache.druid.java.util.common.ISE;
 import org.apache.druid.java.util.common.granularity.Granularity;
 import org.apache.druid.java.util.common.granularity.GranularityType;
 import org.apache.druid.java.util.common.logger.Logger;
-import org.apache.druid.metadata.LockFilterPolicy;
 import org.apache.druid.query.aggregation.AggregatorFactory;
 import org.apache.druid.rpc.indexing.OverlordClient;
 import org.apache.druid.segment.transform.CompactionTransformSpec;
 import org.apache.druid.server.compaction.CompactionCandidate;
 import org.apache.druid.server.compaction.CompactionCandidateSearchPolicy;
 import org.apache.druid.server.compaction.CompactionSegmentIterator;
+import org.apache.druid.server.compaction.CompactionSlotManager;
+import org.apache.druid.server.compaction.CompactionSnapshotBuilder;
+import org.apache.druid.server.compaction.CompactionStatus;
 import org.apache.druid.server.compaction.CompactionStatusTracker;
 import org.apache.druid.server.compaction.PriorityBasedCompactionSegmentIterator;
 import org.apache.druid.server.coordinator.AutoCompactionSnapshot;
@@ -66,10 +61,8 @@ import org.apache.druid.timeline.DataSegment;
 import org.joda.time.Interval;
 
 import javax.annotation.Nullable;
-import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -79,21 +72,16 @@ import java.util.stream.Collectors;
 
 public class CompactSegments implements CoordinatorCustomDuty
 {
-  /**
-   * Must be the same as org.apache.druid.indexing.common.task.CompactionTask.TYPE.
-   */
-  public static final String COMPACTION_TASK_TYPE = "compact";
   /**
    * Must be the same as org.apache.druid.indexing.common.task.Tasks.STORE_COMPACTION_STATE_KEY
    */
   public static final String STORE_COMPACTION_STATE_KEY = "storeCompactionState";
+  public static final String COMPACTION_INTERVAL_KEY = "compactionInterval";
   private static final String COMPACTION_REASON_KEY = "compactionReason";
 
   private static final Logger LOG = new Logger(CompactSegments.class);
 
   private static final String TASK_ID_PREFIX = "coordinator-issued";
-  private static final Predicate<TaskStatusPlus> IS_COMPACTION_TASK =
-      status -> null != status && COMPACTION_TASK_TYPE.equals(status.getType());
 
   private final CompactionStatusTracker statusTracker;
   private final OverlordClient overlordClient;
@@ -152,75 +140,36 @@ public class CompactSegments implements CoordinatorCustomDuty
     }
 
     statusTracker.onSegmentTimelineUpdated(dataSources.getSnapshotTime());
-    statusTracker.onCompactionConfigUpdated(dynamicConfig);
     List<DataSourceCompactionConfig> compactionConfigList = dynamicConfig.getCompactionConfigs();
     if (compactionConfigList == null || compactionConfigList.isEmpty()) {
       resetCompactionSnapshot();
+      statusTracker.resetActiveDatasources(Set.of());
       return;
     }
 
     Map<String, DataSourceCompactionConfig> compactionConfigs = compactionConfigList
         .stream()
         .collect(Collectors.toMap(DataSourceCompactionConfig::getDataSource, Function.identity()));
+    statusTracker.resetActiveDatasources(compactionConfigs.keySet());
 
-    // Map from dataSource to list of intervals for which compaction will be skipped in this run
-    final Map<String, List<Interval>> intervalsToSkipCompaction = new HashMap<>();
-
-    // Fetch currently running compaction tasks
-    int busyCompactionTaskSlots = 0;
-    final List<TaskStatusPlus> compactionTasks = CoordinatorDutyUtils.getStatusOfActiveTasks(
+    final CompactionSlotManager slotManager = new CompactionSlotManager(
         overlordClient,
-        IS_COMPACTION_TASK
+        statusTracker,
+        dynamicConfig.clusterConfig()
     );
+    stats.add(Stats.Compaction.MAX_SLOTS, slotManager.getNumAvailableTaskSlots());
 
-    final Set<String> activeTaskIds
-        = compactionTasks.stream().map(TaskStatusPlus::getId).collect(Collectors.toSet());
-    trackStatusOfCompletedTasks(activeTaskIds);
-
-    for (TaskStatusPlus status : compactionTasks) {
-      final TaskPayloadResponse response =
-          FutureUtils.getUnchecked(overlordClient.taskPayload(status.getId()), true);
-      if (response == null) {
-        throw new ISE("Could not find payload for active compaction task[%s]", status.getId());
-      } else if (!COMPACTION_TASK_TYPE.equals(response.getPayload().getType())) {
-        throw new ISE(
-            "Payload of active compaction task[%s] is of invalid type[%s]",
-            status.getId(), response.getPayload().getType()
-        );
-      }
-
-      final ClientCompactionTaskQuery compactionTaskQuery = (ClientCompactionTaskQuery) response.getPayload();
-      DataSourceCompactionConfig dataSourceCompactionConfig = compactionConfigs.get(status.getDataSource());
-      if (cancelTaskIfGranularityChanged(compactionTaskQuery, dataSourceCompactionConfig)) {
-        continue;
-      }
-
-      // Skip this interval as the current active compaction task is good
-      final Interval interval = compactionTaskQuery.getIoConfig().getInputSpec().getInterval();
-      intervalsToSkipCompaction.computeIfAbsent(status.getDataSource(), k -> new ArrayList<>())
-                               .add(interval);
-      // Note: The default compactionRunnerType used here should match the default runner used in CompactionTask when
-      // no runner is provided there.
-      CompactionEngine compactionRunnerType = compactionTaskQuery.getCompactionRunner() == null
-                                              ? CompactionEngine.NATIVE
-                                              : compactionTaskQuery.getCompactionRunner().getType();
-      if (compactionRunnerType == CompactionEngine.NATIVE) {
-        busyCompactionTaskSlots +=
-            findMaxNumTaskSlotsUsedByOneNativeCompactionTask(compactionTaskQuery.getTuningConfig());
-      } else {
-        busyCompactionTaskSlots += findMaxNumTaskSlotsUsedByOneMsqCompactionTask(compactionTaskQuery.getContext());
+    // Fetch currently running compaction tasks
+    for (ClientCompactionTaskQuery compactionTaskQuery : slotManager.fetchRunningCompactionTasks()) {
+      final String dataSource = compactionTaskQuery.getDataSource();
+      DataSourceCompactionConfig dataSourceCompactionConfig = compactionConfigs.get(dataSource);
+      if (slotManager.cancelTaskOnlyIfGranularityChanged(compactionTaskQuery, dataSourceCompactionConfig)) {
+        stats.add(Stats.Compaction.CANCELLED_TASKS, RowKey.of(Dimension.DATASOURCE, dataSource), 1L);
       }
     }
+    stats.add(Stats.Compaction.AVAILABLE_SLOTS, slotManager.getNumAvailableTaskSlots());
 
-    // Skip all the intervals locked by higher priority tasks for each datasource
-    // This must be done after the invalid compaction tasks are cancelled
-    // in the loop above so that their intervals are not considered locked
-    getLockedIntervals(compactionConfigList).forEach(
-        (dataSource, intervals) ->
-            intervalsToSkipCompaction
-                .computeIfAbsent(dataSource, ds -> new ArrayList<>())
-                .addAll(intervals)
-    );
+    slotManager.skipLockedIntervals(compactionConfigList);
 
     // Get iterator over segments to compact and submit compaction tasks
     final CompactionCandidateSearchPolicy policy = dynamicConfig.getCompactionPolicy();
@@ -228,27 +177,21 @@ public class CompactSegments implements CoordinatorCustomDuty
         policy,
         compactionConfigs,
         dataSources.getUsedSegmentsTimelinesPerDataSource(),
-        intervalsToSkipCompaction,
-        statusTracker
+        slotManager.getDatasourceIntervalsToSkipCompaction()
     );
 
-    final int compactionTaskCapacity = getCompactionTaskCapacity(dynamicConfig);
-    final int availableCompactionTaskSlots
-        = getAvailableCompactionTaskSlots(compactionTaskCapacity, busyCompactionTaskSlots);
-
-    final Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders = new HashMap<>();
+    final CompactionSnapshotBuilder compactionSnapshotBuilder = new CompactionSnapshotBuilder(stats);
     final int numSubmittedCompactionTasks = submitCompactionTasks(
         compactionConfigs,
-        currentRunAutoCompactionSnapshotBuilders,
-        availableCompactionTaskSlots,
+        compactionSnapshotBuilder,
+        slotManager,
         iterator,
+        policy,
         defaultEngine
     );
 
-    stats.add(Stats.Compaction.MAX_SLOTS, compactionTaskCapacity);
-    stats.add(Stats.Compaction.AVAILABLE_SLOTS, availableCompactionTaskSlots);
     stats.add(Stats.Compaction.SUBMITTED_TASKS, numSubmittedCompactionTasks);
-    updateCompactionSnapshotStats(currentRunAutoCompactionSnapshotBuilders, iterator, stats);
+    updateCompactionSnapshotStats(compactionSnapshotBuilder, iterator, compactionConfigs);
   }
 
   private void resetCompactionSnapshot()
@@ -271,334 +214,168 @@ public class CompactSegments implements CoordinatorCustomDuty
     }
   }
 
-  /**
-   * Queries the Overlord for the status of all tasks that were submitted
-   * recently but are not active anymore. The statuses are then updated in the
-   * {@link #statusTracker}.
-   */
-  private void trackStatusOfCompletedTasks(Set<String> activeTaskIds)
-  {
-    final Set<String> finishedTaskIds = new HashSet<>(statusTracker.getSubmittedTaskIds());
-    finishedTaskIds.removeAll(activeTaskIds);
-
-    if (finishedTaskIds.isEmpty()) {
-      return;
-    }
-
-    final Map<String, TaskStatus> taskStatusMap
-        = FutureUtils.getUnchecked(overlordClient.taskStatuses(finishedTaskIds), true);
-    for (String taskId : finishedTaskIds) {
-      // Assume unknown task to have finished successfully
-      final TaskStatus taskStatus = taskStatusMap.getOrDefault(taskId, TaskStatus.success(taskId));
-      if (taskStatus.isComplete()) {
-        statusTracker.onTaskFinished(taskId, taskStatus);
-      }
-    }
-  }
-
-  /**
-   * Cancels a currently running compaction task if the segment granularity
-   * for this datasource has changed in the compaction config.
-   *
-   * @return true if the task was canceled, false otherwise.
-   */
-  private boolean cancelTaskIfGranularityChanged(
-      ClientCompactionTaskQuery compactionTaskQuery,
-      DataSourceCompactionConfig dataSourceCompactionConfig
-  )
-  {
-    if (dataSourceCompactionConfig == null
-        || dataSourceCompactionConfig.getGranularitySpec() == null
-        || compactionTaskQuery.getGranularitySpec() == null) {
-      return false;
-    }
-
-    Granularity configuredSegmentGranularity = dataSourceCompactionConfig.getGranularitySpec()
-                                                                         .getSegmentGranularity();
-    Granularity taskSegmentGranularity = compactionTaskQuery.getGranularitySpec().getSegmentGranularity();
-    if (configuredSegmentGranularity == null || configuredSegmentGranularity.equals(taskSegmentGranularity)) {
-      return false;
-    }
-
-    LOG.info(
-        "Cancelling task[%s] as task segmentGranularity[%s] differs from compaction config segmentGranularity[%s].",
-        compactionTaskQuery.getId(), taskSegmentGranularity, configuredSegmentGranularity
-    );
-    overlordClient.cancelTask(compactionTaskQuery.getId());
-    return true;
-  }
-
-  /**
-   * Gets a List of Intervals locked by higher priority tasks for each datasource.
-   * However, when using a REPLACE lock for compaction, intervals locked with any APPEND lock will not be returned
-   * Since compaction tasks submitted for these Intervals would have to wait anyway,
-   * we skip these Intervals until the next compaction run.
-   * <p>
-   * For now, Segment Locks are being treated the same as Time Chunk Locks even
-   * though they lock only a Segment and not the entire Interval. Thus,
-   * a compaction task will not be submitted for an Interval if
-   * <ul>
-   *   <li>either the whole Interval is locked by a higher priority Task with an incompatible lock type</li>
-   *   <li>or there is atleast one Segment in the Interval that is locked by a
-   *   higher priority Task</li>
-   * </ul>
-   */
-  private Map<String, List<Interval>> getLockedIntervals(
-      List<DataSourceCompactionConfig> compactionConfigs
-  )
-  {
-    final List<LockFilterPolicy> lockFilterPolicies = compactionConfigs
-        .stream()
-        .map(config ->
-                 new LockFilterPolicy(config.getDataSource(), config.getTaskPriority(), null, config.getTaskContext()))
-        .collect(Collectors.toList());
-    final Map<String, List<Interval>> datasourceToLockedIntervals =
-        new HashMap<>(FutureUtils.getUnchecked(overlordClient.findLockedIntervals(lockFilterPolicies), true));
-    LOG.debug(
-        "Skipping the following intervals for Compaction as they are currently locked: %s",
-        datasourceToLockedIntervals
-    );
-
-    return datasourceToLockedIntervals;
-  }
-
-  /**
-   * Returns the maximum number of task slots used by one native compaction task at any time when the task is
-   * issued with the given tuningConfig.
-   */
-  public static int findMaxNumTaskSlotsUsedByOneNativeCompactionTask(
-      @Nullable ClientCompactionTaskQueryTuningConfig tuningConfig
-  )
-  {
-    if (isParallelMode(tuningConfig)) {
-      @Nullable
-      Integer maxNumConcurrentSubTasks = tuningConfig.getMaxNumConcurrentSubTasks();
-      // Max number of task slots used in parallel mode = maxNumConcurrentSubTasks + 1 (supervisor task)
-      return (maxNumConcurrentSubTasks == null ? 1 : maxNumConcurrentSubTasks) + 1;
-    } else {
-      return 1;
-    }
-  }
-
-  /**
-   * Returns the maximum number of task slots used by one MSQ compaction task at any time when the task is
-   * issued with the given context.
-   */
-  static int findMaxNumTaskSlotsUsedByOneMsqCompactionTask(@Nullable Map<String, Object> context)
-  {
-    return context == null
-           ? ClientMSQContext.DEFAULT_MAX_NUM_TASKS
-           : (int) context.getOrDefault(ClientMSQContext.CTX_MAX_NUM_TASKS, ClientMSQContext.DEFAULT_MAX_NUM_TASKS);
-  }
-
-
-  /**
-   * Returns true if the compaction task can run in the parallel mode with the given tuningConfig.
-   * This method should be synchronized with ParallelIndexSupervisorTask.isParallelMode(InputSource, ParallelIndexTuningConfig).
-   */
-  @VisibleForTesting
-  static boolean isParallelMode(@Nullable ClientCompactionTaskQueryTuningConfig tuningConfig)
-  {
-    if (null == tuningConfig) {
-      return false;
-    }
-    boolean useRangePartitions = useRangePartitions(tuningConfig);
-    int minRequiredNumConcurrentSubTasks = useRangePartitions ? 1 : 2;
-    return tuningConfig.getMaxNumConcurrentSubTasks() != null
-           && tuningConfig.getMaxNumConcurrentSubTasks() >= minRequiredNumConcurrentSubTasks;
-  }
-
-  private static boolean useRangePartitions(ClientCompactionTaskQueryTuningConfig tuningConfig)
-  {
-    // dynamic partitionsSpec will be used if getPartitionsSpec() returns null
-    return tuningConfig.getPartitionsSpec() instanceof DimensionRangePartitionsSpec;
-  }
-
-  private int getCompactionTaskCapacity(DruidCompactionConfig dynamicConfig)
-  {
-    int totalWorkerCapacity = CoordinatorDutyUtils.getTotalWorkerCapacity(overlordClient);
-
-    return Math.min(
-        (int) (totalWorkerCapacity * dynamicConfig.getCompactionTaskSlotRatio()),
-        dynamicConfig.getMaxCompactionTaskSlots()
-    );
-  }
-
-  private int getAvailableCompactionTaskSlots(int compactionTaskCapacity, int busyCompactionTaskSlots)
-  {
-    final int availableCompactionTaskSlots;
-    if (busyCompactionTaskSlots > 0) {
-      availableCompactionTaskSlots = Math.max(0, compactionTaskCapacity - busyCompactionTaskSlots);
-    } else {
-      // compactionTaskCapacity might be 0 if totalWorkerCapacity is low.
-      // This guarantees that at least one slot is available if
-      // compaction is enabled and estimatedIncompleteCompactionTasks is 0.
-      availableCompactionTaskSlots = Math.max(1, compactionTaskCapacity);
-    }
-    LOG.debug(
-        "Found [%d] available task slots for compaction out of max compaction task capacity [%d]",
-        availableCompactionTaskSlots, compactionTaskCapacity
-    );
-
-    return availableCompactionTaskSlots;
-  }
-
   /**
    * Submits compaction tasks to the Overlord. Returns total number of tasks submitted.
    */
   private int submitCompactionTasks(
       Map<String, DataSourceCompactionConfig> compactionConfigs,
-      Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders,
-      int numAvailableCompactionTaskSlots,
+      CompactionSnapshotBuilder snapshotBuilder,
+      CompactionSlotManager slotManager,
       CompactionSegmentIterator iterator,
+      CompactionCandidateSearchPolicy policy,
       CompactionEngine defaultEngine
   )
   {
-    if (numAvailableCompactionTaskSlots <= 0) {
+    if (slotManager.getNumAvailableTaskSlots() <= 0) {
       return 0;
     }
 
     int numSubmittedTasks = 0;
     int totalTaskSlotsAssigned = 0;
 
-    while (iterator.hasNext() && totalTaskSlotsAssigned < numAvailableCompactionTaskSlots) {
+    while (iterator.hasNext() && totalTaskSlotsAssigned < slotManager.getNumAvailableTaskSlots()) {
       final CompactionCandidate entry = iterator.next();
       final String dataSourceName = entry.getDataSource();
+      final DataSourceCompactionConfig config = compactionConfigs.get(dataSourceName);
 
-      // As these segments will be compacted, we will aggregate the statistic to the Compacted statistics
-      currentRunAutoCompactionSnapshotBuilders
-          .computeIfAbsent(dataSourceName, AutoCompactionSnapshot::builder)
-          .incrementCompactedStats(entry.getStats());
+      final CompactionStatus compactionStatus =
+          statusTracker.computeCompactionStatus(entry, policy);
+      final CompactionCandidate candidatesWithStatus = entry.withCurrentStatus(compactionStatus);
+      statusTracker.onCompactionStatusComputed(candidatesWithStatus, config);
 
-      final DataSourceCompactionConfig config = compactionConfigs.get(dataSourceName);
-      final List<DataSegment> segmentsToCompact = entry.getSegments();
-      // ^ feed segmentsToCompact to config to get grouping of configs to segments
-
-      // Create granularitySpec to send to compaction task
-      Granularity segmentGranularityToUse = null;
-      if (config.getGranularitySpec() == null || config.getGranularitySpec().getSegmentGranularity() == null) {
-        // Determines segmentGranularity from the segmentsToCompact
-        // Each batch of segmentToCompact from CompactionSegmentIterator will contain the same interval as
-        // segmentGranularity is not set in the compaction config
-        Interval interval = segmentsToCompact.get(0).getInterval();
-        if (segmentsToCompact.stream().allMatch(segment -> interval.overlaps(segment.getInterval()))) {
-          try {
-            segmentGranularityToUse = GranularityType.fromPeriod(interval.toPeriod()).getDefaultGranularity();
-          }
-          catch (IllegalArgumentException iae) {
-            // This case can happen if the existing segment interval result in complicated periods.
-            // Fall back to setting segmentGranularity as null
-            LOG.warn("Cannot determine segmentGranularity from interval[%s].", interval);
-          }
-        } else {
-          LOG.warn(
-              "Not setting 'segmentGranularity' for auto-compaction task as"
-              + " the segments to compact do not have the same interval."
-          );
-        }
+      if (compactionStatus.isComplete()) {
+        snapshotBuilder.addToComplete(candidatesWithStatus);
+      } else if (compactionStatus.isSkipped()) {
+        snapshotBuilder.addToSkipped(candidatesWithStatus);
       } else {
-        segmentGranularityToUse = config.getGranularitySpec().getSegmentGranularity();
+        // As these segments will be compacted, we will aggregate the statistic to the Compacted statistics
+        snapshotBuilder.addToComplete(entry);
       }
-      final ClientCompactionTaskGranularitySpec granularitySpec = new ClientCompactionTaskGranularitySpec(
-          segmentGranularityToUse,
-          config.getGranularitySpec() != null ? config.getGranularitySpec().getQueryGranularity() : null,
-          config.getGranularitySpec() != null ? config.getGranularitySpec().isRollup() : null
-      );
 
-      // Create dimensionsSpec to send to compaction task
-      final ClientCompactionTaskDimensionsSpec dimensionsSpec;
-      if (config.getDimensionsSpec() != null) {
-        dimensionsSpec = new ClientCompactionTaskDimensionsSpec(
-            config.getDimensionsSpec().getDimensions()
-        );
-      } else {
-        dimensionsSpec = null;
-      }
+      final ClientCompactionTaskQuery taskPayload = createCompactionTask(entry, config, defaultEngine);
 
-      Boolean dropExisting = null;
-      if (config.getIoConfig() != null) {
-        dropExisting = config.getIoConfig().isDropExisting();
-      }
+      final String taskId = taskPayload.getId();
+      FutureUtils.getUnchecked(overlordClient.runTask(taskId, taskPayload), true);
+      statusTracker.onTaskSubmitted(taskId, entry);
 
-      // If all the segments found to be compacted are tombstones then dropExisting
-      // needs to be forced to true. This forcing needs to  happen in the case that
-      // the flag is null, or it is false. It is needed when it is null to avoid the
-      // possibility of the code deciding to default it to false later.
-      // Forcing the flag to true will enable the task ingestion code to generate new, compacted, tombstones to
-      // cover the tombstones found to be compacted as well as to mark them
-      // as compacted (update their lastCompactionState). If we don't force the
-      // flag then every time this compact duty runs it will find the same tombstones
-      // in the interval since their lastCompactionState
-      // was not set repeating this over and over and the duty will not make progress; it
-      // will become stuck on this set of tombstones.
-      // This forcing code should be revised
-      // when/if the autocompaction code policy to decide which segments to compact changes
-      if (dropExisting == null || !dropExisting) {
-        if (segmentsToCompact.stream().allMatch(DataSegment::isTombstone)) {
-          dropExisting = true;
-          LOG.info("Forcing dropExisting to true since all segments to compact are tombstones.");
-        }
-      }
+      LOG.debug(
+          "Submitted a compaction task[%s] for [%d] segments in datasource[%s], umbrella interval[%s].",
+          taskId, entry.numSegments(), dataSourceName, entry.getUmbrellaInterval()
+      );
+      LOG.debugSegments(entry.getSegments(), "Compacting segments");
+      numSubmittedTasks++;
+      totalTaskSlotsAssigned += slotManager.computeSlotsRequiredForTask(taskPayload, config);
+    }
 
-      final CompactionEngine compactionEngine = config.getEngine() == null ? defaultEngine : config.getEngine();
-      final Map<String, Object> autoCompactionContext = newAutoCompactionContext(config.getTaskContext());
-      int slotsRequiredForCurrentTask;
-
-      if (compactionEngine == CompactionEngine.MSQ) {
-        if (autoCompactionContext.containsKey(ClientMSQContext.CTX_MAX_NUM_TASKS)) {
-          slotsRequiredForCurrentTask = (int) autoCompactionContext.get(ClientMSQContext.CTX_MAX_NUM_TASKS);
-        } else {
-          // Since MSQ needs all task slots for the calculated #tasks to be available upfront, allot all available
-          // compaction slots (upto a max of MAX_TASK_SLOTS_FOR_MSQ_COMPACTION) to current compaction task to avoid
-          // stalling. Setting "taskAssignment" to "auto" has the problem of not being able to determine the actual
-          // count, which is required for subsequent tasks.
-          slotsRequiredForCurrentTask = Math.min(
-              // Update the slots to 2 (min required for MSQ) if only 1 slot is available.
-              numAvailableCompactionTaskSlots == 1 ? 2 : numAvailableCompactionTaskSlots,
-              ClientMSQContext.MAX_TASK_SLOTS_FOR_MSQ_COMPACTION_TASK
-          );
-          autoCompactionContext.put(ClientMSQContext.CTX_MAX_NUM_TASKS, slotsRequiredForCurrentTask);
+    LOG.info("Submitted a total of [%d] compaction tasks.", numSubmittedTasks);
+    return numSubmittedTasks;
+  }
+
+  /**
+   * Creates a {@link ClientCompactionTaskQuery} which can be submitted to an
+   * {@link OverlordClient} to start a compaction task.
+   */
+  public static ClientCompactionTaskQuery createCompactionTask(
+      CompactionCandidate candidate,
+      DataSourceCompactionConfig config,
+      CompactionEngine defaultEngine
+  )
+  {
+    final List<DataSegment> segmentsToCompact = candidate.getSegments();
+
+    // Create granularitySpec to send to compaction task
+    Granularity segmentGranularityToUse = null;
+    if (config.getGranularitySpec() == null || config.getGranularitySpec().getSegmentGranularity() == null) {
+      // Determines segmentGranularity from the segmentsToCompact
+      // Each batch of segmentToCompact from CompactionSegmentIterator will contain the same interval as
+      // segmentGranularity is not set in the compaction config
+      Interval interval = segmentsToCompact.get(0).getInterval();
+      if (segmentsToCompact.stream().allMatch(segment -> interval.overlaps(segment.getInterval()))) {
+        try {
+          segmentGranularityToUse = GranularityType.fromPeriod(interval.toPeriod()).getDefaultGranularity();
+        }
+        catch (IllegalArgumentException iae) {
+          // This case can happen if the existing segment interval result in complicated periods.
+          // Fall back to setting segmentGranularity as null
+          LOG.warn("Cannot determine segmentGranularity from interval[%s].", interval);
         }
       } else {
-        slotsRequiredForCurrentTask = findMaxNumTaskSlotsUsedByOneNativeCompactionTask(config.getTuningConfig());
+        LOG.warn(
+            "Not setting 'segmentGranularity' for auto-compaction task as"
+            + " the segments to compact do not have the same interval."
+        );
       }
+    } else {
+      segmentGranularityToUse = config.getGranularitySpec().getSegmentGranularity();
+    }
+    final ClientCompactionTaskGranularitySpec granularitySpec = new ClientCompactionTaskGranularitySpec(
+        segmentGranularityToUse,
+        config.getGranularitySpec() != null ? config.getGranularitySpec().getQueryGranularity() : null,
+        config.getGranularitySpec() != null ? config.getGranularitySpec().isRollup() : null
+    );
+
+    // Create dimensionsSpec to send to compaction task
+    final ClientCompactionTaskDimensionsSpec dimensionsSpec;
+    if (config.getDimensionsSpec() != null) {
+      dimensionsSpec = new ClientCompactionTaskDimensionsSpec(
+          config.getDimensionsSpec().getDimensions()
+      );
+    } else {
+      dimensionsSpec = null;
+    }
 
-      if (entry.getCurrentStatus() != null) {
-        autoCompactionContext.put(COMPACTION_REASON_KEY, entry.getCurrentStatus().getReason());
+    Boolean dropExisting = null;
+    if (config.getIoConfig() != null) {
+      dropExisting = config.getIoConfig().isDropExisting();
+    }
+
+    // If all the segments found to be compacted are tombstones then dropExisting
+    // needs to be forced to true. This forcing needs to  happen in the case that
+    // the flag is null, or it is false. It is needed when it is null to avoid the
+    // possibility of the code deciding to default it to false later.
+    // Forcing the flag to true will enable the task ingestion code to generate new, compacted, tombstones to
+    // cover the tombstones found to be compacted as well as to mark them
+    // as compacted (update their lastCompactionState). If we don't force the
+    // flag then every time this compact duty runs it will find the same tombstones
+    // in the interval since their lastCompactionState
+    // was not set repeating this over and over and the duty will not make progress; it
+    // will become stuck on this set of tombstones.
+    // This forcing code should be revised
+    // when/if the autocompaction code policy to decide which segments to compact changes
+    if (dropExisting == null || !dropExisting) {
+      if (segmentsToCompact.stream().allMatch(DataSegment::isTombstone)) {
+        dropExisting = true;
+        LOG.info("Forcing dropExisting to true since all segments to compact are tombstones.");
       }
+    }
 
-      final String taskId = compactSegments(
-          entry,
-          config.getTaskPriority(),
-          ClientCompactionTaskQueryTuningConfig.from(
-              config.getTuningConfig(),
-              config.getMaxRowsPerSegment(),
-              config.getMetricsSpec() != null
-          ),
-          granularitySpec,
-          dimensionsSpec,
-          config.getMetricsSpec(),
-          config.getTransformSpec(),
-          config.getProjections(),
-          dropExisting,
-          autoCompactionContext,
-          new ClientCompactionRunnerInfo(compactionEngine)
-      );
+    final CompactionEngine compactionEngine = config.getEngine() == null ? defaultEngine : config.getEngine();
+    final Map<String, Object> autoCompactionContext = newAutoCompactionContext(config.getTaskContext());
 
-      LOG.debug(
-          "Submitted a compaction task[%s] for [%d] segments in datasource[%s], umbrella interval[%s].",
-          taskId, segmentsToCompact.size(), dataSourceName, entry.getUmbrellaInterval()
-      );
-      LOG.debugSegments(segmentsToCompact, "Compacting segments");
-      numSubmittedTasks++;
-      totalTaskSlotsAssigned += slotsRequiredForCurrentTask;
+    if (candidate.getCurrentStatus() != null) {
+      autoCompactionContext.put(COMPACTION_REASON_KEY, candidate.getCurrentStatus().getReason());
     }
 
-    LOG.info("Submitted a total of [%d] compaction tasks.", numSubmittedTasks);
-    return numSubmittedTasks;
+    return compactSegments(
+        candidate,
+        config.getTaskPriority(),
+        ClientCompactionTaskQueryTuningConfig.from(
+            config.getTuningConfig(),
+            config.getMaxRowsPerSegment(),
+            config.getMetricsSpec() != null
+        ),
+        granularitySpec,
+        dimensionsSpec,
+        config.getMetricsSpec(),
+        config.getTransformSpec(),
+        config.getProjections(),
+        dropExisting,
+        autoCompactionContext,
+        new ClientCompactionRunnerInfo(compactionEngine)
+    );
   }
 
-  private Map<String, Object> newAutoCompactionContext(@Nullable Map<String, Object> configuredContext)
+  private static Map<String, Object> newAutoCompactionContext(@Nullable Map<String, Object> configuredContext)
   {
     final Map<String, Object> newContext = configuredContext == null
                                            ? new HashMap<>()
@@ -608,60 +385,23 @@ public class CompactSegments implements CoordinatorCustomDuty
   }
 
   private void updateCompactionSnapshotStats(
-      Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders,
+      CompactionSnapshotBuilder snapshotBuilder,
       CompactionSegmentIterator iterator,
-      CoordinatorRunStats stats
+      Map<String, DataSourceCompactionConfig> datasourceToConfig
   )
   {
     // Mark all the segments remaining in the iterator as "awaiting compaction"
     while (iterator.hasNext()) {
-      final CompactionCandidate entry = iterator.next();
-      currentRunAutoCompactionSnapshotBuilders
-          .computeIfAbsent(entry.getDataSource(), AutoCompactionSnapshot::builder)
-          .incrementWaitingStats(entry.getStats());
+      snapshotBuilder.addToPending(iterator.next());
     }
-
-    // Statistics of all segments considered compacted after this run
-    iterator.getCompactedSegments().forEach(
-        candidateSegments -> currentRunAutoCompactionSnapshotBuilders
-            .computeIfAbsent(candidateSegments.getDataSource(), AutoCompactionSnapshot::builder)
-            .incrementCompactedStats(candidateSegments.getStats())
-    );
-
-    // Statistics of all segments considered skipped after this run
-    iterator.getSkippedSegments().forEach(
-        candidateSegments -> currentRunAutoCompactionSnapshotBuilders
-            .computeIfAbsent(candidateSegments.getDataSource(), AutoCompactionSnapshot::builder)
-            .incrementSkippedStats(candidateSegments.getStats())
-    );
-
-    final Map<String, AutoCompactionSnapshot> currentAutoCompactionSnapshotPerDataSource = new HashMap<>();
-    currentRunAutoCompactionSnapshotBuilders.forEach((dataSource, builder) -> {
-      final AutoCompactionSnapshot autoCompactionSnapshot = builder.build();
-      currentAutoCompactionSnapshotPerDataSource.put(dataSource, autoCompactionSnapshot);
-      collectSnapshotStats(autoCompactionSnapshot, stats);
+    iterator.getCompactedSegments().forEach(snapshotBuilder::addToComplete);
+    iterator.getSkippedSegments().forEach(entry -> {
+      statusTracker.onCompactionStatusComputed(entry, datasourceToConfig.get(entry.getDataSource()));
+      snapshotBuilder.addToSkipped(entry);
     });
 
     // Atomic update of autoCompactionSnapshotPerDataSource with the latest from this coordinator run
-    autoCompactionSnapshotPerDataSource.set(currentAutoCompactionSnapshotPerDataSource);
-  }
-
-  private void collectSnapshotStats(
-      AutoCompactionSnapshot autoCompactionSnapshot,
-      CoordinatorRunStats stats
-  )
-  {
-    final RowKey rowKey = RowKey.of(Dimension.DATASOURCE, autoCompactionSnapshot.getDataSource());
-
-    stats.add(Stats.Compaction.PENDING_BYTES, rowKey, autoCompactionSnapshot.getBytesAwaitingCompaction());
-    stats.add(Stats.Compaction.PENDING_SEGMENTS, rowKey, autoCompactionSnapshot.getSegmentCountAwaitingCompaction());
-    stats.add(Stats.Compaction.PENDING_INTERVALS, rowKey, autoCompactionSnapshot.getIntervalCountAwaitingCompaction());
-    stats.add(Stats.Compaction.COMPACTED_BYTES, rowKey, autoCompactionSnapshot.getBytesCompacted());
-    stats.add(Stats.Compaction.COMPACTED_SEGMENTS, rowKey, autoCompactionSnapshot.getSegmentCountCompacted());
-    stats.add(Stats.Compaction.COMPACTED_INTERVALS, rowKey, autoCompactionSnapshot.getIntervalCountCompacted());
-    stats.add(Stats.Compaction.SKIPPED_BYTES, rowKey, autoCompactionSnapshot.getBytesSkipped());
-    stats.add(Stats.Compaction.SKIPPED_SEGMENTS, rowKey, autoCompactionSnapshot.getSegmentCountSkipped());
-    stats.add(Stats.Compaction.SKIPPED_INTERVALS, rowKey, autoCompactionSnapshot.getIntervalCountSkipped());
+    autoCompactionSnapshotPerDataSource.set(snapshotBuilder.build());
   }
 
   @Nullable
@@ -675,7 +415,7 @@ public class CompactSegments implements CoordinatorCustomDuty
     return autoCompactionSnapshotPerDataSource.get();
   }
 
-  private String compactSegments(
+  private static ClientCompactionTaskQuery compactSegments(
       CompactionCandidate entry,
       int compactionTaskPriority,
       ClientCompactionTaskQueryTuningConfig tuningConfig,
@@ -685,7 +425,7 @@ public class CompactSegments implements CoordinatorCustomDuty
       @Nullable CompactionTransformSpec transformSpec,
       @Nullable List<AggregateProjectionSpec> projectionSpecs,
       @Nullable Boolean dropExisting,
-      @Nullable Map<String, Object> context,
+      Map<String, Object> context,
       ClientCompactionRunnerInfo compactionRunner
   )
   {
@@ -698,17 +438,15 @@ public class CompactSegments implements CoordinatorCustomDuty
         "Segments must have the same dataSource"
     );
 
-    context = context == null ? new HashMap<>() : context;
     context.put("priority", compactionTaskPriority);
 
     final String taskId = IdUtils.newTaskId(TASK_ID_PREFIX, ClientCompactionTaskQuery.TYPE, dataSource, null);
-    final Granularity segmentGranularity = granularitySpec == null ? null : granularitySpec.getSegmentGranularity();
 
-    final ClientCompactionTaskQuery taskPayload = new ClientCompactionTaskQuery(
+    return new ClientCompactionTaskQuery(
         taskId,
         dataSource,
         new ClientCompactionIOConfig(
-            ClientCompactionIntervalSpec.fromSegments(segments, segmentGranularity),
+            new ClientCompactionIntervalSpec(entry.getCompactionInterval(), null),
             dropExisting
         ),
         tuningConfig,
@@ -720,9 +458,5 @@ public class CompactSegments implements CoordinatorCustomDuty
         context,
         compactionRunner
     );
-    FutureUtils.getUnchecked(overlordClient.runTask(taskId, taskPayload), true);
-    statusTracker.onTaskSubmitted(taskPayload, entry);
-
-    return taskId;
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/stats/CoordinatorRunStats.java b/server/src/main/java/org/apache/druid/server/coordinator/stats/CoordinatorRunStats.java
index 76d788cb43..a9473b298c 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/stats/CoordinatorRunStats.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/stats/CoordinatorRunStats.java
@@ -95,6 +95,20 @@ public class CoordinatorRunStats
     return statValues == null ? 0 : statValues.getLong(stat);
   }
 
+  /**
+   * Invokes the {@link StatHandler} for each value of the specified stat.
+   */
+  public void forEachEntry(CoordinatorStat stat, StatHandler handler)
+  {
+    allStats.forEach(
+        (rowKey, stats) -> stats.object2LongEntrySet().fastForEach(entry -> {
+          if (entry.getKey().equals(stat)) {
+            handler.handle(stat, rowKey, entry.getLongValue());
+          }
+        })
+    );
+  }
+
   public void forEachStat(StatHandler handler)
   {
     allStats.forEach(
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/stats/Stats.java b/server/src/main/java/org/apache/druid/server/coordinator/stats/Stats.java
index f4ae1b5f8a..3851b38fc9 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/stats/Stats.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/stats/Stats.java
@@ -110,8 +110,17 @@ public class Stats
 
   public static class Compaction
   {
+    public static final CoordinatorStat JOB_CREATION_TIME
+        = CoordinatorStat.toDebugAndEmit("jobCreationTime", "compact/createJobs/time");
+    public static final CoordinatorStat CREATED_JOBS
+        = CoordinatorStat.toDebugAndEmit("jobQueueSize", "compact/createJobs/count");
+    public static final CoordinatorStat SCHEDULER_RUN_TIME
+        = CoordinatorStat.toDebugAndEmit("schedulerRunTime", "compact/runScheduler/time");
+
     public static final CoordinatorStat SUBMITTED_TASKS
         = CoordinatorStat.toDebugAndEmit("compactTasks", "compact/task/count");
+    public static final CoordinatorStat CANCELLED_TASKS
+        = CoordinatorStat.toDebugAndEmit("compactCancelled", "compactTask/cancelled/count");
     public static final CoordinatorStat MAX_SLOTS
         = CoordinatorStat.toDebugAndEmit("compactMaxSlots", "compactTask/maxSlot/count");
     public static final CoordinatorStat AVAILABLE_SLOTS
diff --git a/server/src/test/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpecTest.java b/server/src/test/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpecTest.java
index 646e798aa6..46ecc64d72 100644
--- a/server/src/test/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpecTest.java
+++ b/server/src/test/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpecTest.java
@@ -24,6 +24,7 @@ import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.java.util.common.granularity.Granularities;
 import org.apache.druid.segment.IndexIO;
+import org.apache.druid.server.compaction.CompactionCandidate;
 import org.apache.druid.timeline.DataSegment;
 import org.apache.druid.timeline.partition.NoneShardSpec;
 import org.junit.Assert;
@@ -72,43 +73,43 @@ public class ClientCompactionIntervalSpecTest
   public void testFromSegmentWithNoSegmentGranularity()
   {
     // The umbrella interval of segments is 2015-02-12/2015-04-14
-    ClientCompactionIntervalSpec actual = ClientCompactionIntervalSpec.fromSegments(ImmutableList.of(dataSegment1, dataSegment2, dataSegment3), null);
-    Assert.assertEquals(Intervals.of("2015-02-12/2015-04-14"), actual.getInterval());
+    CompactionCandidate actual = CompactionCandidate.from(ImmutableList.of(dataSegment1, dataSegment2, dataSegment3), null);
+    Assert.assertEquals(Intervals.of("2015-02-12/2015-04-14"), actual.getCompactionInterval());
   }
 
   @Test
   public void testFromSegmentWitSegmentGranularitySameAsSegment()
   {
     // The umbrella interval of segments is 2015-04-11/2015-04-12
-    ClientCompactionIntervalSpec actual = ClientCompactionIntervalSpec.fromSegments(ImmutableList.of(dataSegment1), Granularities.DAY);
-    Assert.assertEquals(Intervals.of("2015-04-11/2015-04-12"), actual.getInterval());
+    CompactionCandidate actual = CompactionCandidate.from(ImmutableList.of(dataSegment1), Granularities.DAY);
+    Assert.assertEquals(Intervals.of("2015-04-11/2015-04-12"), actual.getCompactionInterval());
   }
 
   @Test
   public void testFromSegmentWithCoarserSegmentGranularity()
   {
     // The umbrella interval of segments is 2015-02-12/2015-04-14
-    ClientCompactionIntervalSpec actual = ClientCompactionIntervalSpec.fromSegments(ImmutableList.of(dataSegment1, dataSegment2, dataSegment3), Granularities.YEAR);
+    CompactionCandidate actual = CompactionCandidate.from(ImmutableList.of(dataSegment1, dataSegment2, dataSegment3), Granularities.YEAR);
     // The compaction interval should be expanded to start of the year and end of the year to cover the segmentGranularity
-    Assert.assertEquals(Intervals.of("2015-01-01/2016-01-01"), actual.getInterval());
+    Assert.assertEquals(Intervals.of("2015-01-01/2016-01-01"), actual.getCompactionInterval());
   }
 
   @Test
   public void testFromSegmentWithFinerSegmentGranularityAndUmbrellaIntervalAlign()
   {
     // The umbrella interval of segments is 2015-02-12/2015-04-14
-    ClientCompactionIntervalSpec actual = ClientCompactionIntervalSpec.fromSegments(ImmutableList.of(dataSegment1, dataSegment2, dataSegment3), Granularities.DAY);
+    CompactionCandidate actual = CompactionCandidate.from(ImmutableList.of(dataSegment1, dataSegment2, dataSegment3), Granularities.DAY);
     // The segmentGranularity of DAY align with the umbrella interval (umbrella interval can be evenly divide into the segmentGranularity)
-    Assert.assertEquals(Intervals.of("2015-02-12/2015-04-14"), actual.getInterval());
+    Assert.assertEquals(Intervals.of("2015-02-12/2015-04-14"), actual.getCompactionInterval());
   }
 
   @Test
   public void testFromSegmentWithFinerSegmentGranularityAndUmbrellaIntervalNotAlign()
   {
     // The umbrella interval of segments is 2015-02-12/2015-04-14
-    ClientCompactionIntervalSpec actual = ClientCompactionIntervalSpec.fromSegments(ImmutableList.of(dataSegment1, dataSegment2, dataSegment3), Granularities.WEEK);
+    CompactionCandidate actual = CompactionCandidate.from(ImmutableList.of(dataSegment1, dataSegment2, dataSegment3), Granularities.WEEK);
     // The segmentGranularity of WEEK does not align with the umbrella interval (umbrella interval cannot be evenly divide into the segmentGranularity)
     // Hence the compaction interval is modified to aling with the segmentGranularity
-    Assert.assertEquals(Intervals.of("2015-02-09/2015-04-20"), actual.getInterval());
+    Assert.assertEquals(Intervals.of("2015-02-09/2015-04-20"), actual.getCompactionInterval());
   }
 }
diff --git a/server/src/test/java/org/apache/druid/server/compaction/CompactionRunSimulatorTest.java b/server/src/test/java/org/apache/druid/server/compaction/CompactionRunSimulatorTest.java
index fd491ef930..bab2e351df 100644
--- a/server/src/test/java/org/apache/druid/server/compaction/CompactionRunSimulatorTest.java
+++ b/server/src/test/java/org/apache/druid/server/compaction/CompactionRunSimulatorTest.java
@@ -19,14 +19,12 @@
 
 package org.apache.druid.server.compaction;
 
-import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
 import org.apache.druid.client.indexing.TaskPayloadResponse;
 import org.apache.druid.indexer.CompactionEngine;
 import org.apache.druid.indexer.TaskStatus;
 import org.apache.druid.indexer.TaskStatusPlus;
-import org.apache.druid.jackson.DefaultObjectMapper;
 import org.apache.druid.java.util.common.CloseableIterators;
 import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.java.util.common.granularity.Granularities;
@@ -51,10 +49,8 @@ import java.util.Set;
 
 public class CompactionRunSimulatorTest
 {
-  private static final ObjectMapper OBJECT_MAPPER = new DefaultObjectMapper();
-
   private final CompactionRunSimulator simulator = new CompactionRunSimulator(
-      new CompactionStatusTracker(OBJECT_MAPPER),
+      new CompactionStatusTracker(),
       new TestOverlordClient()
   );
 
@@ -115,7 +111,7 @@ public class CompactionRunSimulatorTest
     );
     Assert.assertEquals(
         Collections.singletonList(
-            Arrays.asList("wiki", Intervals.of("2013-01-10/P1D"), 10, 1_000_000_000L, "skip offset from latest[P1D]")
+            Arrays.asList("wiki", Intervals.of("2013-01-10/P1D"), 10, 1_000_000_000L, 1, "skip offset from latest[P1D]")
         ),
         skippedTable.getRows()
     );
diff --git a/server/src/test/java/org/apache/druid/server/compaction/CompactionStatusTest.java b/server/src/test/java/org/apache/druid/server/compaction/CompactionStatusTest.java
index 14af1c78da..d201b84135 100644
--- a/server/src/test/java/org/apache/druid/server/compaction/CompactionStatusTest.java
+++ b/server/src/test/java/org/apache/druid/server/compaction/CompactionStatusTest.java
@@ -19,7 +19,6 @@
 
 package org.apache.druid.server.compaction;
 
-import com.fasterxml.jackson.databind.ObjectMapper;
 import org.apache.druid.client.indexing.ClientCompactionTaskQueryTuningConfig;
 import org.apache.druid.data.input.impl.AggregateProjectionSpec;
 import org.apache.druid.data.input.impl.DimensionsSpec;
@@ -31,7 +30,6 @@ import org.apache.druid.indexer.partitions.DimensionRangePartitionsSpec;
 import org.apache.druid.indexer.partitions.DynamicPartitionsSpec;
 import org.apache.druid.indexer.partitions.HashedPartitionsSpec;
 import org.apache.druid.indexer.partitions.PartitionsSpec;
-import org.apache.druid.jackson.DefaultObjectMapper;
 import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.java.util.common.granularity.Granularities;
 import org.apache.druid.query.aggregation.LongSumAggregatorFactory;
@@ -47,6 +45,7 @@ import org.apache.druid.server.coordinator.UserCompactionTaskGranularityConfig;
 import org.apache.druid.server.coordinator.UserCompactionTaskQueryTuningConfig;
 import org.apache.druid.timeline.CompactionState;
 import org.apache.druid.timeline.DataSegment;
+import org.apache.druid.timeline.SegmentId;
 import org.junit.Assert;
 import org.junit.Test;
 
@@ -55,14 +54,9 @@ import java.util.List;
 
 public class CompactionStatusTest
 {
-  private static final ObjectMapper OBJECT_MAPPER = new DefaultObjectMapper();
-
   private static final DataSegment WIKI_SEGMENT
-      = DataSegment.builder()
-                   .dataSource(TestDataSource.WIKI)
-                   .interval(Intervals.of("2013-01-01/PT1H"))
+      = DataSegment.builder(SegmentId.of(TestDataSource.WIKI, Intervals.of("2013-01-01/PT1H"), "v1", 0))
                    .size(100_000_000L)
-                   .version("v1")
                    .build();
 
   @Test
@@ -70,8 +64,7 @@ public class CompactionStatusTest
   {
     final ClientCompactionTaskQueryTuningConfig tuningConfig
         = ClientCompactionTaskQueryTuningConfig.from(null);
-    Assert.assertEquals(
-        new DynamicPartitionsSpec(null, Long.MAX_VALUE),
+    Assert.assertNull(
         CompactionStatus.findPartitionsSpecFromConfig(tuningConfig)
     );
   }
@@ -203,9 +196,14 @@ public class CompactionStatusTest
   @Test
   public void testStatusWhenLastCompactionStateIsEmpty()
   {
+    final PartitionsSpec requiredPartitionsSpec = new DynamicPartitionsSpec(5_000_000, null);
     verifyCompactionStatusIsPendingBecause(
         new CompactionState(null, null, null, null, null, null, null),
-        InlineSchemaDataSourceCompactionConfig.builder().forDataSource(TestDataSource.WIKI).build(),
+        InlineSchemaDataSourceCompactionConfig
+            .builder()
+            .withTuningConfig(createTuningConfig(requiredPartitionsSpec, null))
+            .forDataSource(TestDataSource.WIKI)
+            .build(),
         "'partitionsSpec' mismatch: required['dynamic' with 5,000,000 rows], current[null]"
     );
   }
@@ -213,12 +211,16 @@ public class CompactionStatusTest
   @Test
   public void testStatusOnPartitionsSpecMismatch()
   {
+    final PartitionsSpec requiredPartitionsSpec = new DynamicPartitionsSpec(5_000_000, null);
     final PartitionsSpec currentPartitionsSpec = new DynamicPartitionsSpec(100, null);
 
     final CompactionState lastCompactionState
         = new CompactionState(currentPartitionsSpec, null, null, null, null, null, null);
-    final DataSourceCompactionConfig compactionConfig
-        = InlineSchemaDataSourceCompactionConfig.builder().forDataSource(TestDataSource.WIKI).build();
+    final DataSourceCompactionConfig compactionConfig = InlineSchemaDataSourceCompactionConfig
+        .builder()
+        .withTuningConfig(createTuningConfig(requiredPartitionsSpec, null))
+        .forDataSource(TestDataSource.WIKI)
+        .build();
 
     verifyCompactionStatusIsPendingBecause(
         lastCompactionState,
@@ -323,9 +325,8 @@ public class CompactionStatusTest
 
     final DataSegment segment = DataSegment.builder(WIKI_SEGMENT).lastCompactionState(lastCompactionState).build();
     final CompactionStatus status = CompactionStatus.compute(
-        CompactionCandidate.from(Collections.singletonList(segment)),
-        compactionConfig,
-        OBJECT_MAPPER
+        CompactionCandidate.from(List.of(segment), Granularities.HOUR),
+        compactionConfig
     );
     Assert.assertTrue(status.isComplete());
   }
@@ -373,9 +374,8 @@ public class CompactionStatusTest
 
     final DataSegment segment = DataSegment.builder(WIKI_SEGMENT).lastCompactionState(lastCompactionState).build();
     final CompactionStatus status = CompactionStatus.compute(
-        CompactionCandidate.from(Collections.singletonList(segment)),
-        compactionConfig,
-        OBJECT_MAPPER
+        CompactionCandidate.from(List.of(segment), Granularities.HOUR),
+        compactionConfig
     );
     Assert.assertTrue(status.isComplete());
   }
@@ -428,9 +428,8 @@ public class CompactionStatusTest
 
     final DataSegment segment = DataSegment.builder(WIKI_SEGMENT).lastCompactionState(lastCompactionState).build();
     final CompactionStatus status = CompactionStatus.compute(
-        CompactionCandidate.from(Collections.singletonList(segment)),
-        compactionConfig,
-        OBJECT_MAPPER
+        CompactionCandidate.from(List.of(segment), Granularities.HOUR),
+        compactionConfig
     );
     Assert.assertFalse(status.isComplete());
   }
@@ -482,9 +481,8 @@ public class CompactionStatusTest
 
     final DataSegment segment = DataSegment.builder(WIKI_SEGMENT).lastCompactionState(lastCompactionState).build();
     final CompactionStatus status = CompactionStatus.compute(
-        CompactionCandidate.from(Collections.singletonList(segment)),
-        compactionConfig,
-        OBJECT_MAPPER
+        CompactionCandidate.from(List.of(segment), null),
+        compactionConfig
     );
     Assert.assertTrue(status.isComplete());
   }
@@ -536,9 +534,8 @@ public class CompactionStatusTest
 
     final DataSegment segment = DataSegment.builder(WIKI_SEGMENT).lastCompactionState(lastCompactionState).build();
     final CompactionStatus status = CompactionStatus.compute(
-        CompactionCandidate.from(Collections.singletonList(segment)),
-        compactionConfig,
-        OBJECT_MAPPER
+        CompactionCandidate.from(List.of(segment), null),
+        compactionConfig
     );
     Assert.assertFalse(status.isComplete());
   }
@@ -554,9 +551,8 @@ public class CompactionStatusTest
                      .lastCompactionState(lastCompactionState)
                      .build();
     final CompactionStatus status = CompactionStatus.compute(
-        CompactionCandidate.from(Collections.singletonList(segment)),
-        compactionConfig,
-        OBJECT_MAPPER
+        CompactionCandidate.from(List.of(segment), null),
+        compactionConfig
     );
 
     Assert.assertFalse(status.isComplete());
diff --git a/server/src/test/java/org/apache/druid/server/compaction/CompactionStatusTrackerTest.java b/server/src/test/java/org/apache/druid/server/compaction/CompactionStatusTrackerTest.java
index 1878aab8c0..c0496b0705 100644
--- a/server/src/test/java/org/apache/druid/server/compaction/CompactionStatusTrackerTest.java
+++ b/server/src/test/java/org/apache/druid/server/compaction/CompactionStatusTrackerTest.java
@@ -19,26 +19,20 @@
 
 package org.apache.druid.server.compaction;
 
-import com.fasterxml.jackson.databind.ObjectMapper;
-import org.apache.druid.client.indexing.ClientCompactionTaskQuery;
 import org.apache.druid.indexer.TaskState;
 import org.apache.druid.indexer.TaskStatus;
-import org.apache.druid.jackson.DefaultObjectMapper;
 import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.segment.TestDataSource;
 import org.apache.druid.server.coordinator.CreateDataSegments;
-import org.apache.druid.server.coordinator.DataSourceCompactionConfig;
-import org.apache.druid.server.coordinator.InlineSchemaDataSourceCompactionConfig;
 import org.apache.druid.timeline.DataSegment;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
 
-import java.util.Collections;
+import java.util.List;
 
 public class CompactionStatusTrackerTest
 {
-  private static final ObjectMapper MAPPER = new DefaultObjectMapper();
   private static final DataSegment WIKI_SEGMENT
       = CreateDataSegments.ofDatasource(TestDataSource.WIKI).eachOfSizeInMb(100).get(0);
 
@@ -47,15 +41,15 @@ public class CompactionStatusTrackerTest
   @Before
   public void setup()
   {
-    statusTracker = new CompactionStatusTracker(MAPPER);
+    statusTracker = new CompactionStatusTracker();
   }
 
   @Test
   public void testGetLatestTaskStatusForSubmittedTask()
   {
     final CompactionCandidate candidateSegments
-        = CompactionCandidate.from(Collections.singletonList(WIKI_SEGMENT));
-    statusTracker.onTaskSubmitted(createCompactionTask("task1"), candidateSegments);
+        = CompactionCandidate.from(List.of(WIKI_SEGMENT), null);
+    statusTracker.onTaskSubmitted("task1", candidateSegments);
 
     CompactionTaskStatus status = statusTracker.getLatestTaskStatus(candidateSegments);
     Assert.assertEquals(TaskState.RUNNING, status.getState());
@@ -65,8 +59,8 @@ public class CompactionStatusTrackerTest
   public void testGetLatestTaskStatusForSuccessfulTask()
   {
     final CompactionCandidate candidateSegments
-        = CompactionCandidate.from(Collections.singletonList(WIKI_SEGMENT));
-    statusTracker.onTaskSubmitted(createCompactionTask("task1"), candidateSegments);
+        = CompactionCandidate.from(List.of(WIKI_SEGMENT), null);
+    statusTracker.onTaskSubmitted("task1", candidateSegments);
     statusTracker.onTaskFinished("task1", TaskStatus.success("task1"));
 
     CompactionTaskStatus status = statusTracker.getLatestTaskStatus(candidateSegments);
@@ -77,8 +71,8 @@ public class CompactionStatusTrackerTest
   public void testGetLatestTaskStatusForFailedTask()
   {
     final CompactionCandidate candidateSegments
-        = CompactionCandidate.from(Collections.singletonList(WIKI_SEGMENT));
-    statusTracker.onTaskSubmitted(createCompactionTask("task1"), candidateSegments);
+        = CompactionCandidate.from(List.of(WIKI_SEGMENT), null);
+    statusTracker.onTaskSubmitted("task1", candidateSegments);
     statusTracker.onTaskFinished("task1", TaskStatus.failure("task1", "some failure"));
 
     CompactionTaskStatus status = statusTracker.getLatestTaskStatus(candidateSegments);
@@ -90,12 +84,12 @@ public class CompactionStatusTrackerTest
   public void testGetLatestTaskStatusForRepeatedlyFailingTask()
   {
     final CompactionCandidate candidateSegments
-        = CompactionCandidate.from(Collections.singletonList(WIKI_SEGMENT));
+        = CompactionCandidate.from(List.of(WIKI_SEGMENT), null);
 
-    statusTracker.onTaskSubmitted(createCompactionTask("task1"), candidateSegments);
+    statusTracker.onTaskSubmitted("task1", candidateSegments);
     statusTracker.onTaskFinished("task1", TaskStatus.failure("task1", "some failure"));
 
-    statusTracker.onTaskSubmitted(createCompactionTask("task2"), candidateSegments);
+    statusTracker.onTaskSubmitted("task2", candidateSegments);
     CompactionTaskStatus status = statusTracker.getLatestTaskStatus(candidateSegments);
     Assert.assertEquals(TaskState.RUNNING, status.getState());
     Assert.assertEquals(1, status.getNumConsecutiveFailures());
@@ -110,25 +104,23 @@ public class CompactionStatusTrackerTest
   @Test
   public void testComputeCompactionStatusForSuccessfulTask()
   {
-    final DataSourceCompactionConfig compactionConfig
-        = InlineSchemaDataSourceCompactionConfig.builder().forDataSource(TestDataSource.WIKI).build();
     final NewestSegmentFirstPolicy policy = new NewestSegmentFirstPolicy(null);
     final CompactionCandidate candidateSegments
-        = CompactionCandidate.from(Collections.singletonList(WIKI_SEGMENT));
+        = CompactionCandidate.from(List.of(WIKI_SEGMENT), null);
 
     // Verify that interval is originally eligible for compaction
     CompactionStatus status
-        = statusTracker.computeCompactionStatus(candidateSegments, compactionConfig, policy);
+        = statusTracker.computeCompactionStatus(candidateSegments, policy);
     Assert.assertEquals(CompactionStatus.State.PENDING, status.getState());
-    Assert.assertEquals("not compacted yet", status.getReason());
+    Assert.assertEquals("Not compacted yet", status.getReason());
 
     // Verify that interval is skipped for compaction after task has finished
     statusTracker.onSegmentTimelineUpdated(DateTimes.nowUtc().minusMinutes(1));
-    statusTracker.onTaskSubmitted(createCompactionTask("task1"), candidateSegments);
+    statusTracker.onTaskSubmitted("task1", candidateSegments);
     statusTracker.onTaskFinished("task1", TaskStatus.success("task1"));
 
-    status = statusTracker.computeCompactionStatus(candidateSegments, compactionConfig, policy);
-    Assert.assertEquals(CompactionStatus.State.SKIPPED, status.getState());
+    status = statusTracker.computeCompactionStatus(candidateSegments, policy);
+    Assert.assertEquals(CompactionStatus.State.COMPLETE, status.getState());
     Assert.assertEquals(
         "Segment timeline not updated since last compaction task succeeded",
         status.getReason()
@@ -136,26 +128,7 @@ public class CompactionStatusTrackerTest
 
     // Verify that interval becomes eligible again after timeline has been updated
     statusTracker.onSegmentTimelineUpdated(DateTimes.nowUtc());
-    status = statusTracker.computeCompactionStatus(candidateSegments, compactionConfig, policy);
+    status = statusTracker.computeCompactionStatus(candidateSegments, policy);
     Assert.assertEquals(CompactionStatus.State.PENDING, status.getState());
   }
-
-  private ClientCompactionTaskQuery createCompactionTask(
-      String taskId
-  )
-  {
-    return new ClientCompactionTaskQuery(
-        taskId,
-        TestDataSource.WIKI,
-        null,
-        null,
-        null,
-        null,
-        null,
-        null,
-        null,
-        null,
-        null
-    );
-  }
 }
diff --git a/server/src/test/java/org/apache/druid/server/compaction/NewestSegmentFirstPolicyTest.java b/server/src/test/java/org/apache/druid/server/compaction/NewestSegmentFirstPolicyTest.java
index c655dc8ae4..1c92c9a249 100644
--- a/server/src/test/java/org/apache/druid/server/compaction/NewestSegmentFirstPolicyTest.java
+++ b/server/src/test/java/org/apache/druid/server/compaction/NewestSegmentFirstPolicyTest.java
@@ -65,7 +65,6 @@ import org.joda.time.DateTimeZone;
 import org.joda.time.Interval;
 import org.joda.time.Period;
 import org.junit.Assert;
-import org.junit.Before;
 import org.junit.Test;
 
 import java.util.ArrayList;
@@ -82,13 +81,6 @@ public class NewestSegmentFirstPolicyTest
   private static final int DEFAULT_NUM_SEGMENTS_PER_SHARD = 4;
   private final ObjectMapper mapper = new DefaultObjectMapper();
   private final NewestSegmentFirstPolicy policy = new NewestSegmentFirstPolicy(null);
-  private CompactionStatusTracker statusTracker;
-
-  @Before
-  public void setup()
-  {
-    statusTracker = new CompactionStatusTracker(mapper);
-  }
 
   @Test
   public void testLargeOffsetAndSmallSegmentInterval()
@@ -284,8 +276,7 @@ public class NewestSegmentFirstPolicyTest
                                 .withNumPartitions(4)
             )
         ),
-        Collections.emptyMap(),
-        statusTracker
+        Collections.emptyMap()
     );
 
     assertCompactSegmentIntervals(
@@ -517,8 +508,7 @@ public class NewestSegmentFirstPolicyTest
                 Intervals.of("2017-11-15T00:00:00/2017-11-15T20:00:00"),
                 Intervals.of("2017-11-13T00:00:00/2017-11-14T01:00:00")
             )
-        ),
-        statusTracker
+        )
     );
 
     assertCompactSegmentIntervals(
@@ -557,8 +547,7 @@ public class NewestSegmentFirstPolicyTest
                 Intervals.of("2017-11-16T04:00:00/2017-11-16T10:00:00"),
                 Intervals.of("2017-11-16T14:00:00/2017-11-16T20:00:00")
             )
-        ),
-        statusTracker
+        )
     );
 
     assertCompactSegmentIntervals(
@@ -1756,7 +1745,7 @@ public class NewestSegmentFirstPolicyTest
                 null,
                 new OnheapIncrementalIndex.Spec(true),
                 null,
-                1000L,
+                null,
                 null,
                 partitionsSpec,
                 IndexSpec.getDefault(),
@@ -1784,7 +1773,7 @@ public class NewestSegmentFirstPolicyTest
                 null,
                 new OnheapIncrementalIndex.Spec(false),
                 null,
-                1000L,
+                null,
                 null,
                 partitionsSpec,
                 IndexSpec.getDefault(),
@@ -2063,8 +2052,7 @@ public class NewestSegmentFirstPolicyTest
             TestDataSource.WIKI, SegmentTimeline.forSegments(wikiSegments),
             TestDataSource.KOALA, SegmentTimeline.forSegments(koalaSegments)
         ),
-        Collections.emptyMap(),
-        statusTracker
+        Collections.emptyMap()
     );
 
     // Verify that the segments of WIKI are preferred even though they are older
@@ -2085,8 +2073,7 @@ public class NewestSegmentFirstPolicyTest
         policy,
         Collections.singletonMap(TestDataSource.WIKI, config),
         Collections.singletonMap(TestDataSource.WIKI, timeline),
-        Collections.emptyMap(),
-        statusTracker
+        Collections.emptyMap()
     );
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/CreateDataSegments.java b/server/src/test/java/org/apache/druid/server/coordinator/CreateDataSegments.java
index 371d251b06..380533c729 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/CreateDataSegments.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/CreateDataSegments.java
@@ -105,6 +105,10 @@ public class CreateDataSegments
     return this;
   }
 
+  /**
+   * Specifies the version to use for creating the segments. Default version is
+   * {@code "1"}.
+   */
   public CreateDataSegments withVersion(String version)
   {
     this.version = version;
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java b/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java
index f77da3140a..39213ff861 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java
@@ -139,8 +139,7 @@ public class DruidCoordinatorTest
         )
     ).andReturn(new AtomicReference<>(DruidCompactionConfig.empty())).anyTimes();
     EasyMock.replay(configManager);
-    final ObjectMapper objectMapper = new DefaultObjectMapper();
-    statusTracker = new CompactionStatusTracker(objectMapper);
+    statusTracker = new CompactionStatusTracker();
     druidCoordinatorConfig = new DruidCoordinatorConfig(
         new CoordinatorRunConfig(new Duration(COORDINATOR_START_DELAY), new Duration(COORDINATOR_PERIOD)),
         new CoordinatorPeriodConfig(null, null),
@@ -169,7 +168,7 @@ public class DruidCoordinatorTest
         new TestDruidLeaderSelector(),
         null,
         CentralizedDatasourceSchemaConfig.create(),
-        new CompactionStatusTracker(OBJECT_MAPPER),
+        new CompactionStatusTracker(),
         EasyMock.niceMock(CoordinatorDynamicConfigSyncer.class),
         EasyMock.niceMock(CloneStatusManager.class)
     );
@@ -481,7 +480,7 @@ public class DruidCoordinatorTest
         new TestDruidLeaderSelector(),
         null,
         CentralizedDatasourceSchemaConfig.create(),
-        new CompactionStatusTracker(OBJECT_MAPPER),
+        new CompactionStatusTracker(),
         EasyMock.niceMock(CoordinatorDynamicConfigSyncer.class),
         EasyMock.niceMock(CloneStatusManager.class)
     );
@@ -533,7 +532,7 @@ public class DruidCoordinatorTest
         new TestDruidLeaderSelector(),
         null,
         CentralizedDatasourceSchemaConfig.create(),
-        new CompactionStatusTracker(OBJECT_MAPPER),
+        new CompactionStatusTracker(),
         EasyMock.niceMock(CoordinatorDynamicConfigSyncer.class),
         EasyMock.niceMock(CloneStatusManager.class)
     );
@@ -585,7 +584,7 @@ public class DruidCoordinatorTest
         new TestDruidLeaderSelector(),
         null,
         CentralizedDatasourceSchemaConfig.create(),
-        new CompactionStatusTracker(OBJECT_MAPPER),
+        new CompactionStatusTracker(),
         EasyMock.niceMock(CoordinatorDynamicConfigSyncer.class),
         EasyMock.niceMock(CloneStatusManager.class)
     );
@@ -695,7 +694,7 @@ public class DruidCoordinatorTest
         new TestDruidLeaderSelector(),
         null,
         CentralizedDatasourceSchemaConfig.create(),
-        new CompactionStatusTracker(OBJECT_MAPPER),
+        new CompactionStatusTracker(),
         EasyMock.niceMock(CoordinatorDynamicConfigSyncer.class),
         EasyMock.niceMock(CloneStatusManager.class)
     );
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java
index 0d00228960..301dd77493 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java
@@ -80,7 +80,9 @@ import org.apache.druid.segment.IndexSpec;
 import org.apache.druid.segment.incremental.OnheapIncrementalIndex;
 import org.apache.druid.segment.indexing.BatchIOConfig;
 import org.apache.druid.segment.transform.CompactionTransformSpec;
+import org.apache.druid.server.compaction.CompactionCandidate;
 import org.apache.druid.server.compaction.CompactionCandidateSearchPolicy;
+import org.apache.druid.server.compaction.CompactionSlotManager;
 import org.apache.druid.server.compaction.CompactionStatusTracker;
 import org.apache.druid.server.compaction.FixedIntervalOrderPolicy;
 import org.apache.druid.server.compaction.NewestSegmentFirstPolicy;
@@ -226,7 +228,7 @@ public class CompactSegmentsTest
       }
     }
     dataSources = DataSourcesSnapshot.fromUsedSegments(allSegments);
-    statusTracker = new CompactionStatusTracker(JSON_MAPPER);
+    statusTracker = new CompactionStatusTracker();
     policy = new NewestSegmentFirstPolicy(null);
   }
 
@@ -870,8 +872,8 @@ public class CompactSegmentsTest
     // All segments is compact at the same time since we changed the segment granularity to YEAR and all segment
     // are within the same year
     Assert.assertEquals(
-        ClientCompactionIntervalSpec.fromSegments(datasourceToSegments.get(dataSource), Granularities.YEAR),
-        taskPayload.getIoConfig().getInputSpec()
+        CompactionCandidate.from(datasourceToSegments.get(dataSource), Granularities.YEAR).getCompactionInterval(),
+        taskPayload.getIoConfig().getInputSpec().getInterval()
     );
 
     ClientCompactionTaskGranularitySpec expectedGranularitySpec =
@@ -1061,8 +1063,8 @@ public class CompactSegmentsTest
     // All segments is compact at the same time since we changed the segment granularity to YEAR and all segment
     // are within the same year
     Assert.assertEquals(
-        ClientCompactionIntervalSpec.fromSegments(datasourceToSegments.get(dataSource), Granularities.YEAR),
-        taskPayload.getIoConfig().getInputSpec()
+        CompactionCandidate.from(datasourceToSegments.get(dataSource), Granularities.YEAR).getCompactionInterval(),
+        taskPayload.getIoConfig().getInputSpec().getInterval()
     );
 
     ClientCompactionTaskGranularitySpec expectedGranularitySpec =
@@ -1156,8 +1158,8 @@ public class CompactSegmentsTest
     // All segments is compact at the same time since we changed the segment granularity to YEAR and all segment
     // are within the same year
     Assert.assertEquals(
-        ClientCompactionIntervalSpec.fromSegments(datasourceToSegments.get(dataSource), Granularities.YEAR),
-        taskPayload.getIoConfig().getInputSpec()
+        CompactionCandidate.from(datasourceToSegments.get(dataSource), Granularities.YEAR).getCompactionInterval(),
+        taskPayload.getIoConfig().getInputSpec().getInterval()
     );
 
     ClientCompactionTaskGranularitySpec expectedGranularitySpec =
@@ -1397,8 +1399,8 @@ public class CompactSegmentsTest
     ClientCompactionTaskQuery taskPayload = (ClientCompactionTaskQuery) payloadCaptor.getValue();
 
     Assert.assertEquals(
-        ClientCompactionIntervalSpec.fromSegments(segments, Granularities.DAY),
-        taskPayload.getIoConfig().getInputSpec()
+        CompactionCandidate.from(segments, Granularities.DAY).getCompactionInterval(),
+        taskPayload.getIoConfig().getInputSpec().getInterval()
     );
 
     ClientCompactionTaskGranularitySpec expectedGranularitySpec =
@@ -1460,8 +1462,8 @@ public class CompactSegmentsTest
     ClientCompactionTaskQuery taskPayload = (ClientCompactionTaskQuery) payloadCaptor.getValue();
 
     Assert.assertEquals(
-        ClientCompactionIntervalSpec.fromSegments(segments, Granularities.YEAR),
-        taskPayload.getIoConfig().getInputSpec()
+        CompactionCandidate.from(segments, Granularities.YEAR).getCompactionInterval(),
+        taskPayload.getIoConfig().getInputSpec().getInterval()
     );
 
     ClientCompactionTaskGranularitySpec expectedGranularitySpec =
@@ -2004,7 +2006,7 @@ public class CompactSegmentsTest
     @Test
     public void testIsParalleModeNullTuningConfigReturnFalse()
     {
-      Assert.assertFalse(CompactSegments.isParallelMode(null));
+      Assert.assertFalse(CompactionSlotManager.isParallelMode(null));
     }
 
     @Test
@@ -2012,7 +2014,7 @@ public class CompactSegmentsTest
     {
       ClientCompactionTaskQueryTuningConfig tuningConfig = Mockito.mock(ClientCompactionTaskQueryTuningConfig.class);
       Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(null);
-      Assert.assertFalse(CompactSegments.isParallelMode(tuningConfig));
+      Assert.assertFalse(CompactionSlotManager.isParallelMode(tuningConfig));
     }
 
     @Test
@@ -2022,13 +2024,13 @@ public class CompactSegmentsTest
       Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(Mockito.mock(PartitionsSpec.class));
 
       Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(null);
-      Assert.assertFalse(CompactSegments.isParallelMode(tuningConfig));
+      Assert.assertFalse(CompactionSlotManager.isParallelMode(tuningConfig));
 
       Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
-      Assert.assertFalse(CompactSegments.isParallelMode(tuningConfig));
+      Assert.assertFalse(CompactionSlotManager.isParallelMode(tuningConfig));
 
       Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
-      Assert.assertTrue(CompactSegments.isParallelMode(tuningConfig));
+      Assert.assertTrue(CompactionSlotManager.isParallelMode(tuningConfig));
     }
 
     @Test
@@ -2038,13 +2040,13 @@ public class CompactSegmentsTest
       Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(Mockito.mock(SingleDimensionPartitionsSpec.class));
 
       Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(null);
-      Assert.assertFalse(CompactSegments.isParallelMode(tuningConfig));
+      Assert.assertFalse(CompactionSlotManager.isParallelMode(tuningConfig));
 
       Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
-      Assert.assertTrue(CompactSegments.isParallelMode(tuningConfig));
+      Assert.assertTrue(CompactionSlotManager.isParallelMode(tuningConfig));
 
       Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
-      Assert.assertTrue(CompactSegments.isParallelMode(tuningConfig));
+      Assert.assertTrue(CompactionSlotManager.isParallelMode(tuningConfig));
     }
 
     @Test
@@ -2053,7 +2055,7 @@ public class CompactSegmentsTest
       ClientCompactionTaskQueryTuningConfig tuningConfig = Mockito.mock(ClientCompactionTaskQueryTuningConfig.class);
       Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(Mockito.mock(PartitionsSpec.class));
       Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
-      Assert.assertEquals(3, CompactSegments.findMaxNumTaskSlotsUsedByOneNativeCompactionTask(tuningConfig));
+      Assert.assertEquals(3, CompactionSlotManager.getMaxTaskSlotsForNativeCompactionTask(tuningConfig));
     }
 
     @Test
@@ -2062,7 +2064,7 @@ public class CompactSegmentsTest
       ClientCompactionTaskQueryTuningConfig tuningConfig = Mockito.mock(ClientCompactionTaskQueryTuningConfig.class);
       Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(Mockito.mock(PartitionsSpec.class));
       Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
-      Assert.assertEquals(1, CompactSegments.findMaxNumTaskSlotsUsedByOneNativeCompactionTask(tuningConfig));
+      Assert.assertEquals(1, CompactionSlotManager.getMaxTaskSlotsForNativeCompactionTask(tuningConfig));
     }
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBuilder.java b/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBuilder.java
index 8d63492375..0838b41402 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBuilder.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBuilder.java
@@ -223,7 +223,7 @@ public class CoordinatorSimulationBuilder
         env.leaderSelector,
         null,
         CentralizedDatasourceSchemaConfig.create(),
-        new CompactionStatusTracker(OBJECT_MAPPER),
+        new CompactionStatusTracker(),
         env.configSyncer,
         env.cloneStatusManager
     );
diff --git a/server/src/test/java/org/apache/druid/server/metrics/LatchableEmitter.java b/server/src/test/java/org/apache/druid/server/metrics/LatchableEmitter.java
index 247b74d8e5..5d8b110e17 100644
--- a/server/src/test/java/org/apache/druid/server/metrics/LatchableEmitter.java
+++ b/server/src/test/java/org/apache/druid/server/metrics/LatchableEmitter.java
@@ -24,6 +24,8 @@ import org.apache.druid.java.util.common.logger.Logger;
 import org.apache.druid.java.util.emitter.core.Event;
 import org.apache.druid.java.util.emitter.service.ServiceMetricEvent;
 import org.apache.druid.java.util.metrics.StubServiceEmitter;
+import org.hamcrest.Matcher;
+import org.hamcrest.Matchers;
 import org.junit.jupiter.api.Timeout;
 
 import java.util.ArrayList;
@@ -245,8 +247,8 @@ public class LatchableEmitter extends StubServiceEmitter
     private String host;
     private String service;
     private String metricName;
-    private Long metricValue;
-    private final Map<String, Object> dimensions = new HashMap<>();
+    private Matcher<Long> valueMatcher;
+    private final Map<String, Matcher<Object>> dimensionMatchers = new HashMap<>();
 
     private final AtomicReference<ServiceMetricEvent> matchingEvent = new AtomicReference<>();
 
@@ -260,12 +262,11 @@ public class LatchableEmitter extends StubServiceEmitter
     }
 
     /**
-     * Matches an event only if it has a metric value equal to or greater than
-     * the given value.
+     * Matches an event only if the metric value satisfies the given matcher.
      */
-    public EventMatcher hasValueAtLeast(long metricValue)
+    public EventMatcher hasValueMatching(Matcher<Long> valueMatcher)
     {
-      this.metricValue = metricValue;
+      this.valueMatcher = valueMatcher;
       return this;
     }
 
@@ -274,7 +275,16 @@ public class LatchableEmitter extends StubServiceEmitter
      */
     public EventMatcher hasDimension(String dimension, Object value)
     {
-      dimensions.put(dimension, value);
+      dimensionMatchers.put(dimension, Matchers.equalTo(value));
+      return this;
+    }
+
+    /**
+     * Matches an event if the value of the given dimension satisfies the matcher.
+     */
+    public EventMatcher hasDimensionMatching(String dimension, Matcher<Object> matcher)
+    {
+      dimensionMatchers.put(dimension, matcher);
       return this;
     }
 
@@ -301,7 +311,7 @@ public class LatchableEmitter extends StubServiceEmitter
     {
       if (metricName != null && !event.getMetric().equals(metricName)) {
         return false;
-      } else if (metricValue != null && event.getValue().longValue() < metricValue) {
+      } else if (valueMatcher != null && !valueMatcher.matches(event.getValue())) {
         return false;
       } else if (service != null && !service.equals(event.getService())) {
         return false;
@@ -309,10 +319,8 @@ public class LatchableEmitter extends StubServiceEmitter
         return false;
       }
 
-      final boolean matches = dimensions.entrySet().stream().allMatch(
-          dimValue -> event.getUserDims()
-                           .getOrDefault(dimValue.getKey(), "")
-                           .equals(dimValue.getValue())
+      final boolean matches = dimensionMatchers.entrySet().stream().allMatch(
+          dimValue -> dimValue.getValue().matches(event.getUserDims().get(dimValue.getKey()))
       );
 
       if (matches) {
diff --git a/services/src/main/java/org/apache/druid/cli/CliOverlord.java b/services/src/main/java/org/apache/druid/cli/CliOverlord.java
index 81a0c13671..63a1d28bfc 100644
--- a/services/src/main/java/org/apache/druid/cli/CliOverlord.java
+++ b/services/src/main/java/org/apache/druid/cli/CliOverlord.java
@@ -70,6 +70,7 @@ import org.apache.druid.indexing.common.task.batch.parallel.ShuffleClient;
 import org.apache.druid.indexing.common.tasklogs.SwitchingTaskLogStreamer;
 import org.apache.druid.indexing.common.tasklogs.TaskRunnerTaskLogStreamer;
 import org.apache.druid.indexing.compact.CompactionScheduler;
+import org.apache.druid.indexing.compact.DruidInputSourceFactory;
 import org.apache.druid.indexing.compact.OverlordCompactionScheduler;
 import org.apache.druid.indexing.overlord.DruidOverlord;
 import org.apache.druid.indexing.overlord.ForkingTaskRunnerFactory;
@@ -247,6 +248,7 @@ public class CliOverlord extends ServerRunnable
             binder.bind(TaskQueryTool.class).in(LazySingleton.class);
             binder.bind(IndexerMetadataStorageAdapter.class).in(LazySingleton.class);
             binder.bind(CompactionScheduler.class).to(OverlordCompactionScheduler.class).in(ManageLifecycle.class);
+            binder.bind(DruidInputSourceFactory.class).in(LazySingleton.class);
             binder.bind(ScheduledBatchTaskManager.class).in(LazySingleton.class);
             binder.bind(SupervisorManager.class).in(LazySingleton.class);
 
diff --git a/sql/src/test/java/org/apache/druid/sql/calcite/schema/DruidCalciteSchemaModuleTest.java b/sql/src/test/java/org/apache/druid/sql/calcite/schema/DruidCalciteSchemaModuleTest.java
index 2333561540..eb0ab607e8 100644
--- a/sql/src/test/java/org/apache/druid/sql/calcite/schema/DruidCalciteSchemaModuleTest.java
+++ b/sql/src/test/java/org/apache/druid/sql/calcite/schema/DruidCalciteSchemaModuleTest.java
@@ -28,6 +28,8 @@ import com.google.inject.Key;
 import com.google.inject.Scopes;
 import com.google.inject.TypeLiteral;
 import com.google.inject.name.Names;
+import org.apache.druid.catalog.MapMetadataCatalog;
+import org.apache.druid.catalog.MetadataCatalog;
 import org.apache.druid.client.FilteredServerInventoryView;
 import org.apache.druid.client.TimelineServerView;
 import org.apache.druid.client.coordinator.CoordinatorClient;
@@ -56,7 +58,7 @@ import org.apache.druid.sql.calcite.view.ViewManager;
 import org.easymock.EasyMock;
 import org.easymock.EasyMockExtension;
 import org.easymock.Mock;
-import org.junit.Assert;
+import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.extension.ExtendWith;
@@ -120,6 +122,7 @@ public class DruidCalciteSchemaModuleTest extends CalciteTestBase
           binder.bindScope(LazySingleton.class, Scopes.SINGLETON);
           binder.bind(LookupExtractorFactoryContainerProvider.class).toInstance(lookupReferencesManager);
           binder.bind(CatalogResolver.class).toInstance(CatalogResolver.NULL_RESOLVER);
+          binder.bind(MetadataCatalog.class).toInstance(new MapMetadataCatalog(objectMapper));
           binder.bind(ServiceEmitter.class).toInstance(new ServiceEmitter("", "", null));
           binder.bind(OverlordClient.class).to(NoopOverlordClient.class);
           binder.bind(CoordinatorClient.class).to(NoopCoordinatorClient.class);
@@ -134,35 +137,39 @@ public class DruidCalciteSchemaModuleTest extends CalciteTestBase
   public void testDruidSchemaNameIsInjected()
   {
     String schemaName = injector.getInstance(Key.get(String.class, DruidSchemaName.class));
-    Assert.assertEquals(DRUID_SCHEMA_NAME, schemaName);
+    Assertions.assertEquals(DRUID_SCHEMA_NAME, schemaName);
   }
 
   @Test
   public void testDruidSqlSchemaIsInjectedAsSingleton()
   {
     NamedDruidSchema namedDruidSchema = injector.getInstance(NamedDruidSchema.class);
-    Assert.assertNotNull(namedDruidSchema);
+    Assertions.assertNotNull(namedDruidSchema);
     NamedDruidSchema other = injector.getInstance(NamedDruidSchema.class);
-    Assert.assertSame(other, namedDruidSchema);
+    Assertions.assertSame(other, namedDruidSchema);
   }
 
   @Test
   public void testSystemSqlSchemaIsInjectedAsSingleton()
   {
     NamedSystemSchema namedSystemSchema = injector.getInstance(NamedSystemSchema.class);
-    Assert.assertNotNull(namedSystemSchema);
+    Assertions.assertNotNull(namedSystemSchema);
     NamedSystemSchema other = injector.getInstance(NamedSystemSchema.class);
-    Assert.assertSame(other, namedSystemSchema);
+    Assertions.assertSame(other, namedSystemSchema);
   }
 
   @Test
   public void testDruidCalciteSchemasAreInjected()
   {
     Set<NamedSchema> sqlSchemas = injector.getInstance(Key.get(new TypeLiteral<>() {}));
-    Set<Class<? extends NamedSchema>> expectedSchemas =
-        ImmutableSet.of(NamedSystemSchema.class, NamedDruidSchema.class, NamedLookupSchema.class, NamedViewSchema.class);
-    Assert.assertEquals(expectedSchemas.size(), sqlSchemas.size());
-    Assert.assertEquals(
+    Set<Class<? extends NamedSchema>> expectedSchemas = Set.of(
+        NamedSystemSchema.class,
+        NamedDruidSchema.class,
+        NamedLookupSchema.class,
+        NamedViewSchema.class
+    );
+    Assertions.assertEquals(expectedSchemas.size(), sqlSchemas.size());
+    Assertions.assertEquals(
         expectedSchemas,
         sqlSchemas.stream().map(NamedSchema::getClass).collect(Collectors.toSet()));
   }
@@ -171,36 +178,36 @@ public class DruidCalciteSchemaModuleTest extends CalciteTestBase
   public void testDruidSchemaIsInjectedAsSingleton()
   {
     DruidSchema schema = injector.getInstance(DruidSchema.class);
-    Assert.assertNotNull(schema);
+    Assertions.assertNotNull(schema);
     DruidSchema other = injector.getInstance(DruidSchema.class);
-    Assert.assertSame(other, schema);
+    Assertions.assertSame(other, schema);
   }
 
   @Test
   public void testSystemSchemaIsInjectedAsSingleton()
   {
     SystemSchema schema = injector.getInstance(SystemSchema.class);
-    Assert.assertNotNull(schema);
+    Assertions.assertNotNull(schema);
     SystemSchema other = injector.getInstance(SystemSchema.class);
-    Assert.assertSame(other, schema);
+    Assertions.assertSame(other, schema);
   }
 
   @Test
   public void testInformationSchemaIsInjectedAsSingleton()
   {
     InformationSchema schema = injector.getInstance(InformationSchema.class);
-    Assert.assertNotNull(schema);
+    Assertions.assertNotNull(schema);
     InformationSchema other = injector.getInstance(InformationSchema.class);
-    Assert.assertSame(other, schema);
+    Assertions.assertSame(other, schema);
   }
 
   @Test
   public void testLookupSchemaIsInjectedAsSingleton()
   {
     LookupSchema schema = injector.getInstance(LookupSchema.class);
-    Assert.assertNotNull(schema);
+    Assertions.assertNotNull(schema);
     LookupSchema other = injector.getInstance(LookupSchema.class);
-    Assert.assertSame(other, schema);
+    Assertions.assertSame(other, schema);
   }
 
   @Test
@@ -209,22 +216,22 @@ public class DruidCalciteSchemaModuleTest extends CalciteTestBase
     DruidSchemaCatalog rootSchema = injector.getInstance(
         Key.get(DruidSchemaCatalog.class, Names.named(DruidCalciteSchemaModule.INCOMPLETE_SCHEMA))
     );
-    Assert.assertNotNull(rootSchema);
+    Assertions.assertNotNull(rootSchema);
     DruidSchemaCatalog other = injector.getInstance(
         Key.get(DruidSchemaCatalog.class, Names.named(DruidCalciteSchemaModule.INCOMPLETE_SCHEMA))
     );
-    Assert.assertSame(other, rootSchema);
+    Assertions.assertSame(other, rootSchema);
   }
 
   @Test
   public void testRootSchemaIsInjectedAsSingleton()
   {
     DruidSchemaCatalog rootSchema = injector.getInstance(Key.get(DruidSchemaCatalog.class));
-    Assert.assertNotNull(rootSchema);
+    Assertions.assertNotNull(rootSchema);
     DruidSchemaCatalog other = injector.getInstance(
         Key.get(DruidSchemaCatalog.class, Names.named(DruidCalciteSchemaModule.INCOMPLETE_SCHEMA))
     );
-    Assert.assertSame(other, rootSchema);
+    Assertions.assertSame(other, rootSchema);
   }
 
   @Test
@@ -232,7 +239,7 @@ public class DruidCalciteSchemaModuleTest extends CalciteTestBase
   {
     DruidSchemaCatalog rootSchema = injector.getInstance(Key.get(DruidSchemaCatalog.class));
     InformationSchema expectedSchema = injector.getInstance(InformationSchema.class);
-    Assert.assertNotNull(rootSchema);
-    Assert.assertSame(expectedSchema, rootSchema.getSubSchema("INFORMATION_SCHEMA").unwrap(InformationSchema.class));
+    Assertions.assertNotNull(rootSchema);
+    Assertions.assertSame(expectedSchema, rootSchema.getSubSchema("INFORMATION_SCHEMA").unwrap(InformationSchema.class));
   }
 }
diff --git a/sql/src/test/java/org/apache/druid/sql/calcite/util/SqlTestFramework.java b/sql/src/test/java/org/apache/druid/sql/calcite/util/SqlTestFramework.java
index 7a7cf16459..38ddba751b 100644
--- a/sql/src/test/java/org/apache/druid/sql/calcite/util/SqlTestFramework.java
+++ b/sql/src/test/java/org/apache/druid/sql/calcite/util/SqlTestFramework.java
@@ -35,6 +35,7 @@ import org.apache.druid.client.cache.CacheConfig;
 import org.apache.druid.collections.BlockingPool;
 import org.apache.druid.collections.NonBlockingPool;
 import org.apache.druid.guice.BuiltInTypesModule;
+import org.apache.druid.guice.CatalogCoreModule;
 import org.apache.druid.guice.DruidInjectorBuilder;
 import org.apache.druid.guice.ExpressionModule;
 import org.apache.druid.guice.LazySingleton;
@@ -409,6 +410,7 @@ public class SqlTestFramework
           new LookylooModule(),
           new SegmentWranglerModule(),
           new ExpressionModule(),
+          new CatalogCoreModule(),
           DruidModule.override(
               new QueryRunnerFactoryModule(),
               new Module()
diff --git a/sql/src/test/java/org/apache/druid/sql/guice/SqlModuleTest.java b/sql/src/test/java/org/apache/druid/sql/guice/SqlModuleTest.java
index 28e8923f63..f0057eb9c5 100644
--- a/sql/src/test/java/org/apache/druid/sql/guice/SqlModuleTest.java
+++ b/sql/src/test/java/org/apache/druid/sql/guice/SqlModuleTest.java
@@ -32,6 +32,7 @@ import org.apache.druid.client.TimelineServerView;
 import org.apache.druid.client.coordinator.CoordinatorClient;
 import org.apache.druid.client.coordinator.NoopCoordinatorClient;
 import org.apache.druid.discovery.DruidNodeDiscoveryProvider;
+import org.apache.druid.guice.CatalogCoreModule;
 import org.apache.druid.guice.DruidGuiceExtensions;
 import org.apache.druid.guice.JsonConfigurator;
 import org.apache.druid.guice.LazySingleton;
@@ -182,6 +183,7 @@ public class SqlModuleTest
             new JacksonModule(),
             new PolicyModule(),
             new AuthenticatorMapperModule(),
+            new CatalogCoreModule(),
             binder -> {
               binder.bind(Validator.class).toInstance(Validation.buildDefaultValidatorFactory().getValidator());
               binder.bind(JsonConfigurator.class).in(LazySingleton.class);
