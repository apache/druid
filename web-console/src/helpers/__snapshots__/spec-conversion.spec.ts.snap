// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`spec conversion converts index_hadoop spec (with rollup) 1`] = `
-- This SQL query was auto generated from an ingestion spec
REPLACE INTO "newSource" OVERWRITE ALL
WITH "source" AS (SELECT * FROM TABLE(
  EXTERN(
    '{"type":"s3","uris":["s3://path"]}',
    '{"columns":["col1","col2","col3","col4","metric1","metric2","metric3","metric4","metric5","metric6","metric7"],"type":"timeAndDims"}'
  )
) EXTEND ("event_ts" VARCHAR, "col1" VARCHAR, "col2" VARCHAR, "col3" VARCHAR, "col4" VARCHAR, "field1" DOUBLE, "field2" DOUBLE, "field3" DOUBLE, "field4" VARCHAR, "field5" VARCHAR, "field6" BIGINT, "field7" DOUBLE))
SELECT
  TIME_FLOOR(CASE WHEN CAST("event_ts" AS BIGINT) > 0 THEN MILLIS_TO_TIMESTAMP(CAST("event_ts" AS BIGINT)) ELSE TIME_PARSE(TRIM("event_ts")) END, 'PT1H') AS "__time",
  "col1",
  "col2",
  "col3",
  "col4",
  SUM("field1") AS "metric1",
  MAX("field2") AS "metric2",
  MIN("field3") AS "metric3",
  APPROX_COUNT_DISTINCT_BUILTIN("field4") AS "metric4",
  APPROX_COUNT_DISTINCT_BUILTIN("field5") AS "metric5",
  SUM("field6") AS "metric6",
  SUM("field7") AS "metric7"
FROM "source"
WHERE "col2" = 'xxx'
GROUP BY 1, 2, 3, 4, 5
PARTITIONED BY HOUR
`;

exports[`spec conversion converts index_parallel spec (with rollup) 1`] = `
-- This SQL query was auto generated from an ingestion spec
REPLACE INTO "wikipedia_rollup" OVERWRITE ALL
WITH "source" AS (SELECT * FROM TABLE(
  EXTERN(
    '{"type":"http","uris":["https://druid.apache.org/data/wikipedia.json.gz"]}',
    '{"type":"json"}'
  )
) EXTEND ("timestamp" VARCHAR, "isRobot" VARCHAR, "channel" VARCHAR, "flags" VARCHAR, "isUnpatrolled" VARCHAR, "comment" VARCHAR, "isNew" VARCHAR, "isMinor" VARCHAR, "isAnonymous" VARCHAR, "user" VARCHAR, "namespace" VARCHAR, "cityName" VARCHAR, "countryName" VARCHAR, "regionIsoCode" VARCHAR, "metroCode" VARCHAR, "countryIsoCode" VARCHAR, "regionName" VARCHAR, "added" BIGINT, "commentLength" BIGINT, "delta" BIGINT, "deltaBucket" BIGINT, "deleted" BIGINT, "page" VARCHAR))
SELECT
  TIME_FLOOR(TIME_PARSE("timestamp"), 'PT1H') AS "__time",
  "isRobot",
  "channel",
  "flags",
  "isUnpatrolled",
  "comment",
  "isNew",
  "isMinor",
  "isAnonymous",
  "user",
  "namespace",
  "cityName",
  "countryName",
  "regionIsoCode",
  "metroCode",
  "countryIsoCode",
  "regionName",
  COUNT(*) AS "count",
  SUM("added") AS "sum_added",
  SUM("commentLength") AS "sum_commentLength",
  MAX("commentLength") AS "max_commentLength",
  SUM("delta") AS "sum_delta",
  SUM("deltaBucket") AS "sum_deltaBucket",
  SUM("deleted") AS "sum_deleted",
  APPROX_COUNT_DISTINCT_DS_THETA("page") AS "page_theta"
FROM "source"
GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17
PARTITIONED BY HOUR
`;

exports[`spec conversion converts index_parallel spec (without rollup) 1`] = `
-- This SQL query was auto generated from an ingestion spec
REPLACE INTO "wikipedia" OVERWRITE ALL
WITH "source" AS (SELECT * FROM TABLE(
  EXTERN(
    '{"type":"http","uris":["https://druid.apache.org/data/wikipedia.json.gz"]}',
    '{"type":"json"}'
  )
) EXTEND ("timestamp" VARCHAR, "isRobot" VARCHAR, "channel" VARCHAR, "flags" VARCHAR, "isUnpatrolled" VARCHAR, "page" VARCHAR, "diffUrl" VARCHAR, "added" BIGINT, "comment" VARCHAR, "commentLength" BIGINT, "isNew" VARCHAR, "isMinor" VARCHAR, "delta" BIGINT, "isAnonymous" VARCHAR, "user" VARCHAR, "deltaBucket" BIGINT, "deleted" BIGINT, "namespace" VARCHAR, "cityName" VARCHAR, "countryName" VARCHAR, "regionIsoCode" VARCHAR, "metroCode" VARCHAR, "countryIsoCode" VARCHAR, "regionName" VARCHAR, "event" TYPE('COMPLEX<json>')))
SELECT
  CASE WHEN CAST("timestamp" AS BIGINT) > 0 THEN MILLIS_TO_TIMESTAMP(CAST("timestamp" AS BIGINT)) ELSE TIME_PARSE(TRIM("timestamp")) END AS "__time",
  "isRobot",
  "channel",
  "flags",
  "isUnpatrolled",
  "page",
  "diffUrl",
  "added",
  "comment",
  "commentLength",
  "isNew",
  "isMinor",
  "delta",
  "isAnonymous",
  "user",
  "deltaBucket",
  "deleted",
  "namespace",
  "cityName",
  "countryName",
  "regionIsoCode",
  "metroCode",
  "countryIsoCode",
  "regionName",
  "event"
FROM "source"
WHERE NOT ("channel" = 'xxx')
PARTITIONED BY HOUR
CLUSTERED BY "isRobot"
`;

exports[`spec conversion converts with issue when there is a __time transform 1`] = `
-- This SQL query was auto generated from an ingestion spec
REPLACE INTO "wikipedia" OVERWRITE ALL
WITH "source" AS (SELECT * FROM TABLE(
  EXTERN(
    '{"type":"http","uris":["https://druid.apache.org/data/wikipedia.json.gz"]}',
    '{"type":"json"}'
  )
) EXTEND ("isRobot" VARCHAR, "channel" VARCHAR, "flags" VARCHAR, "isUnpatrolled" VARCHAR, "page" VARCHAR, "diffUrl" VARCHAR, "added" BIGINT, "comment" VARCHAR, "commentLength" BIGINT, "isNew" VARCHAR, "isMinor" VARCHAR, "delta" BIGINT, "isAnonymous" VARCHAR, "user" VARCHAR, "deltaBucket" BIGINT, "deleted" BIGINT, "namespace" VARCHAR, "cityName" VARCHAR, "countryName" VARCHAR, "regionIsoCode" VARCHAR, "metroCode" VARCHAR, "countryIsoCode" VARCHAR, "regionName" VARCHAR))
SELECT
  --:ISSUE: The spec contained transforms that could not be automatically converted.
  REWRITE_[_some_time_parse_expression_]_TO_SQL AS "__time", --:ISSUE: Transform for __time could not be converted
  "isRobot",
  "channel",
  "flags",
  "isUnpatrolled",
  "page",
  "diffUrl",
  "added",
  "comment",
  "commentLength",
  "isNew",
  "isMinor",
  "delta",
  "isAnonymous",
  "user",
  "deltaBucket",
  "deleted",
  "namespace",
  "cityName",
  "countryName",
  "regionIsoCode",
  "metroCode",
  "countryIsoCode",
  "regionName"
FROM "source"
PARTITIONED BY HOUR
CLUSTERED BY "isRobot"
`;

exports[`spec conversion converts with issue when there is a dimension transform and strange filter 1`] = `
-- This SQL query was auto generated from an ingestion spec
REPLACE INTO "wikipedia" OVERWRITE ALL
WITH "source" AS (SELECT * FROM TABLE(
  EXTERN(
    '{"type":"http","uris":["https://druid.apache.org/data/wikipedia.json.gz"]}',
    '{"type":"json"}'
  )
) EXTEND ("timestamp" VARCHAR, "isRobot" VARCHAR, "channel" VARCHAR, "flags" VARCHAR, "isUnpatrolled" VARCHAR, "page" VARCHAR, "diffUrl" VARCHAR, "added" BIGINT, "comment" VARCHAR, "commentLength" BIGINT, "isNew" VARCHAR, "isMinor" VARCHAR, "delta" BIGINT, "isAnonymous" VARCHAR, "user" VARCHAR, "deltaBucket" BIGINT, "deleted" BIGINT, "namespace" VARCHAR, "cityName" VARCHAR, "countryName" VARCHAR, "regionIsoCode" VARCHAR, "metroCode" VARCHAR, "countryIsoCode" VARCHAR, "regionName" VARCHAR))
SELECT
  --:ISSUE: The spec contained transforms that could not be automatically converted.
  CASE WHEN CAST("timestamp" AS BIGINT) > 0 THEN MILLIS_TO_TIMESTAMP(CAST("timestamp" AS BIGINT)) ELSE TIME_PARSE(TRIM("timestamp")) END AS "__time",
  "isRobot",
  "channel",
  "flags",
  "isUnpatrolled",
  "page",
  "diffUrl",
  "added",
  REWRITE_[_some_expression_]_TO_SQL AS "comment", --:ISSUE: Transform for dimension could not be converted
  "commentLength",
  "isNew",
  "isMinor",
  "delta",
  "isAnonymous",
  "user",
  "deltaBucket",
  "deleted",
  "namespace",
  "cityName",
  "countryName",
  "regionIsoCode",
  "metroCode",
  "countryIsoCode",
  "regionName"
FROM "source"
WHERE REWRITE_[{"type":"strange"}]_TO_SQL --:ISSUE: The spec contained a filter that could not be automatically converted, please convert it manually
PARTITIONED BY HOUR
CLUSTERED BY "isRobot"
`;

exports[`spec conversion converts with when the __time column is used as the __time column 1`] = `
-- This SQL query was auto generated from an ingestion spec
REPLACE INTO "wikipedia" OVERWRITE ALL
WITH "source" AS (SELECT * FROM TABLE(
  EXTERN(
    '{"type":"http","uris":["https://druid.apache.org/data/wikipedia.json.gz"]}',
    '{"type":"json"}'
  )
) EXTEND ("__time" BIGINT, "isRobot" VARCHAR, "channel" VARCHAR, "flags" VARCHAR))
SELECT
  "__time" AS "__time",
  "isRobot",
  "channel",
  "flags"
FROM "source"
PARTITIONED BY HOUR
CLUSTERED BY "isRobot"
`;

exports[`spec conversion works with ARRAY mode 1`] = `
-- This SQL query was auto generated from an ingestion spec
REPLACE INTO "lol" OVERWRITE ALL
WITH "source" AS (SELECT * FROM TABLE(
  EXTERN(
    '{"type":"inline","data":"{\\"s\\":\\"X\\", \\"l\\":10, \\"f\\":10.1, \\"array_s\\":[\\"A\\", \\"B\\"], \\"array_l\\":[1,2], \\"array_f\\":[1.1,2.2], \\"mix1\\":[1, \\"lol\\"], \\"mix2\\":[1.1, 77]}\\n{\\"s\\":\\"Y\\", \\"l\\":11, \\"f\\":11.1, \\"array_s\\":[\\"C\\", \\"D\\"], \\"array_l\\":[3,4], \\"array_f\\":[3.3,4.4], \\"mix1\\":[2, \\"zoz\\"], \\"mix2\\":[1.2, 88]}"}',
    '{"type":"json"}'
  )
) EXTEND ("s" VARCHAR, "l" BIGINT, "f" DOUBLE, "array_s" VARCHAR ARRAY, "array_l" BIGINT ARRAY, "array_f" DOUBLE ARRAY, "mix1" VARCHAR ARRAY, "mix2" DOUBLE ARRAY))
SELECT
  TIME_PARSE('2010-01-01T00:00:00Z') AS "__time",
  "s",
  "l",
  "f",
  "array_s",
  "array_l",
  "array_f",
  "mix1",
  "mix2"
FROM "source"
PARTITIONED BY DAY
`;
